<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Blog · &quot;ML Mondays&quot;</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="A weekly USGS-CDI course on image analysis using machine learning"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Blog · &quot;ML Mondays&quot;"/><meta property="og:type" content="website"/><meta property="og:url" content="https://dbuscombe-usgs.github.io/MLMONDAYS/"/><meta property="og:description" content="A weekly USGS-CDI course on image analysis using machine learning"/><meta property="og:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/MLMONDAYS/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/atom.xml" title="&quot;ML Mondays&quot; Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/feed.xml" title="&quot;ML Mondays&quot; Blog RSS Feed"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/MLMONDAYS/js/scrollSpy.js"></script><link rel="stylesheet" href="/MLMONDAYS/css/main.css"/><script src="/MLMONDAYS/js/codetabs.js"></script></head><body class="blog"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/MLMONDAYS/"><img class="logo" src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;"/><h2 class="headerTitleWithLogo">&quot;ML Mondays&quot;</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/MLMONDAYS/docs/doc1" target="_self">Docs</a></li><li class=""><a href="/MLMONDAYS/docs/doc2" target="_self">Data</a></li><li class=""><a href="/MLMONDAYS/docs/doc3" target="_self">Models</a></li><li class=""><a href="/MLMONDAYS/docs/doc4" target="_self">API</a></li><li class=""><a href="/MLMONDAYS/help" target="_self">Help</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/MLMONDAYS/blog/" target="_self">Blog</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Recent Posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Recent Posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/14/blog-post">Converting makesense.ai JSON labels to label (mask) imagery for an image segmentation project</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/11/blog-post">Preparing a new dataset for an object recognition project</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/03/blog-post">ML terminology, demystified</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/01/blog-post">Creating a Tensorflow Dataset for an image segmentation task</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/09/15/blog-post">Making workflows reproducible</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="posts"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/10/14/blog-post">Converting makesense.ai JSON labels to label (mask) imagery for an image segmentation project</a></h1><p class="post-meta">October 14, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><h2><a class="anchor" aria-hidden="true" id="annotate-images-on-makesenseai"></a><a href="#annotate-images-on-makesenseai" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Annotate images on makesense.ai</h2>
<p><a href="makesense.ai">makesense.ai</a> is pretty great and the tool I generally recommend for labeling images because it:</p>
<ul>
<li>works well and has a well designed interface</li>
<li>is free and open source</li>
<li>requires no account or uploading of data.</li>
</ul>
<p>Press 'Get started'</p>
<p><img src="/MLMONDAYS/blog/assets/shot1.png" alt=""></p>
<p>Load images (in my example, I am using two pictures of coins and other objects on sand). Select 'object detection' which will give you the point, line, box, and polygon toolsets</p>
<p><img src="/MLMONDAYS/blog/assets/shot2.png" alt=""></p>
<p>Create a list of labels (mine are 'coin', 'sand' and 'other')</p>
<p><img src="/MLMONDAYS/blog/assets/shot3.png" alt=""></p>
<p>Use the polygon tool to start delineating the scene, and select the label from the drop down list for each annotation</p>
<p><img src="/MLMONDAYS/blog/assets/shot4.png" alt=""></p>
<p><img src="/MLMONDAYS/blog/assets/shot5.png" alt=""></p>
<p>Image two (these sorts of scenes are tricky to label because sand is really a background class)</p>
<p><img src="/MLMONDAYS/blog/assets/shot6.png" alt=""></p>
<p>Actions &gt; Export annotations</p>
<p><img src="/MLMONDAYS/blog/assets/shot7.png" alt=""></p>
<p>Export in VGG JSON format in the polygon category</p>
<p><img src="/MLMONDAYS/blog/assets/shot8.png" alt=""></p>
<p>This is what your JSON format file looks like</p>
<p><img src="/MLMONDAYS/blog/assets/shot9.png" alt=""></p>
<p>Let's read it into python and convert it into a label mask</p>
<h2><a class="anchor" aria-hidden="true" id="create-label-images"></a><a href="#create-label-images" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Create label images</h2>
<p>Load the libraries we need</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> json, os, glob
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image, ImageDraw
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
<p>Define a class dictionary that allows for mapping of class string names to integers. Avoid zero - that is usually reserved for null/background for a binary segmentation. 'Other' is different in this context (rulers, and other things in the scene)</p>
<pre><code class="hljs css language-python">class_dict = {<span class="hljs-string">'coin'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'sand'</span>:<span class="hljs-number">2</span>, <span class="hljs-string">'other'</span>:<span class="hljs-number">3</span>}
</code></pre>
<p>Load the contents of the VGG JSON file downloaded from makesense.ai into the dictionary, <code>all_labels</code></p>
<pre><code class="hljs css language-python">json_file = <span class="hljs-string">'labels_my-project-name_2020-10-15-03-40-44.json'</span>
all_labels = json.load(open(json_file))
</code></pre>
<p>The keys of the dictionary are the image filenames</p>
<pre><code class="hljs css language-python">print(all_labels.keys())
</code></pre>
<p>And these are the quantities defined for each image</p>
<pre><code class="hljs css language-python">rawfile = <span class="hljs-string">'20181223_133712.jpg'</span>

print(all_labels[rawfile].keys())
</code></pre>
<p>This function will strip image coordinates (<code>X</code> and <code>Y</code>) of polygons, and associated class labels (<code>L</code>) from</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_data</span><span class="hljs-params">(data)</span>:</span>
    X = []; Y = []; L=[] <span class="hljs-comment">#pre-allocate lists to fill in a for loop</span>
    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> data[<span class="hljs-string">'regions'</span>]: <span class="hljs-comment">#cycle through each polygon</span>
        <span class="hljs-comment"># get the x and y points from the dictionary</span>
        X.append(data[<span class="hljs-string">'regions'</span>][k][<span class="hljs-string">'shape_attributes'</span>][<span class="hljs-string">'all_points_x'</span>])
        Y.append(data[<span class="hljs-string">'regions'</span>][k][<span class="hljs-string">'shape_attributes'</span>][<span class="hljs-string">'all_points_y'</span>])
        L.append(data[<span class="hljs-string">'regions'</span>][k][<span class="hljs-string">'region_attributes'</span>][<span class="hljs-string">'label'</span>])
    <span class="hljs-keyword">return</span> Y,X,L <span class="hljs-comment">#image coordinates are flipped relative to json coordinates</span>
</code></pre>
<p>Use it to extract the polygons from the first image:</p>
<pre><code class="hljs css language-python">X, Y, L = get_data(all_labels[rawfile])
</code></pre>
<p>Open an image to get its dimensions:</p>
<pre><code class="hljs css language-python">image = Image.open(rawfile)

nx, ny, nz = np.shape(image)
</code></pre>
<p>Next we need a function that will create a label image from the polygon vector data (coordinates and labels)</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_mask</span><span class="hljs-params">(X, Y, nx, ny, L, class_dict)</span>:</span>
    <span class="hljs-comment"># get the dimensions of the image</span>
    mask = np.zeros((nx,ny))

    <span class="hljs-keyword">for</span> y,x,l <span class="hljs-keyword">in</span> zip(X,Y,L):
        <span class="hljs-comment"># the ImageDraw.Draw().polygon function we will use to create the mask</span>
        <span class="hljs-comment"># requires the x's and y's are interweaved, which is what the following</span>
        <span class="hljs-comment"># one-liner does</span>
        polygon = np.vstack((x,y)).reshape((<span class="hljs-number">-1</span>,),order=<span class="hljs-string">'F'</span>).tolist()

        <span class="hljs-comment"># create a mask image of the right size and infill according to the polygon</span>
        <span class="hljs-keyword">if</span> nx&gt;ny:
           x,y = y,x
           img = Image.new(<span class="hljs-string">'L'</span>, (nx, ny), <span class="hljs-number">0</span>)
        <span class="hljs-keyword">elif</span> ny&gt;nx:
           <span class="hljs-comment">#x,y = y,x</span>
           img = Image.new(<span class="hljs-string">'L'</span>, (ny, nx), <span class="hljs-number">0</span>)
        <span class="hljs-keyword">else</span>:
           img = Image.new(<span class="hljs-string">'L'</span>, (nx, ny), <span class="hljs-number">0</span>)
        ImageDraw.Draw(img).polygon(polygon, outline=<span class="hljs-number">0</span>, fill=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># turn into a numpy array</span>
        m = np.flipud(np.rot90(np.array(img)))
        <span class="hljs-keyword">try</span>:
            mask[m==<span class="hljs-number">1</span>] = class_dict[l] <span class="hljs-comment">#mask + m</span>
        <span class="hljs-keyword">except</span>:
            mask[m.T==<span class="hljs-number">1</span>] = class_dict[l]  <span class="hljs-comment">#mask + m.T</span>

    <span class="hljs-keyword">return</span> mask
</code></pre>
<p>Apply it to get the label mask for the first image</p>
<pre><code class="hljs css language-python">mask = get_mask(X, Y, nx, ny, L, class_dict)
</code></pre>
<p>Next we'll define a function that we rescale our integer codes into 8-bit integer codes that span the full range. This 8-bit scaling will facilitate creation of label images that can be viewed using ordinary operating system image viewer software</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">rescale</span><span class="hljs-params">(dat,mn,mx)</span>:</span>
    <span class="hljs-string">'''
    rescales an input dat between mn and mx
    '''</span>
    m = min(dat.flatten())
    M = max(dat.flatten())
    <span class="hljs-keyword">return</span> (mx-mn)*(dat-m)/(M-m)+mn
</code></pre>
<p>Rescale the mask and convert it into a greyscale Image object, then save to file</p>
<pre><code class="hljs css language-python">mask = Image.fromarray(rescale(mask,<span class="hljs-number">0</span>,<span class="hljs-number">255</span>)).convert(<span class="hljs-string">'L'</span>)

mask.save(rawfile.replace(<span class="hljs-string">'imagery'</span>,<span class="hljs-string">'labels'</span>), format=<span class="hljs-string">'PNG'</span>)
</code></pre>
<p>Loop through several files at once:</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">for</span> rawfile <span class="hljs-keyword">in</span> all_labels.keys():

    X, Y, L = get_data(all_labels[rawfile])

    image = Image.open(rawfile)

    nx, ny, nz = np.shape(image)

    mask = get_mask(X, Y, nx, ny, L, class_dict)

    mask = Image.fromarray(rescale(mask,<span class="hljs-number">0</span>,<span class="hljs-number">255</span>)).convert(<span class="hljs-string">'L'</span>)

    mask.save(rawfile.replace(<span class="hljs-string">'.jpg'</span>,<span class="hljs-string">'_label.jpg'</span>), format=<span class="hljs-string">'PNG'</span>)    
</code></pre>
<p>Here are the label images:</p>
<p><img src="/MLMONDAYS/blog/assets/20181223_133712_label.jpg" alt=""></p>
<p><img src="/MLMONDAYS/blog/assets/20181223_133732_label.jpg" alt=""></p>
<p>Clearly, I'm a careless labeller. How could you make these labels better? Read on ...</p>
<h2><a class="anchor" aria-hidden="true" id="refine-label-images-with-a-crf"></a><a href="#refine-label-images-with-a-crf" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Refine label images with a CRF</h2>
<p>A CRF is a model that we will introduce and use in Week 3 and is useful for pre-processing manual labels, such as here, or post-processing model estimates.</p>
<p>It works by examining the label in each pixel of the label image, and assessing the likelihood of it, given the distribution of image values that it observes in the same and other classes in the scene. It is a probabilistic assessment based on both image features that it extracts, append</p>
<p>A CRF is not a deep learning model, or a neural network at all, but it is a network-based (or so-called <code>graphical</code> model). You can read more about it in <a href="https://www.mdpi.com/2076-3263/8/7/244">this paper</a>, where it was used as a post-processing rather than a pre-processing step.</p>
<p>These are the extra python libraries we need (within the <code>mlmondays</code> conda environment)</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">import</span> pydensecrf.densecrf <span class="hljs-keyword">as</span> dcrf
<span class="hljs-keyword">from</span> pydensecrf.utils <span class="hljs-keyword">import</span> create_pairwise_bilateral, unary_from_labels
</code></pre>
<p>Next we define a function that will use the CRF to process the label with respect to the image, and provide a new refined label</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">crf_refine</span><span class="hljs-params">(label, img)</span>:</span>
    <span class="hljs-string">"""
    "crf_refine(label, img)"
    This function refines a label image based on an input label image and the associated image
    Uses a conditional random field algorithm using spatial and image features
    INPUTS:
        * label [ndarray]: label image 2D matrix of integers
        * image [ndarray]: image 3D matrix of integers
    OPTIONAL INPUTS: None
    GLOBAL INPUTS: None
    OUTPUTS: label [ndarray]: label image 2D matrix of integers
    """</span>
    H = label.shape[<span class="hljs-number">0</span>]
    W = label.shape[<span class="hljs-number">1</span>]
    U = unary_from_labels(label,<span class="hljs-number">1</span>+len(np.unique(label)),gt_prob=<span class="hljs-number">0.51</span>)
    d = dcrf.DenseCRF2D(H, W, <span class="hljs-number">5</span>)
    d.setUnaryEnergy(U)

    <span class="hljs-comment"># to add the color-independent term, where features are the locations only:</span>
    d.addPairwiseGaussian(sxy=(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>),
                 compat=<span class="hljs-number">3</span>,
                 kernel=dcrf.DIAG_KERNEL,
                 normalization=dcrf.NORMALIZE_SYMMETRIC)
    feats = create_pairwise_bilateral(
                          sdims=(<span class="hljs-number">100</span>, <span class="hljs-number">100</span>),
                          schan=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>),
                          img=img,
                          chdim=<span class="hljs-number">2</span>)

    d.addPairwiseEnergy(feats, compat=<span class="hljs-number">120</span>,kernel=dcrf.DIAG_KERNEL,normalization=dcrf.NORMALIZE_SYMMETRIC)
    Q = d.inference(<span class="hljs-number">10</span>)
    <span class="hljs-keyword">return</span> np.argmax(Q, axis=<span class="hljs-number">0</span>).reshape((H, W)).astype(np.uint8)
</code></pre>
<p>Now we modify the <code>get_mask</code> function from before with the post-processing step</p>
<pre><code class="hljs css language-python">
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_mask_crf</span><span class="hljs-params">(X, Y, nx, ny, L, class_dict, image)</span>:</span>
    <span class="hljs-comment"># get the dimensions of the image</span>
    mask = np.zeros((nx,ny))

    <span class="hljs-keyword">for</span> y,x,l <span class="hljs-keyword">in</span> zip(X,Y,L):
        <span class="hljs-comment"># the ImageDraw.Draw().polygon function we will use to create the mask</span>
        <span class="hljs-comment"># requires the x's and y's are interweaved, which is what the following</span>
        <span class="hljs-comment"># one-liner does</span>
        polygon = np.vstack((x,y)).reshape((<span class="hljs-number">-1</span>,),order=<span class="hljs-string">'F'</span>).tolist()

        <span class="hljs-comment"># create a mask image of the right size and infill according to the polygon</span>
        <span class="hljs-keyword">if</span> nx&gt;ny:
           x,y = y,x
           img = Image.new(<span class="hljs-string">'L'</span>, (nx, ny), <span class="hljs-number">0</span>)
        <span class="hljs-keyword">elif</span> ny&gt;nx:
           <span class="hljs-comment">#x,y = y,x</span>
           img = Image.new(<span class="hljs-string">'L'</span>, (ny, nx), <span class="hljs-number">0</span>)
        <span class="hljs-keyword">else</span>:
           img = Image.new(<span class="hljs-string">'L'</span>, (nx, ny), <span class="hljs-number">0</span>)
        ImageDraw.Draw(img).polygon(polygon, outline=<span class="hljs-number">0</span>, fill=<span class="hljs-number">1</span>)
        <span class="hljs-comment"># turn into a numpy array</span>
        m = np.flipud(np.rot90(np.array(img)))
        <span class="hljs-keyword">try</span>:
            mask[m==<span class="hljs-number">1</span>] = class_dict[l] <span class="hljs-comment">#mask + m</span>
        <span class="hljs-keyword">except</span>:
            mask[m.T==<span class="hljs-number">1</span>] = class_dict[l]  <span class="hljs-comment">#mask + m.T</span>

    mask = crf_refine(np.array(mask, dtype=np.int), np.array(image, dtype=np.uint8))

    <span class="hljs-keyword">return</span> mask
</code></pre>
<p>And use a similar loop as before to apply this CRF processing</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">for</span> rawfile <span class="hljs-keyword">in</span> all_labels.keys():

    X, Y, L = get_data(all_labels[rawfile])

    image = Image.open(rawfile)

    nx, ny, nz = np.shape(image)

    mask = get_mask_crf(X, Y, nx, ny, L, class_dict, image)

    mask = Image.fromarray(rescale(mask/<span class="hljs-number">255</span>,<span class="hljs-number">0</span>,<span class="hljs-number">255</span>)).convert(<span class="hljs-string">'L'</span>)

    mask.save(rawfile.replace(<span class="hljs-string">'.jpg'</span>,<span class="hljs-string">'_label_crf.jpg'</span>), format=<span class="hljs-string">'PNG'</span>)
</code></pre>
<p>Here are the CRF-refined label images. Now there is no black (0) background class. The black (0) class is class 1; class 2 is 127; and class 3 is 255.</p>
<p><img src="/MLMONDAYS/blog/assets/20181223_133712_label_crf.jpg" alt=""></p>
<p><img src="/MLMONDAYS/blog/assets/20181223_133732_label_crf.jpg" alt=""></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/10/11/blog-post">Preparing a new dataset for an object recognition project</a></h1><p class="post-meta">October 11, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><p>Formatting your own data into a format that you can use in your data processing pipeline is arguably the most difficult aspect of this largely self-guided course.</p>
<p>The workflows I have provided each week to create tfrecords can be adapted to many different datasets, but will require some work on your part, using my provided examples and these blog posts. I don't know of any package that helps you out for every case. Perhaps I should write one! Perhaps you should!</p>
<p>We're going to:</p>
<ul>
<li>download some data from the internet</li>
<li>convert the labels data into a format we can use</li>
<li>write all the data to one big TFrecord file</li>
<li>write the data out in chunks to TFRecord shards</li>
</ul>
<p>This workflow is also provided in the following script: <code>2_ObjRecog/conservationdrones_make_tfrecords.py</code></p>
<h2><a class="anchor" aria-hidden="true" id="convert-images-and-bounding-boxes-into-tfrecords"></a><a href="#convert-images-and-bounding-boxes-into-tfrecords" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Convert images and bounding boxes into TFrecords</h2>
<h3><a class="anchor" aria-hidden="true" id="prepare-your-dataset-as-images-and-corresponding-labels"></a><a href="#prepare-your-dataset-as-images-and-corresponding-labels" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Prepare your dataset as images and corresponding labels</h3>
<p>To create an example, I headed over the excellent <a href="http://lila.science/datasets/">Labeled Information Library of Alexandria: Biology and Conservation</a> and chose the <a href="http://lila.science/datasets/conservationdrones">Conservation Drones</a> dataset. Specifically, I downloaded <a href="https://lilablobssc.blob.core.windows.net/conservationdrones/TrainReal.zip">TrainReal.zip</a>. Each of these folders contains folders for the annotation .csv files for each video (annotations) and the individual .jpg frames in each video (images).</p>
<p>We're not afraid of real-world examples in ML-Mondays - I chose a particularly difficult computer vision problem. We'll see why ...</p>
<p>I unzipped the TrainReal data to a folder on my computer called <code>/media/marda/TWOTB/USGS/DATA/TrainReal</code>, which contains <code>images</code> (jpegs) and <code>labels</code> (annotations in csv format). Here is an example image. Notice it is infrared and therefore greyscale. That's ok- the models we use can cope with single-banded inputs</p>
<p><img src="/MLMONDAYS/blog/assets/0000000359_0000000000_0000000142.jpg" alt=""></p>
<p>The associated label data for this image is</p>
<pre><code class="hljs"><span class="hljs-number">142</span> <span class="hljs-number">2</span>   <span class="hljs-number">276</span> <span class="hljs-number">243</span> <span class="hljs-number">11</span>  <span class="hljs-number">12</span>  <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
<span class="hljs-number">142</span> <span class="hljs-number">4</span>   <span class="hljs-number">204</span> <span class="hljs-number">260</span> <span class="hljs-number">16</span>  <span class="hljs-number">13</span>  <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
<span class="hljs-number">142</span> <span class="hljs-number">5</span>   <span class="hljs-number">266</span> <span class="hljs-number">246</span> <span class="hljs-number">11</span>  <span class="hljs-number">10</span>  <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
<span class="hljs-number">142</span> <span class="hljs-number">108</span> <span class="hljs-number">424</span> <span class="hljs-number">136</span> <span class="hljs-number">11</span>  <span class="hljs-number">12</span>  <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
<span class="hljs-number">142</span> <span class="hljs-number">114</span> <span class="hljs-number">430</span> <span class="hljs-number">101</span> <span class="hljs-number">17</span>  <span class="hljs-number">21</span>  <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
<span class="hljs-number">142</span> <span class="hljs-number">115</span> <span class="hljs-number">429</span> <span class="hljs-number">121</span> <span class="hljs-number">11</span>  <span class="hljs-number">11</span>  <span class="hljs-number">0</span>   <span class="hljs-number">2</span>   <span class="hljs-number">0</span>   <span class="hljs-number">0</span>
</code></pre>
<p>This is the <a href="https://motchallenge.net/instructions/">MOT</a> annotation format, with the following columns:</p>
<pre><code class="hljs"><span class="hljs-selector-attr">[frame_number]</span>, <span class="hljs-selector-attr">[object_id]</span>, <span class="hljs-selector-attr">[x]</span>, <span class="hljs-selector-attr">[y]</span>, <span class="hljs-selector-attr">[w]</span>, <span class="hljs-selector-attr">[h]</span>, <span class="hljs-selector-attr">[class]</span>, <span class="hljs-selector-attr">[species]</span>, <span class="hljs-selector-attr">[occlusion]</span>, <span class="hljs-selector-attr">[noise]</span>
</code></pre>
<ul>
<li>class: 0 if animals, 1 if humans</li>
<li>species: -1: unknown, 0: human, 1: elephant, 2: lion, 3: giraffe, 4: dog, 5: crocodile, 6: hippo, 7: zebra, 8: rhino. 3 and 4 occur only in real data. 5, 6, 7, 8 occur only in synthetic data.</li>
<li>occlusion: 0 if there is no occlusion, 1 if there is an occlusion (i.e., either occluding or occluded) (note: intersection over union threshold of 0.3 used to assign * occlusion; more details in paper)</li>
<li>noise: 0 if there is no noise, 1 if there is noise (note: noise labels were interpolated from object locations in previous and next frames; for more than 4 consecutive frames without labels, no noise labels were included; more details in paper)</li>
</ul>
<p>So, based on this info, we have 6 lions in the scene, each only 10-20 pixels in size</p>
<h3><a class="anchor" aria-hidden="true" id="converting-between-label-formats"></a><a href="#converting-between-label-formats" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Converting between label formats</h3>
<p>The first thing we need to do is convert this csv format into one of</p>
<pre><code class="hljs"><span class="hljs-selector-attr">[filename]</span>, <span class="hljs-selector-attr">[width]</span>, <span class="hljs-selector-attr">[height]</span>, <span class="hljs-selector-attr">[class]</span>, <span class="hljs-selector-attr">[xmin]</span>, <span class="hljs-selector-attr">[ymin]</span>, <span class="hljs-selector-attr">[xmax]</span>, <span class="hljs-selector-attr">[ymax]</span>
</code></pre>
<p>which is perhaps slightly more standard for deep learning workflows. You could do this manually in excel, by using <code>w</code> and <code>h</code> to compute <code>xmax</code> and <code>ymax</code>, then convert the <code>frame_number</code> into a <code>filename</code>. In this case we'll use <code>species</code> as <code>class</code>.</p>
<p>Honestly, every dataset has its foibles and you have to <code>wrangle</code> the data into one form or another, so get used to it! The python library <code>pandas</code> can help a lot in these situations. I won't lie - this is tricky - as I said before, the data part of any data modeling project is just as hard - if not more so - than the 'modeling' part.</p>
<h3><a class="anchor" aria-hidden="true" id="combining-all-csv-files-into-one"></a><a href="#combining-all-csv-files-into-one" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Combining all csv files into one</h3>
<p>These are the libraries we'll need:</p>
<pre><code class="hljs"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-title">from</span> glob <span class="hljs-keyword">import</span> glob
<span class="hljs-keyword">import</span> os
</code></pre>
<p>This is the top level directory where all the annotation csv files are</p>
<pre><code class="hljs"><span class="hljs-attr">csv_folder</span> = <span class="hljs-string">'/media/marda/TWOTB/USGS/DATA/TrainReal/annotations'</span>
</code></pre>
<p>First we define empty lists to collate image file names, and to concatenate all label data into one list</p>
<pre><code class="hljs"><span class="hljs-attr">all_label_data</span> = []<span class="hljs-comment">; files = []</span>
</code></pre>
<p>We cycle through each csv file, read it in using pandas, and append it to <code>all_label_data</code>. Next we get the filename of the image that <code>id</code> (column zero) corresponds to. This is not an easy file naming convention to deal with ... all strings are forced to have the same length.</p>
<pre><code class="hljs"><span class="hljs-keyword">for</span> <span class="hljs-keyword">f</span> in csv_file<span class="hljs-variable">s:</span>
    dat = np.array(pd.read_csv(<span class="hljs-keyword">f</span>))
    all_label_data.<span class="hljs-keyword">append</span>(dat)
    # <span class="hljs-built_in">get</span> the <span class="hljs-keyword">file</span> name root
    tmp = <span class="hljs-keyword">f</span>.replace(<span class="hljs-string">'annotations'</span>, <span class="hljs-string">'images'</span>).replace(<span class="hljs-string">'.csv'</span>,<span class="hljs-string">''</span>)
    # construct filenames <span class="hljs-keyword">for</span> each annotation
    <span class="hljs-keyword">for</span> i in dat[:,<span class="hljs-number">0</span>]:
       <span class="hljs-keyword">if</span> i&lt;<span class="hljs-number">10</span>:
          <span class="hljs-keyword">files</span>.<span class="hljs-keyword">append</span>(tmp+os.sep+tmp.<span class="hljs-keyword">split</span>(os.sep)[-<span class="hljs-number">1</span>]+<span class="hljs-string">'_000000000'</span>+str(i)+<span class="hljs-string">'.jpg'</span>)
       elif i&lt;<span class="hljs-number">100</span>:
          <span class="hljs-keyword">files</span>.<span class="hljs-keyword">append</span>(tmp+os.sep+tmp.<span class="hljs-keyword">split</span>(os.sep)[-<span class="hljs-number">1</span>]+<span class="hljs-string">'_00000000'</span>+str(i)+<span class="hljs-string">'.jpg'</span>)
       elif i&lt;<span class="hljs-number">1000</span>:
          <span class="hljs-keyword">files</span>.<span class="hljs-keyword">append</span>(tmp+os.sep+tmp.<span class="hljs-keyword">split</span>(os.sep)[-<span class="hljs-number">1</span>]+<span class="hljs-string">'_0000000'</span>+str(i)+<span class="hljs-string">'.jpg'</span>)
       elif i&lt;<span class="hljs-number">10000</span>:
          <span class="hljs-keyword">files</span>.<span class="hljs-keyword">append</span>(tmp+os.sep+tmp.<span class="hljs-keyword">split</span>(os.sep)[-<span class="hljs-number">1</span>]+<span class="hljs-string">'_000000'</span>+str(i)+<span class="hljs-string">'.jpg'</span>)
       elif i&lt;<span class="hljs-number">100000</span>:
          <span class="hljs-keyword">files</span>.<span class="hljs-keyword">append</span>(tmp+os.sep+tmp.<span class="hljs-keyword">split</span>(os.sep)[-<span class="hljs-number">1</span>]+<span class="hljs-string">'_00000'</span>+str(i)+<span class="hljs-string">'.jpg'</span>)

</code></pre>
<p>We use numpy's <code>vstack</code> to concatenate the list of lists into a numpy array with the correct shape (samples x columns)</p>
<pre><code class="hljs">all_label_data = np.vstack(all_label_data)
files = np.vstack(files).<span class="hljs-built_in">squeeze</span>()

<span class="hljs-keyword">print</span>(all_label_data.shape)
<span class="hljs-meta"># 87167 annotations, 10 columns</span>
<span class="hljs-keyword">print</span>(files.shape)
<span class="hljs-meta"># 87167 filenames, 1 column</span>
</code></pre>
<p>We have converted all the ids to filenames already, next we need to make xmaxs ymaxs</p>
<pre><code class="hljs"><span class="hljs-attr">xmax</span> = all_label_data[:,<span class="hljs-number">2</span>] + all_label_data[:,<span class="hljs-number">4</span>] <span class="hljs-comment">#xmin + width</span>
<span class="hljs-attr">ymax</span> = all_label_data[:,<span class="hljs-number">3</span>] + all_label_data[:,<span class="hljs-number">5</span>] <span class="hljs-comment">#ymin + height</span>
</code></pre>
<p>Next we map the integers to strings - strings are better in general than integers for class identification</p>
<pre><code class="hljs"><span class="hljs-comment"># list of integers</span>
<span class="hljs-attr">classes</span> = all_label_data[:,<span class="hljs-number">7</span>]
<span class="hljs-comment"># mapping from integers to strings</span>
<span class="hljs-attr">class_dict</span> = {-<span class="hljs-number">1</span>:<span class="hljs-string">'unknown'</span>,<span class="hljs-number">0</span>: <span class="hljs-string">'human'</span>, <span class="hljs-number">1</span>:<span class="hljs-string">'elephant'</span>, <span class="hljs-number">2</span>:<span class="hljs-string">'lion'</span>, <span class="hljs-number">3</span>:<span class="hljs-string">'giraffe'</span>}
<span class="hljs-comment">#list of strings</span>
<span class="hljs-attr">classes_string</span> = [class_dict[i] for i in classes]
</code></pre>
<p>Make a pandas dataset so we can write it out to csv file</p>
<pre><code class="hljs">d = {<span class="hljs-string">'filename'</span>: files, <span class="hljs-string">'width'</span>: all_label_data[:,<span class="hljs-number">4</span>], <span class="hljs-string">'height'</span>: all_label_data[:,<span class="hljs-number">5</span>], <span class="hljs-string">'class'</span>: classes_string,
     <span class="hljs-string">'xmin'</span>: all_label_data[:,<span class="hljs-number">2</span>], <span class="hljs-string">'ymin'</span>: all_label_data[:,<span class="hljs-number">3</span>], <span class="hljs-string">'xmax'</span>: xmax, <span class="hljs-string">'ymax'</span>: ymax }
df = pd.<span class="hljs-symbol">DataFrame</span>(data=d)
</code></pre>
<p>Interrogate the columns:</p>
<pre><code class="hljs"><span class="hljs-selector-tag">df</span><span class="hljs-selector-class">.keys</span>()
</code></pre>
<p>Print the first few examples to screen:</p>
<pre><code class="hljs"><span class="hljs-selector-tag">df</span><span class="hljs-selector-class">.head</span>()
</code></pre>
<p>Print the last few examples to screen:</p>
<pre><code class="hljs"><span class="hljs-selector-tag">df</span><span class="hljs-selector-class">.tail</span>()
</code></pre>
<p>Write to file:</p>
<pre><code class="hljs">df.<span class="hljs-keyword">to</span><span class="hljs-constructor">_csv('<span class="hljs-params">conservationdrones_labels</span>.<span class="hljs-params">csv</span>')</span>
</code></pre>
<p>Much better! All labels are in one file, that is more manageable and easier to read (in fact, it is stand-alone)</p>
<h3><a class="anchor" aria-hidden="true" id="writing-data-to-a-single-tfrecord"></a><a href="#writing-data-to-a-single-tfrecord" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Writing data to a single TFRecord</h3>
<p>Define some paths and inputs</p>
<pre><code class="hljs"><span class="hljs-attr">root</span> = <span class="hljs-string">'data/conservationdrones'</span>+os.sep
<span class="hljs-attr">output_path</span> = root+<span class="hljs-string">'conservationdrones.tfrecord'</span>
<span class="hljs-attr">csv_input</span> = root+<span class="hljs-string">'conservationdrones_labels.csv'</span>
</code></pre>
<p>Initiate a <code>TFRecordWriter</code> object that will write the TFRecords</p>
<pre><code class="hljs"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-title">writer</span> = tf.io.<span class="hljs-type">TFRecordWriter</span>(output_path)
</code></pre>
<p>Each image has variable number of annotations, so just like in the SECORRA example, we split the data into groups based on filename</p>
<pre><code class="hljs">examples = pd.read_csv(csv_input)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">'Number of labels: %i'</span> % len(examples)</span></span>)
grouped = split(examples, <span class="hljs-string">'filename'</span>)
</code></pre>
<p>How many images?</p>
<pre><code class="hljs"><span class="hljs-attribute">nb_images</span>=len(grouped)
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">'Number of images: %i'</span> % nb_images)
</code></pre>
<p>We need a function like <code>create_tf_example_coco</code> that creates a bytestring from an image and associated boundng box</p>
<p>The following function differs from <code>create_tf_example_coco</code> in that filename paths need not be specified and concatenated, and that <code>class_dict = {'unknown':-1,'human':0,'elephant':1, 'lion':2, 'giraffe':3}</code> is usd to convert the strings back to integers</p>
<pre><code class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_tf_example_conservationdrones</span><span class="hljs-params">(group)</span>:</span>
    <span class="hljs-string">"""
    create_tf_example_conservationdrones(group)
    ""
    This function creates an example tfrecord consisting of an image and label encoded as bytestrings
    The jpeg image is read into a bytestring, and the bbox coordinates and classes are collated and
    converted also
    INPUTS:
        * group [pandas dataframe group object]
        * path [tensorflow dataset]: training dataset
    OPTIONAL INPUTS: None
    OUTPUTS:
        * tf_example [tf.train.Example object]
    GLOBAL INPUTS: BATCH_SIZE
    """</span>
    <span class="hljs-keyword">with</span> tf.io.gfile.GFile(group.filename, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> fid:
        encoded_jpg = fid.read()
    encoded_jpg_io = io.BytesIO(encoded_jpg)

    filename = group.filename.encode(<span class="hljs-string">'utf8'</span>)

    ids = []
    areas = []
    xmins = [] ; xmaxs = []; ymins = []; ymaxs = []
    labels = []
    is_crowds = []

    <span class="hljs-comment">#for converting back to integer</span>
    class_dict = {<span class="hljs-string">'unknown'</span>:<span class="hljs-number">-1</span>,<span class="hljs-string">'human'</span>:<span class="hljs-number">0</span>,<span class="hljs-string">'elephant'</span>:<span class="hljs-number">1</span>, <span class="hljs-string">'lion'</span>:<span class="hljs-number">2</span>, <span class="hljs-string">'giraffe'</span>:<span class="hljs-number">3</span>}

    <span class="hljs-keyword">for</span> index, row <span class="hljs-keyword">in</span> group.object.iterrows():
        labels.append(class_dict[row[<span class="hljs-string">'class'</span>]])
        ids.append(index)
        xmins.append(row[<span class="hljs-string">'xmin'</span>])
        ymins.append(row[<span class="hljs-string">'ymin'</span>])
        xmaxs.append(row[<span class="hljs-string">'xmax'</span>])
        ymaxs.append(row[<span class="hljs-string">'ymax'</span>])
        areas.append((row[<span class="hljs-string">'xmax'</span>]-row[<span class="hljs-string">'xmin'</span>])*(row[<span class="hljs-string">'ymax'</span>]-row[<span class="hljs-string">'ymin'</span>]))
        is_crowds.append(<span class="hljs-literal">False</span>)

    tf_example = tf.train.Example(features=tf.train.Features(feature={
        <span class="hljs-string">'objects/is_crowd'</span>: int64_list_feature(is_crowds),
        <span class="hljs-string">'image/filename'</span>: bytes_feature(filename),
        <span class="hljs-string">'image/id'</span>: int64_list_feature(ids),
        <span class="hljs-string">'image'</span>: bytes_feature(encoded_jpg),
        <span class="hljs-string">'objects/xmin'</span>: float_list_feature(xmins), <span class="hljs-comment">#xs</span>
        <span class="hljs-string">'objects/xmax'</span>: float_list_feature(xmaxs), <span class="hljs-comment">#xs</span>
        <span class="hljs-string">'objects/ymin'</span>: float_list_feature(ymins), <span class="hljs-comment">#xs</span>
        <span class="hljs-string">'objects/ymax'</span>: float_list_feature(ymaxs), <span class="hljs-comment">#xs</span>
        <span class="hljs-string">'objects/area'</span>: float_list_feature(areas), <span class="hljs-comment">#ys</span>
        <span class="hljs-string">'objects/id'</span>: int64_list_feature(ids), <span class="hljs-comment">#ys</span>
        <span class="hljs-string">'objects/label'</span>: int64_list_feature(labels),
    }))

    <span class="hljs-keyword">return</span> tf_example
</code></pre>
<p>Now we can write out each group (bounding boxes of each image)</p>
<pre><code class="hljs"><span class="hljs-keyword">for</span><span class="hljs-built_in"> group </span><span class="hljs-keyword">in</span> grouped:
    tf_example = create_tf_example_conservationdrones(group)
    writer.write(tf_example.SerializeToString())
</code></pre>
<p>Close the writer  - we are done!</p>
<pre><code class="hljs">writer.<span class="hljs-built_in">close</span>()
output_path = <span class="hljs-built_in">os</span>.<span class="hljs-built_in">path</span>.join(<span class="hljs-built_in">os</span>.getcwd(), output_path)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Successfully created the TFRecords: {}'</span>.<span class="hljs-built_in">format</span>(output_path))
</code></pre>
<p>This is a big file (1.2 GB). But not as big as the 42,684 individual files that this one file contains, which is 2.3 GB</p>
<p>How do we split this big TFRecord up in small chunks?</p>
<h3><a class="anchor" aria-hidden="true" id="writing-data-to-multiple-tfrecords"></a><a href="#writing-data-to-multiple-tfrecords" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Writing data to multiple TFRecords</h3>
<p>This time we'll create smaller files with 1000 examples per file</p>
<pre><code class="hljs"><span class="hljs-attr">ims_per_shard</span> = <span class="hljs-number">1000</span>
</code></pre>
<p>How many individual files would we make?</p>
<pre><code class="hljs">SHARDS = <span class="hljs-built_in">int</span>(nb_images / ims_per_shard) + (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> nb_images % ims_per_shard != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)
print(SHARDS)

shared_size = <span class="hljs-built_in">int</span>(np.ceil(<span class="hljs-number">1.0</span> * nb_images / SHARDS))
print(shared_size)
</code></pre>
<p>Create indices into grouped that will enable writing SHARDS files, each containing shared_size examples</p>
<pre><code class="hljs">grouped_forshards = np<span class="hljs-class">.<span class="hljs-keyword">lib</span>.<span class="hljs-title">stride_tricks</span>.<span class="hljs-title">as_strided</span>(<span class="hljs-title">np</span>.<span class="hljs-title">arange</span>(<span class="hljs-title">len</span>(<span class="hljs-title">grouped</span>)), (<span class="hljs-title">SHARDS</span>, <span class="hljs-title">shared_size</span>))</span>
</code></pre>
<p>Write out each group to a different TFRecord file. Update a counter to increment the file name.</p>
<pre><code class="hljs">counter= 0
<span class="hljs-keyword">for</span> indices <span class="hljs-keyword">in</span> grouped_forshards[:-1]:

    tmp = [] #create a new list containing only data <span class="hljs-keyword">in</span> indices
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> indices:
        tmp.append(grouped[i])

    # modify the original filepath <span class="hljs-keyword">in</span> a consistent way
    output_path = root+<span class="hljs-string">'conservationdrones.tfrecord'</span>
    output_path = output_path.replace(<span class="hljs-string">'.tfrecord'</span>,<span class="hljs-string">''</span>)+ <span class="hljs-string">"{:02d}-{}.tfrec"</span>.format(counter, shared_size)
    writer = tf.io.TFRecordWriter(output_path)

    # write out each example <span class="hljs-keyword">to</span> the shard
    <span class="hljs-keyword">for</span><span class="hljs-built_in"> group </span><span class="hljs-keyword">in</span> tmp:
        tf_example = create_tf_example_conservationdrones(group)
        writer.write(tf_example.SerializeToString())

    writer.close()
    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">'Successfully created the TFRecords: {}'</span>.format(output_path))

    counter += 1
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="how-do-you-know-you-did-it-right"></a><a href="#how-do-you-know-you-did-it-right" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>How do you know you did it right?</h3>
<p>You should read the data back in and plot it. Get a list of the tfrec files you made:</p>
<pre><code class="hljs">filenames = sorted(<span class="hljs-name">tf</span>.io.gfile.glob(<span class="hljs-name">os</span>.getcwd()+os.sep+root+'*.tfrec'))
</code></pre>
<p>This dictionary and associated parsing function is what you need to decode</p>
<pre><code class="hljs">features = {
    'image': tf.io.<span class="hljs-constructor">FixedLenFeature([], <span class="hljs-params">tf</span>.<span class="hljs-params">string</span>, <span class="hljs-params">default_value</span>='')</span>,
    'objects/xmin': tf.io.<span class="hljs-constructor">FixedLenSequenceFeature([], <span class="hljs-params">tf</span>.<span class="hljs-params">float32</span>, <span class="hljs-params">allow_missing</span>=True)</span>,
    'objects/ymin': tf.io.<span class="hljs-constructor">FixedLenSequenceFeature([], <span class="hljs-params">tf</span>.<span class="hljs-params">float32</span>,<span class="hljs-params">allow_missing</span>=True)</span>,
    'objects/xmax': tf.io.<span class="hljs-constructor">FixedLenSequenceFeature([], <span class="hljs-params">tf</span>.<span class="hljs-params">float32</span>,<span class="hljs-params">allow_missing</span>=True)</span>,
    'objects/ymax': tf.io.<span class="hljs-constructor">FixedLenSequenceFeature([], <span class="hljs-params">tf</span>.<span class="hljs-params">float32</span>,<span class="hljs-params">allow_missing</span>=True)</span>,
    'objects/label': tf.io.<span class="hljs-constructor">FixedLenSequenceFeature([], <span class="hljs-params">tf</span>.<span class="hljs-params">int64</span>,<span class="hljs-params">allow_missing</span>=True)</span>,
}

def <span class="hljs-constructor">_parse_function(<span class="hljs-params">example_proto</span>)</span>:
  # Parse the input `tf.train.Example` proto using the dictionary above.
  return tf.io.parse<span class="hljs-constructor">_single_example(<span class="hljs-params">example_proto</span>, <span class="hljs-params">features</span>)</span>
</code></pre>
<p>Create a <code>TFRecordDataset</code> object from the list of files, and using the parsing function to create the dataset</p>
<pre><code class="hljs"><span class="hljs-attr">dataset</span> = tf.data.TFRecordDataset(filenames)
<span class="hljs-attr">dataset</span> = dataset.map(_parse_function)
</code></pre>
<p>Import a plotting library</p>
<pre><code class="hljs"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
</code></pre>
<p>Get ten images and their associated labels and plot them</p>
<pre><code class="hljs">for i <span class="hljs-keyword">in</span> dataset.take(<span class="hljs-number">10</span>):
    image = tf.image.decode<span class="hljs-constructor">_jpeg(<span class="hljs-params">i</span>['<span class="hljs-params">image</span>'], <span class="hljs-params">channels</span>=1)</span>
    bbox = tf.numpy<span class="hljs-constructor">_function(<span class="hljs-params">np</span>.<span class="hljs-params">array</span>,[[<span class="hljs-params">i</span>[<span class="hljs-string">"objects/xmin"</span>], <span class="hljs-params">i</span>[<span class="hljs-string">"objects/ymin"</span>], <span class="hljs-params">i</span>[<span class="hljs-string">"objects/xmax"</span>], <span class="hljs-params">i</span>[<span class="hljs-string">"objects/ymax"</span>]]], <span class="hljs-params">tf</span>.<span class="hljs-params">float32</span>)</span>.numpy<span class="hljs-literal">()</span>.T#.squeeze<span class="hljs-literal">()</span>
    print(len(bbox))

    ids = <span class="hljs-literal">[]</span>
    for id <span class="hljs-keyword">in</span> i<span class="hljs-literal">["<span class="hljs-identifier">objects</span><span class="hljs-operator">/</span><span class="hljs-identifier">label</span>"]</span>.numpy<span class="hljs-literal">()</span>:
       ids.append(class_dict<span class="hljs-literal">[<span class="hljs-identifier">id</span>]</span>)

    fig =plt.figure(figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>))
    plt.axis(<span class="hljs-string">"off"</span>)
    plt.imshow(image, cmap=plt.cm.gray)
    ax = plt.gca<span class="hljs-literal">()</span>

    for box,id <span class="hljs-keyword">in</span> zip(bbox,ids):
        x1, y1, x2, y2 = box
        w, h = x2 - x1, y2 - y1
        patch = plt.<span class="hljs-constructor">Rectangle([<span class="hljs-params">x1</span>, <span class="hljs-params">y1</span>], <span class="hljs-params">w</span>, <span class="hljs-params">h</span>, <span class="hljs-params">fill</span>=False, <span class="hljs-params">edgecolor</span>=[0, 1, 0], <span class="hljs-params">linewidth</span>=1)</span>
        ax.add<span class="hljs-constructor">_patch(<span class="hljs-params">patch</span>)</span>
        ax.text(x1, y1, id, bbox={<span class="hljs-string">"facecolor"</span>: <span class="hljs-literal">[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]</span>, <span class="hljs-string">"alpha"</span>: <span class="hljs-number">0.4</span>}, clip_box=ax.clipbox, clip_on=True, fontsize=<span class="hljs-number">5</span>)
    plt.show<span class="hljs-literal">()</span>
</code></pre>
<p>Here's an example output. Two tiny(!) elephants in scene:</p>
<p><img src="/MLMONDAYS/blog/assets/conservationdrone_label.png" alt=""></p>
<h3><a class="anchor" aria-hidden="true" id="last-word-"></a><a href="#last-word-" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>last word ...</h3>
<p>Datasets like this are extremely difficult to acquire and in my view are just as valuable as other types of scholarly output, so I encourage you to publish your datasets (if they are <a href="https://www.go-fair.org/fair-principles/">F.A.I.R</a>) and cite other's data. If you use this dataset, please consider citing the paper:</p>
<blockquote>
<p>Bondi E, Jain R, Aggrawal P, Anand S, Hannaford R, Kapoor A, Piavis J, Shah S, Joppa L, Dilkina B, Tambe M. BIRDSAI: A Dataset for Detection and Tracking in Aerial Thermal Infrared Videos.</p>
</blockquote>
<p>Thanks to the authors for making it publicly available.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/10/03/blog-post">ML terminology, demystified</a></h1><p class="post-meta">October 3, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><h3><a class="anchor" aria-hidden="true" id="deep-learning-dl"></a><a href="#deep-learning-dl" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Deep Learning (DL)</h3>
<p>Use of large (or deep) neural networks. The primary advantage is the use of many layers that carry out automated feature extraction, rather than rules-based feature extraction, or feature engineering (research into pipelines that extract the optimal features).</p>
<h3><a class="anchor" aria-hidden="true" id="artificial-intelligence-ai"></a><a href="#artificial-intelligence-ai" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Artificial Intelligence (AI)</h3>
<p>I use this term to mean artificial general intelligence (machines that write their own programs from scratch, based on a comprehension of the task at hand). This doesn't exist in the Earth and Life sciences yet! In my opinion, conflating ML with AI is confusing and counter-productive, and it leaves no room for growth.</p>
<h3><a class="anchor" aria-hidden="true" id="model"></a><a href="#model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model</h3>
<p>A model of the data, if you will, or the way that data generates labels, to be more specific. The representation of what a machine learning system has learned from the training data.</p>
<p>Within TensorFlow, and Machine Learning more widely, the term <code>model</code> is overloaded:</p>
<ul>
<li>The model technique/class or the specific python object (such as a Tensorflow graph) that expresses the structure of how a prediction will be computed.</li>
<li>The particular parameters (e.g. weights and biases) of that model, which are determined by training.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="regularization"></a><a href="#regularization" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Regularization</h3>
<p>The penalty on a model's complexity. Regularization helps prevent overfitting. Different kinds of regularization include:</p>
<ul>
<li>L2 regularization: model penalties are squared magnitudes (model fits to the mean)</li>
<li>L1 regularization: model penalties are absolute magnitudes (model fits to the median)</li>
<li>dropout regularization</li>
<li>early stopping (this is not a formal regularization method, but can effectively limit overfitting)</li>
<li>data augmentation (again, not a formal regularization method)</li>
<li>batch normalization (see above)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="data-augmentation"></a><a href="#data-augmentation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data augmentation</h3>
<p>The process of creating transformed versions of the data. This process often, but not always, duplicates data. It's primary purpose is regularization, by providing alternative versions of the data, it learns feature representations that are invariant to location, scale, translation, etc. In ML Mondays we use augmentation in a few different ways; in Part 1 we create an augmentation pipeline that augments imagery without duplicating it, and we create and write augmented imagery to disk in Part 3.</p>
<h3><a class="anchor" aria-hidden="true" id="batch-normalization"></a><a href="#batch-normalization" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch Normalization</h3>
<p>Normalizing the input or output of the activation functions in a hidden layer. The normalization is slightly different each time, because it is dictated by the distribution of values in the batch. So therefore its effects are contingent in part to the size of the batch. Batch normalization can provide the following benefits:</p>
<ul>
<li>Make neural networks more stable by protecting against outlier weights.</li>
<li>Enable higher learning rates, which in turn promotes faster training</li>
<li>Reduce overfitting.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="dropout-regularization"></a><a href="#dropout-regularization" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dropout Regularization</h3>
<p>A form of regularization useful in training neural networks. Dropout regularization works by removing (actually, zeroing out temporarily) a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization.</p>
<h3><a class="anchor" aria-hidden="true" id="early-stopping"></a><a href="#early-stopping" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Early Stopping</h3>
<p>A method for regularization that involves ending model training before training loss finishes decreasing. In early stopping, you end model training when the loss on a validation dataset starts to increase, that is, when generalization performance worsens.</p>
<h3><a class="anchor" aria-hidden="true" id="epoch"></a><a href="#epoch" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Epoch</h3>
<p>A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch size training iterations, where N is the total number of examples.</p>
<h3><a class="anchor" aria-hidden="true" id="convergence"></a><a href="#convergence" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Convergence</h3>
<p>Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.</p>
<h3><a class="anchor" aria-hidden="true" id="convolutional-neural-network"></a><a href="#convolutional-neural-network" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Convolutional Neural Network</h3>
<p>A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some alternate combinations of</p>
<ul>
<li>convolutional layers</li>
<li>pooling layers</li>
</ul>
<p>Followed by application of</p>
<ul>
<li>dense (or fully connected) layers</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="cross-entropy"></a><a href="#cross-entropy" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Cross-Entropy</h3>
<p>A measure of the difference between two probability distributions. A generalization of Log Loss to multi-class classification problems.</p>
<h3><a class="anchor" aria-hidden="true" id="discriminative-model"></a><a href="#discriminative-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Discriminative model</h3>
<p>A model that predicts labels from a set of one or more features. More formally, discriminative models define the conditional probability of an output given the features and weights; that is:</p>
<blockquote>
<p>p(output | features, weights)</p>
</blockquote>
<p>The vast majority of supervised learning models, including classification and regression models, are discriminative models.</p>
<h3><a class="anchor" aria-hidden="true" id="hyperparameter"></a><a href="#hyperparameter" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hyperparameter</h3>
<p>The &quot;knobs&quot; that you can tweak during successive runs of training a model. For example, learning rate is a hyperparameter. Contrast with parameter, which is learned by the model during training</p>
<h3><a class="anchor" aria-hidden="true" id="loss-or-cost"></a><a href="#loss-or-cost" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Loss (or cost)</h3>
<p>A measure of how far a model's predictions are from its label. Or, a measure of how bad the model is. To determine this value, a model must define a loss function.</p>
<h3><a class="anchor" aria-hidden="true" id="one-vs-all"></a><a href="#one-vs-all" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>one-vs.-all</h3>
<p>Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate binary classifiers—one binary classifier for each possible outcome. For example, given a model that classifies examples as animal, vegetable, or mineral, a one-vs.-all solution would provide the following three separate binary classifiers:</p>
<ul>
<li>animal vs. not animal</li>
<li>vegetable vs. not vegetable</li>
<li>mineral vs. not mineral</li>
</ul>
<p>this is the approach we take in segmentation.</p>
<h3><a class="anchor" aria-hidden="true" id="optimizer-numerical-solver"></a><a href="#optimizer-numerical-solver" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Optimizer (&quot;numerical solver&quot;)</h3>
<p>A specific implementation of the gradient descent algorithm. TensorFlow's base class for optimizers is <code>tf.train.Optimizer</code>. Popular optimizers include:</p>
<ul>
<li>AdaGrad, which stands for ADAptive GRADient descent.</li>
<li>Adam, which stands for ADAptive with Momentum (note that <code>rsmprop</code> is adam without momentum)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="supervised-machine-learning"></a><a href="#supervised-machine-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Supervised machine learning</h3>
<p>Training a model from input data and its corresponding labels. Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers. After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with unsupervised machine learning.</p>
<h3><a class="anchor" aria-hidden="true" id="semi-supervised-learning"></a><a href="#semi-supervised-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Semi-supervised learning</h3>
<p>Training a model on data where some of the training examples have labels but others don’t. One technique for semi-supervised learning is to infer labels for the unlabeled examples, and then to train on the inferred labels to create a new model. Semi-supervised learning can be useful if labels are expensive to obtain but unlabeled examples are plentiful.</p>
<h3><a class="anchor" aria-hidden="true" id="unsupervised-learning"></a><a href="#unsupervised-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Unsupervised learning</h3>
<p>Training a model to discover both the classes, and which class each sample represents. The 'holy grail' of Machine Learning, in many ways.</p>
<h3><a class="anchor" aria-hidden="true" id="transfer-learning"></a><a href="#transfer-learning" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transfer learning</h3>
<p>Transferring information from one machine learning task to another. For example, in multi-task learning, a single model solves multiple tasks, such as a deep model that has different output nodes for different tasks. Transfer learning might involve transferring knowledge from the solution of a simpler task to a more complex one, or involve transferring knowledge from a task where there is more data to one where there is less data.</p>
<p>Most machine learning systems solve a single task. Transfer learning is a baby step towards artificial intelligence in which a single program can solve multiple tasks.</p>
<h3><a class="anchor" aria-hidden="true" id="tensor"></a><a href="#tensor" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Tensor</h3>
<p>The primary data structure in TensorFlow programs. Tensors are N-dimensional (where N could be very large) data structures, most commonly scalars, vectors, or matrices. The elements of a Tensor can hold integer, floating-point, or string values.</p>
<h3><a class="anchor" aria-hidden="true" id="gpu-determinism"></a><a href="#gpu-determinism" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>GPU &quot;Determinism&quot;</h3>
<p>There is a lot of random parts to training a deep learning model, such as batches, augmentation, regularization, etc. When you run operations on several parallel threads, you typically do not know which thread will end first. It is not important when threads operate on their own data, so for example, applying an activation function to a tensor should be deterministic. But when those threads need to synchronize, such as when you compute a sum, then the result may depend on the order of the summation.</p>
<p>See this <a href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/2020/09/15/blog-post">ML Mondays blog post</a> for what we are doing in this course to counter these effects</p>
<h3><a class="anchor" aria-hidden="true" id="eager-execution"></a><a href="#eager-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>&quot;Eager execution&quot;</h3>
<p>A TensorFlow programming environment in which operations run immediately. By contrast, operations called in graph execution don't run until they are explicitly evaluated. Eager execution is an imperative interface, much like the code in most programming languages. Eager execution programs are generally far easier to debug than graph execution programs.</p>
<h3><a class="anchor" aria-hidden="true" id="graph-execution"></a><a href="#graph-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>&quot;Graph execution&quot;</h3>
<p>The opposite of eager execution. A TensorFlow programming environment in which the program first constructs a graph and then executes all or part of that graph. Graph execution was the default execution mode in TensorFlow 1.x. A give-away is use of <code>tf.session</code></p>
<p>Adapted from entries in <a href="https://developers.google.com/machine-learning/glossary">https://developers.google.com/machine-learning/glossary</a></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/10/01/blog-post">Creating a Tensorflow Dataset for an image segmentation task</a></h1><p class="post-meta">October 1, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><h3><a class="anchor" aria-hidden="true" id="use-case"></a><a href="#use-case" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use case</h3>
<p>You have a folder called <code>imdir</code> that folder contains tens to millions of jpeg files, and another, <code>lab_path</code> that contains tens to millions of corresponding label images, also as jpeg files. Label images are 8-bit, and encode labels as unique integer pixel values.</p>
<p>For the code to work, the images and label images need to be jpegs. On linux with the <code>convert</code> function from imagemagick, convert folder of pngs into jpegs</p>
<pre><code class="hljs"><span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> *.png
<span class="hljs-keyword">do</span>
convert <span class="hljs-variable">$file</span> $<span class="hljs-string">"<span class="hljs-variable">${file%.png}</span>.jpg"</span>
<span class="hljs-keyword">done</span>
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="create-a-data-pre-processing-pipeline"></a><a href="#create-a-data-pre-processing-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Create a data pre-processing pipeline</h3>
<p>Define a directory where you want to save your TFrecord files, called <code>tfrecord_dir</code></p>
<p>Get a list of the <code>images</code> in <code>imdir</code>, get the number of images <code>nb_images</code>, and compute the size of each shard and the number of shards</p>
<pre><code class="hljs">images = <span class="hljs-keyword">tf</span>.io.gfile.<span class="hljs-built_in">glob</span>(imdir+os.sep+<span class="hljs-string">'*.jpg'</span>)

nb_images=<span class="hljs-built_in">len</span>(<span class="hljs-keyword">tf</span>.io.gfile.<span class="hljs-built_in">glob</span>(imdir+os.sep+<span class="hljs-string">'*.jpg'</span>))

SHARDS = <span class="hljs-keyword">int</span>(nb_images / ims_per_shard) + (<span class="hljs-number">1</span> <span class="hljs-keyword">if</span> nb_images % ims_per_shard != <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>)

shared_size = <span class="hljs-keyword">int</span>(np.<span class="hljs-built_in">ceil</span>(<span class="hljs-number">1.0</span> * nb_images / SHARDS))s, <span class="hljs-string">'filename'</span>)
</code></pre>
<p>Next, define a pre-processing workflow</p>
<pre><code class="hljs"><span class="hljs-attr">dataset</span> = tf.data.Dataset.list_files(imdir+os.sep+<span class="hljs-string">'*.jpg'</span>, seed=<span class="hljs-number">10000</span>) <span class="hljs-comment"># This also shuffles the images</span>
<span class="hljs-attr">dataset</span> = dataset.map(read_image_and_label)
<span class="hljs-attr">dataset</span> = dataset.map(resize_and_crop_image, num_parallel_calls=AUTO)
<span class="hljs-attr">dataset</span> = dataset.map(recompress_image, num_parallel_calls=AUTO)
<span class="hljs-attr">dataset</span> = dataset.batch(shared_size)
</code></pre>
<p>Where the following function reads an image from file, and finds the corresponding label image and reads that in too (this example from the OBX dataset)</p>
<pre><code class="hljs">#-----------------------------------
def read_image_and_label(img_path):
    <span class="hljs-string">""</span><span class="hljs-comment">"</span>
    <span class="hljs-string">"read_image_and_label(img_path)"</span>
    This <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">reads</span> <span class="hljs-title">an</span> <span class="hljs-title">image</span> <span class="hljs-title">and</span> <span class="hljs-title">label</span> <span class="hljs-title">and</span> <span class="hljs-title">decodes</span> <span class="hljs-title">both</span> <span class="hljs-title">jpegs</span></span>
    into bytestring arrays.
    This works by parsing out the label image filename from its image pair
    Thre are different rules <span class="hljs-keyword">for</span> non-augmented versus augmented imagery
    INPUTS:
        * img_path [tensor <span class="hljs-built_in">string</span>]
    OPTIONAL INPUTS: None
    GLOBAL INPUTS: None
    OUTPUTS:
        * image [bytestring]
        * label [bytestring]
    <span class="hljs-string">""</span><span class="hljs-comment">"</span>
    bits = <span class="hljs-keyword">tf</span>.io.read_file(img_path)
    image = <span class="hljs-keyword">tf</span>.image.decode_jpeg(bits)

    # have <span class="hljs-keyword">to</span> use this <span class="hljs-keyword">tf</span>.strings.regex_replace utility because img_path <span class="hljs-keyword">is</span> <span class="hljs-keyword">a</span> Tensor object
    lab_path = <span class="hljs-keyword">tf</span>.strings.regex_replace(img_path, <span class="hljs-string">"images"</span>, <span class="hljs-string">"labels"</span>)
    lab_path = <span class="hljs-keyword">tf</span>.strings.regex_replace(lab_path, <span class="hljs-string">".jpg"</span>, <span class="hljs-string">"_deep_whitewater_shallow_no_water_label.jpg"</span>)
    lab_path = <span class="hljs-keyword">tf</span>.strings.regex_replace(lab_path, <span class="hljs-string">"augimage"</span>, <span class="hljs-string">"auglabel"</span>)
    bits = <span class="hljs-keyword">tf</span>.io.read_file(lab_path)
    label = <span class="hljs-keyword">tf</span>.image.decode_jpeg(bits)

    <span class="hljs-keyword">return</span> image, label
</code></pre>
<p>The following function crops and image and label pair to square and resizes to a <code>TARGET_SIZE</code></p>
<pre><code class="hljs">#-----------------------------------
def resize_and_crop_image(image, label):
    <span class="hljs-string">""</span><span class="hljs-comment">"</span>
    <span class="hljs-string">"resize_and_crop_image"</span>
    This <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">crops</span> <span class="hljs-title">to</span> <span class="hljs-title">square</span> <span class="hljs-title">and</span> <span class="hljs-title">resizes</span> <span class="hljs-title">an</span> <span class="hljs-title">image</span> <span class="hljs-title">and</span> <span class="hljs-title">label</span></span>
    INPUTS:
        * image [tensor array]
        * label [tensor array]
    OPTIONAL INPUTS: None
    GLOBAL INPUTS: TARGET_SIZE
    OUTPUTS:
        * image [tensor array]
        * label [tensor array]
    <span class="hljs-string">""</span><span class="hljs-comment">"</span>
    <span class="hljs-keyword">w</span> = <span class="hljs-keyword">tf</span>.shape(image)[<span class="hljs-number">0</span>]
    h = <span class="hljs-keyword">tf</span>.shape(image)[<span class="hljs-number">1</span>]
    tw = TARGET_SIZE
    <span class="hljs-keyword">th</span> = TARGET_SIZE
    resize_crit = (<span class="hljs-keyword">w</span> * <span class="hljs-keyword">th</span>) / (h * tw)
    image = <span class="hljs-keyword">tf</span>.cond(resize_crit &lt; <span class="hljs-number">1</span>,
                  lambd<span class="hljs-variable">a:</span> <span class="hljs-keyword">tf</span>.image.<span class="hljs-keyword">resize</span>(image, [<span class="hljs-keyword">w</span>*tw/<span class="hljs-keyword">w</span>, h*tw/<span class="hljs-keyword">w</span>]), # <span class="hljs-keyword">if</span> true
                  lambd<span class="hljs-variable">a:</span> <span class="hljs-keyword">tf</span>.image.<span class="hljs-keyword">resize</span>(image, [<span class="hljs-keyword">w</span>*<span class="hljs-keyword">th</span>/h, h*<span class="hljs-keyword">th</span>/h])  # <span class="hljs-keyword">if</span> false
                 )
    nw = <span class="hljs-keyword">tf</span>.shape(image)[<span class="hljs-number">0</span>]
    nh = <span class="hljs-keyword">tf</span>.shape(image)[<span class="hljs-number">1</span>]
    image = <span class="hljs-keyword">tf</span>.image.crop_to_bounding_box(image, (nw - tw) // <span class="hljs-number">2</span>, (nh - <span class="hljs-keyword">th</span>) // <span class="hljs-number">2</span>, tw, <span class="hljs-keyword">th</span>)

    label = <span class="hljs-keyword">tf</span>.cond(resize_crit &lt; <span class="hljs-number">1</span>,
                  lambd<span class="hljs-variable">a:</span> <span class="hljs-keyword">tf</span>.image.<span class="hljs-keyword">resize</span>(label, [<span class="hljs-keyword">w</span>*tw/<span class="hljs-keyword">w</span>, h*tw/<span class="hljs-keyword">w</span>]), # <span class="hljs-keyword">if</span> true
                  lambd<span class="hljs-variable">a:</span> <span class="hljs-keyword">tf</span>.image.<span class="hljs-keyword">resize</span>(label, [<span class="hljs-keyword">w</span>*<span class="hljs-keyword">th</span>/h, h*<span class="hljs-keyword">th</span>/h])  # <span class="hljs-keyword">if</span> false
                 )
    label = <span class="hljs-keyword">tf</span>.image.crop_to_bounding_box(label, (nw - tw) // <span class="hljs-number">2</span>, (nh - <span class="hljs-keyword">th</span>) // <span class="hljs-number">2</span>, tw, <span class="hljs-keyword">th</span>)

    <span class="hljs-keyword">return</span> image, label
</code></pre>
<p>This function takes images and labels as byte strings and recodes as a 8bit jpeg</p>
<pre><code class="hljs">#-----------------------------------
def recompress_image(image, <span class="hljs-keyword">label</span>):
    <span class="hljs-string">""</span>"
    <span class="hljs-string">"recompress_image"</span>
    This function takes <span class="hljs-keyword">an</span> image and <span class="hljs-keyword">label</span> encoded <span class="hljs-keyword">as</span> a byte string
    and recodes <span class="hljs-keyword">as</span> <span class="hljs-keyword">an</span> 8-bit jpeg
    INPUTS:
<span class="hljs-comment">        * image [tensor array]</span>
<span class="hljs-comment">        * label [tensor array]</span>
    OPTIONAL INPUTS: None
    <span class="hljs-keyword">GLOBAL</span> INPUTS: None
    OUTPUTS:
<span class="hljs-comment">        * image [tensor array]</span>
<span class="hljs-comment">        * label [tensor array]</span>
    <span class="hljs-string">""</span>"
    image = tf.cast(image, tf.uint8)
    image = tf.image.encode_jpeg(image, optimize_size=True, chroma_downsampling=False)

    <span class="hljs-keyword">label</span> = tf.cast(<span class="hljs-keyword">label</span>, tf.uint8)
    <span class="hljs-keyword">label</span> = tf.image.encode_jpeg(<span class="hljs-keyword">label</span>, optimize_size=True, chroma_downsampling=False)
    <span class="hljs-keyword">return</span> image, <span class="hljs-keyword">label</span>
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="write-tfrecord-shards-to-disk"></a><a href="#write-tfrecord-shards-to-disk" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Write TFrecord shards to disk</h3>
<p>Finally, write the dataset to tfrecord format files for 'analysis ready data' that is highly compressed and easy to share</p>
<pre><code class="hljs">for shard, (image, <span class="hljs-meta">label</span>) <span class="hljs-meta">in</span> enumerate(dataset):
  shard_size = image.numpy().shape[0]
  <span class="hljs-meta">filename</span> = tfrecord_dir+os.sep+<span class="hljs-string">"obx"</span> + <span class="hljs-string">"{:02d}-{}.tfrec"</span>.<span class="hljs-meta">format</span>(shard, shard_size)

  with tf.io.TFRecordWriter(<span class="hljs-meta">filename</span>) <span class="hljs-meta">as</span> out_file:
    for i <span class="hljs-meta">in</span><span class="hljs-meta"> range(</span>shard_size):
      example = to_seg_tfrecord(image.numpy()[i],<span class="hljs-meta">label</span>.numpy()[i])
      out_file.write(example.SerializeToString())
    p<span class="hljs-meta">rint(</span><span class="hljs-string">"Wrote file {} containing {} records"</span>.<span class="hljs-meta">format</span>(<span class="hljs-meta">filename</span>, shard_size))
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="image-augmentation"></a><a href="#image-augmentation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Image augmentation</h3>
<p>Do you need to augment your imagery (create transformations of the image and label pairs and write them to disk)?</p>
<p>Define image dimensions</p>
<pre><code class="hljs"><span class="hljs-attr">NY</span> = <span class="hljs-number">7360</span>
<span class="hljs-attr">NX</span> = <span class="hljs-number">4912</span>
</code></pre>
<p>Define two <code>ImageDataGenerator</code> instances with the same arguments, one for images and the other for labels (masks)</p>
<pre><code class="hljs">data_gen_args = dict(<span class="hljs-attribute">featurewise_center</span>=<span class="hljs-literal">False</span>,
                     <span class="hljs-attribute">featurewise_std_normalization</span>=<span class="hljs-literal">False</span>,
                     <span class="hljs-attribute">rotation_range</span>=5,
                     <span class="hljs-attribute">width_shift_range</span>=0.1,
                     <span class="hljs-attribute">height_shift_range</span>=0.1,
                     <span class="hljs-attribute">zoom_range</span>=0.2)
image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(*<span class="hljs-number">*da</span>ta_gen_args)
mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(*<span class="hljs-number">*da</span>ta_gen_args)
</code></pre>
<p>You likely can't read all your images into memory, so you'll have to do this in batches. The following loop will grab a new random batch of images, apply the augmentation generator to the images to create alternate versions, then writes those alternate versions of images and labels to disk</p>
<pre><code class="hljs"><span class="hljs-attribute">i</span>=1
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(14):

    #<span class="hljs-builtin-name">set</span> a different seed each time <span class="hljs-keyword">to</span> <span class="hljs-builtin-name">get</span> a new batch of ten
    seed = int(np.random.randint(0,100,<span class="hljs-attribute">size</span>=1))
    img_generator = image_datagen.flow_from_directory(
            imdir,
            target_size=(NX, NY),
            <span class="hljs-attribute">batch_size</span>=10,
            <span class="hljs-attribute">class_mode</span>=None, <span class="hljs-attribute">seed</span>=seed, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>)

    #the seed must be the same as <span class="hljs-keyword">for</span> the training <span class="hljs-builtin-name">set</span> <span class="hljs-keyword">to</span> <span class="hljs-builtin-name">get</span> the same images
    mask_generator = mask_datagen.flow_from_directory(
            lab_path,
            target_size=(NX, NY),
            <span class="hljs-attribute">batch_size</span>=10,
            <span class="hljs-attribute">class_mode</span>=None, <span class="hljs-attribute">seed</span>=seed, <span class="hljs-attribute">shuffle</span>=<span class="hljs-literal">True</span>)

    #The following merges the two generators (<span class="hljs-keyword">and</span> their flows) together:
    train_generator = (pair <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> zip(img_generator, mask_generator))

    #grab a batch of 10 images <span class="hljs-keyword">and</span> label images
    x, y = next(train_generator)

    # write them <span class="hljs-keyword">to</span> file <span class="hljs-keyword">and</span> increment the counter
    <span class="hljs-keyword">for</span> im,lab <span class="hljs-keyword">in</span> zip(x,y):
        imwrite(imdir+os.sep+<span class="hljs-string">'augimage_000'</span>+str(i)+<span class="hljs-string">'.jpg'</span>, im)
        imwrite(lab_path+os.sep+<span class="hljs-string">'auglabel_000'</span>+str(i)+<span class="hljs-string">'_deep_whitewater_shallow_no_water_label.jpg'</span>, lab)
        i += 1

    #save memory
    del x, y, im, lab
    #<span class="hljs-builtin-name">get</span> a new batch
</code></pre>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/09/15/blog-post">Making workflows reproducible</a></h1><p class="post-meta">September 15, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><p>Neural networks need randomization to effectively train, so this will always be a stochastic (rather than deterministic) process and loss curves will therefore always be different each time. Metrics like accuracy may also differ significantly.</p>
<p>However, there are some measures that can be taken that collectively attempt to ensure consistency in training.</p>
<h3><a class="anchor" aria-hidden="true" id="using-tfrecords"></a><a href="#using-tfrecords" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using TFRecords</h3>
<p>One of the motivations for using TFRecords for data is to ensure a consistency in what images get allocated as training ad which get allocated as validation. These images are already randomized, and are not randomized further during training</p>
<h3><a class="anchor" aria-hidden="true" id="use-deterministic-operations"></a><a href="#use-deterministic-operations" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use deterministic operations</h3>
<p>CuDNN does not guarantee reproducibility in some of its routines on GPUs, so it is not &quot;deterministic&quot;. The operating system environment variable &quot;TF_DETERMINISTIC_OPS&quot; can be used to control this behaviour. Setting the environment variable <code>TF_CUDNN_DETERMINISM=1</code> forces the selection of deterministic cuDNN convolution and max-pooling algorithms. When this is enabled, the algorithm selection procedure itself is also deterministic. However, selecting these deterministic options may reduce performance.</p>
<pre><code class="hljs"><span class="hljs-built_in">os</span>.environ[<span class="hljs-string">"TF_DETERMINISTIC_OPS"</span>] = <span class="hljs-string">"1"</span>
</code></pre>
<p>When we create a batched data set, we can set the <code>option_no_order.experimental_deterministic</code> to <code>True</code></p>
<pre><code class="hljs"><span class="hljs-attr">option_no_order</span> = tf.data.Options()
<span class="hljs-attr">option_no_order.experimental_deterministic</span> = <span class="hljs-literal">True</span>

<span class="hljs-attr">dataset</span> = tf.data.Dataset.list_files(filenames)
<span class="hljs-attr">dataset</span> = dataset.with_options(option_no_order)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="use-a-seed-for-random-number-generation"></a><a href="#use-a-seed-for-random-number-generation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use a seed for random number generation</h3>
<p>Use a seed value and substantiate <code>np.random.seed</code> and <code>tf.random.set_seed</code> with it, which will subsequently apply to any numpy operations that involved random numbers</p>
<pre><code class="hljs"><span class="hljs-built_in">SEED</span>=<span class="hljs-number">42</span>
np.random.<span class="hljs-built_in">seed</span>(<span class="hljs-built_in">SEED</span>)
tf.random.set_seed(<span class="hljs-built_in">SEED</span>)
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="where-possible-use-a-larger-batch-size"></a><a href="#where-possible-use-a-larger-batch-size" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Where possible, use a larger batch size</h2>
<p>Larger batch sizes tend to promote more stable validation loss curves. This is usually only possible with relatively large hardware, because large batches mean larger amounts of GPU memory required.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/09/01/blog-post">Creating a Tensorflow Dataset for an image recognition task</a></h1><p class="post-meta">September 1, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><h3><a class="anchor" aria-hidden="true" id="use-case"></a><a href="#use-case" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Use case</h3>
<p>You have a folder called <code>data</code>, in which there are two additional folders <code>train</code> and <code>test</code>. The <code>train</code> folder contains hundreds to millions of jpeg files, each with a descriptive label in the file name. The <code>test</code> folder contains samples for which we do not yet know the label (we will use our trained model to estimate that). These are jpeg images without any descriptive label in the file name.</p>
<h3><a class="anchor" aria-hidden="true" id="problem"></a><a href="#problem" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Problem:</h3>
<p>We use a GPU (or TPU) to train Deep Neural Networks. To use such 'accelerated hardware' most efficiently, you should feed data fast enough to keep them busy. <em>If your data stored as thousands of individual files (i.e. images), you may not be utilizing your GPU at maximum throughput.</em></p>
<h3><a class="anchor" aria-hidden="true" id="solution"></a><a href="#solution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Solution:</h3>
<p>You should split your data across several larger files, and stream from multiple files in parallel</p>
<p>For maintaining efficient high-throughput for GPU computation</p>
<p>Tensorflow has strategies for this scenario:</p>
<ol>
<li>First, we batch the numerous small files in a TFRecord file</li>
<li>Then, we use the power of <code>tf.data.Dataset</code> to read from multiple files in parallel.</li>
</ol>
<p>The TFRecord file format is a record-oriented binary format. If your input data are on disk or working with large data then TensorFlow recommended using TFRecord format. You get a significant impact on the performance of your input pipeline. Binary data takes less space on disk, takes less time to copy and can be read more efficiently from disk.</p>
<pre><code class="hljs"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-title">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-title">from</span> matplotlib.image <span class="hljs-keyword">import</span> imread
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os, json, random, requests, glob, math
</code></pre>
<p><code>tf.data.experimental.AUTOTUNE</code>, or simply <code>AUTO</code> is used in the <code>tf.data.Dataset API</code> as a strategy for efficient use of your hardware. It will prompt the <code>tf.data</code> runtime to tune the value dynamically at runtime. This will be used here in the following contexts:</p>
<ol>
<li>set and optimize the number of parallel calls for functions that apply transformations to imagery (cropping, resizing, etc), using <code>dataset.map</code> calls</li>
<li>Set and optimize the number of parallel calls for functions that load data into memory while models train, using <code>dataset.interleave</code> calls</li>
<li>Set and optimize the number of parallel calls for functions that load data into asynchronously memory while models train, using <code>dataset.prefetch</code> calls. These use a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested.</li>
</ol>
<p>You could familiarize yourself with these topic by following <a href="https://www.tensorflow.org/guide/data">this guide</a> and then <a href="https://www.tensorflow.org/guide/data_performance">this one</a></p>
<pre><code class="hljs">os<span class="hljs-selector-class">.environ</span>[<span class="hljs-string">"TF_DETERMINISTIC_OPS"</span>] = <span class="hljs-string">"1"</span>

SEED=<span class="hljs-number">42</span>
np<span class="hljs-selector-class">.random</span>.seed(SEED)

AUTO = tf<span class="hljs-selector-class">.data</span><span class="hljs-selector-class">.experimental</span><span class="hljs-selector-class">.AUTOTUNE</span>
</code></pre>
<p>You have a folder called <code>data</code>, in which there are two additional folders <code>train</code> and <code>test</code>. The <code>train</code> folder contains hundreds to millions of jpeg files, each with a descriptive label in the file name. The <code>test</code> folder contains samples for which we do not yet know the label (we will use our trained model to estimate that). These are jpeg images without any descriptive label in the file name.</p>
<p>In this example below, we have a folder of more than 250,000 images of cats and dogs. Each jpeg file has either 'cat' or 'dog' in the file name. This workflow would apply to situations where you had more than 2 classes, as I will explain in a later post (illustrated using a different data set)</p>
<h3><a class="anchor" aria-hidden="true" id="creating-tfrecords-of-images-and-discrete-labels"></a><a href="#creating-tfrecords-of-images-and-discrete-labels" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Creating TFRecords of images and discrete labels</h3>
<p>Discrete labels in this sense means two things:</p>
<ol>
<li>There is only one label per image (known as single-label classification, to distingusih from multi-label classification where there are many labels per image)</li>
<li>There is no overlap between labels, such that each label can be represented by a discrete number, i.e. an integer, counting up from zero.</li>
</ol>
<p>We first have to define some variables - how many individual TFRecords per dataset -- called <code>shards</code> -- do we wish to make? (16)</p>
<p>How large are the square images we will use, in pixels? (160)</p>
<p>What are the classes? (cat, and dog, as a list of binary strings). Note that the class labels must be binary strings (<code>b'binary string'</code>) rather than regular strngs (<code>'string'</code>)</p>
<pre><code class="hljs"><span class="hljs-attr">SHARDS</span> = <span class="hljs-number">16</span>
<span class="hljs-attr">TARGET_SIZE</span>=<span class="hljs-number">160</span>
<span class="hljs-attr">CLASSES</span> = [b<span class="hljs-string">'cat'</span>, b<span class="hljs-string">'dog'</span>]
</code></pre>
<p>Next we need to define the number of images in the train directory, <code>nb_images</code>, which is used to define how large each shard will be. Later, <code>nb_images</code> will be used for defining the number of model training and validation steps per epoch - more on that later.</p>
<pre><code class="hljs"><span class="hljs-attr">nb_images</span>=len(tf.io.gfile.glob(<span class="hljs-string">'data/train/*.jpg'</span>))

<span class="hljs-attr">shared_size</span> = math.ceil(<span class="hljs-number">1.0</span> * nb_images / SHARDS)
</code></pre>
<p>To create a TFRecord, we need a function that reads an image from a file, and strips the class name from the file name. This will be different for each dataset and therefore the below function would need modification for each new dataset. In the below, the <code>label</code> is extracted by first stripping the file separator out and removing all the file path before the file name (<code>label = tf.strings.split(img_path, sep='/')</code>), then taking everything before the dot (<code>label = tf.strings.split(label[-1], sep='.')</code>), then finally taking just the first item of the resulting list of file name parts <code>label[0]</code>. The image is simply read in as a binary string of bytes, then reconstructed into the jpeg format using the handy utility <code>tf.image.decode_jpeg</code>.</p>
<pre><code class="hljs">def read_image_and_label(img_path):

  bits = tf.io.read_file(img_path)
  <span class="hljs-built_in">image</span> = tf.<span class="hljs-built_in">image</span>.decode_jpeg(bits)

  <span class="hljs-built_in">label</span> = tf.strings.<span class="hljs-built_in">split</span>(img_path, sep='/')
  <span class="hljs-built_in">label</span> = tf.strings.<span class="hljs-built_in">split</span>(<span class="hljs-built_in">label</span>[-<span class="hljs-number">1</span>], sep='.')

  <span class="hljs-built_in">return</span> <span class="hljs-built_in">image</span>,<span class="hljs-built_in">label</span>[<span class="hljs-number">0</span>]
</code></pre>
<p>Neural nets often use imagery that is not at full resolution, due to memory limitations on GPUs. In this course, downsizing and cropping of imagery will be a fairly common practice. The function below carries out a resizing to the desired <code>TARGET_SIZE</code> (keeping track of which horizontal dimension is the largest), the crops to square</p>
<pre><code class="hljs">def resize_and_crop_image(<span class="hljs-built_in">image</span>, label):
  w = tf.<span class="hljs-built_in">shape</span>(<span class="hljs-built_in">image</span>)[<span class="hljs-number">0</span>]
  h = tf.<span class="hljs-built_in">shape</span>(<span class="hljs-built_in">image</span>)[<span class="hljs-number">1</span>]
  tw = TARGET_SIZE
  th = TARGET_SIZE
  resize_crit = (w * th) / (h * tw)
  <span class="hljs-built_in">image</span> = tf.cond(resize_crit &lt; <span class="hljs-number">1</span>,
                  lambda: tf.<span class="hljs-built_in">image</span>.resize(<span class="hljs-built_in">image</span>, [w*tw/w, h*tw/w]), # <span class="hljs-keyword">if</span> <span class="hljs-keyword">true</span>
                  lambda: tf.<span class="hljs-built_in">image</span>.resize(<span class="hljs-built_in">image</span>, [w*th/h, h*th/h])  # <span class="hljs-keyword">if</span> <span class="hljs-keyword">false</span>
                 )
  nw = tf.<span class="hljs-built_in">shape</span>(<span class="hljs-built_in">image</span>)[<span class="hljs-number">0</span>]
  nh = tf.<span class="hljs-built_in">shape</span>(<span class="hljs-built_in">image</span>)[<span class="hljs-number">1</span>]
  <span class="hljs-built_in">image</span> = tf.<span class="hljs-built_in">image</span>.crop_to_bounding_box(<span class="hljs-built_in">image</span>, (nw - tw) <span class="hljs-comment">// 2, (nh - th) // 2, tw, th)</span>
  <span class="hljs-keyword">return</span> <span class="hljs-built_in">image</span>, label
</code></pre>
<p>When a TFRecord is read back into memory, it is just a string of bytes, so the following function makes sure those bytes get encoded back into jpeg format (with no chroma subsampling)</p>
<pre><code class="hljs">def recompress_image(<span class="hljs-built_in">image</span>, <span class="hljs-built_in">label</span>):
  <span class="hljs-built_in">image</span> = tf.cast(<span class="hljs-built_in">image</span>, tf.uint8)
  <span class="hljs-built_in">image</span> = tf.<span class="hljs-built_in">image</span>.encode_jpeg(<span class="hljs-built_in">image</span>, optimize_size=True, chroma_downsampling=False)
  <span class="hljs-built_in">return</span> <span class="hljs-built_in">image</span>, <span class="hljs-built_in">label</span>
</code></pre>
<p>You should stuff data in a protocol buffer called <code>Example</code>. Example protocol buffer contains <code>Features</code>. The feature is a protocol to describe the data and could have three types: bytes (images), float (floating point labels), and int64 (discrete labels).</p>
<pre><code class="hljs">def <span class="hljs-constructor">_bytestring_feature(<span class="hljs-params">list_of_bytestrings</span>)</span>:
  return tf.train.<span class="hljs-constructor">Feature(<span class="hljs-params">bytes_list</span>=<span class="hljs-params">tf</span>.<span class="hljs-params">train</span>.BytesList(<span class="hljs-params">value</span>=<span class="hljs-params">list_of_bytestrings</span>)</span>)

def <span class="hljs-constructor">_int_feature(<span class="hljs-params">list_of_ints</span>)</span>: # <span class="hljs-built_in">int64</span>
  return tf.train.<span class="hljs-constructor">Feature(<span class="hljs-params">int64_list</span>=<span class="hljs-params">tf</span>.<span class="hljs-params">train</span>.Int64List(<span class="hljs-params">value</span>=<span class="hljs-params">list_of_ints</span>)</span>)

def <span class="hljs-constructor">_float_feature(<span class="hljs-params">list_of_floats</span>)</span>: # float32
  return tf.train.<span class="hljs-constructor">Feature(<span class="hljs-params">float_list</span>=<span class="hljs-params">tf</span>.<span class="hljs-params">train</span>.FloatList(<span class="hljs-params">value</span>=<span class="hljs-params">list_of_floats</span>)</span>)
</code></pre>
<p>We serialize the protocol buffer to a string and write it to a TFRecords files.</p>
<pre><code class="hljs">def <span class="hljs-keyword">to</span><span class="hljs-constructor">_tfrecord(<span class="hljs-params">img_bytes</span>, <span class="hljs-params">label</span>)</span>:  

  class_num = np.argmax(np.<span class="hljs-built_in">array</span>(CLASSES)==label)
  feature = {
      <span class="hljs-string">"image"</span>: <span class="hljs-constructor">_bytestring_feature([<span class="hljs-params">img_bytes</span>])</span>, # one image <span class="hljs-keyword">in</span> the <span class="hljs-built_in">list</span>
      <span class="hljs-string">"class"</span>: <span class="hljs-constructor">_int_feature([<span class="hljs-params">class_num</span>])</span>,        # one <span class="hljs-keyword">class</span> <span class="hljs-keyword">in</span> the <span class="hljs-built_in">list</span>      
  }
  return tf.train.<span class="hljs-constructor">Example(<span class="hljs-params">features</span>=<span class="hljs-params">tf</span>.<span class="hljs-params">train</span>.Features(<span class="hljs-params">feature</span>=<span class="hljs-params">feature</span>)</span>)

</code></pre>
<p>Get a list of files and shuffle them, then create a mapping to link them to the jpeg files so they can be read on the fly</p>
<pre><code class="hljs">dataset = tf.data.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Dataset</span>.</span></span><span class="hljs-built_in">list</span><span class="hljs-constructor">_files('<span class="hljs-params">data</span><span class="hljs-operator">/</span><span class="hljs-params">train</span><span class="hljs-operator">/</span><span class="hljs-operator">*</span>.<span class="hljs-params">jpg</span>', <span class="hljs-params">seed</span>=10000)</span> # This also shuffles the images
dataset = dataset.map(read_image_and_label)
</code></pre>
<p>Next, create a new mapping to the resize and crop function</p>
<pre><code class="hljs"><span class="hljs-attr">dataset</span> = dataset.<span class="hljs-built_in">map</span>(resize_and_crop_image, <span class="hljs-attr">num_parallel_calls=AUTO)</span>  
</code></pre>
<p>Finally, a mapping to the jpeg recompression function, and then set the <code>batch</code> which dictates how many images per shard</p>
<pre><code class="hljs"><span class="hljs-attr">dataset</span> = dataset.map(recompress_image, num_parallel_calls=AUTO)
<span class="hljs-attr">dataset</span> = dataset.batch(shared_size)
</code></pre>
<p>Okay, now the dataset is set up, we can begin writing the data to TFRecords. The following function oads image files, resizes them to a common size and then stores them across <code>NUM_SHARDS</code> TFRecord files. It reads from files in parallel and disregards the order of the data in favour of reading speed. Selecting each (random) pair of image and label sequentially, we call the <code>to_tfrecord</code> function we defined earlier to create the <code>example</code> record, then that is serialized to string and appended to the file. When the requisite number of images for a <code>shard</code> has been reached, a new <code>shard</code> is created.</p>
<pre><code class="hljs">for shard, (image, <span class="hljs-meta">label</span>) <span class="hljs-meta">in</span> enumerate(dataset):
  shard_size = image.numpy().shape[0]
  <span class="hljs-meta">filename</span> = <span class="hljs-string">"cat_dog"</span> + <span class="hljs-string">"{:02d}-{}.tfrec"</span>.<span class="hljs-meta">format</span>(shard, shard_size)

  with tf.io.TFRecordWriter(<span class="hljs-meta">filename</span>) <span class="hljs-meta">as</span> out_file:
    for i <span class="hljs-meta">in</span><span class="hljs-meta"> range(</span>shard_size):
      example = to_tfrecord(image.numpy()[i],<span class="hljs-meta">label</span>.numpy()[i])
      out_file.write(example.SerializeToString())
    p<span class="hljs-meta">rint(</span><span class="hljs-string">"Wrote file {} containing {} records"</span>.<span class="hljs-meta">format</span>(<span class="hljs-meta">filename</span>, shard_size))
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="preparing-for-model-training"></a><a href="#preparing-for-model-training" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Preparing for model training</h3>
<p>We now need some functions to read those TFRecords in and use them during model training. We wish to parallelize the data loading step as much as possible; individual images and labels are small, but there are many thousands of them, so we need to ensure both fast and also consistent rate of data throughput. We acheive this by interleaving the contents of the datasets. The number of datasets to overlap can be specified by the cycle_length argument (set to <code>16</code> here). This is where we also see many of the benefits of <code>tf.data.experimental.AUTOTUNE</code>, or simply <code>AUTO</code> is used in the <code>tf.data.Dataset API</code> to tune the value dynamically at runtime.</p>
<pre><code class="hljs">def get_batched_dataset(filenames):
  <span class="hljs-attr">option_no_order</span> = tf.data.Options()
  option_no_order.<span class="hljs-attr">experimental_deterministic</span> = False

  <span class="hljs-attr">dataset</span> = tf.data.Dataset.list_files(filenames)
  <span class="hljs-attr">dataset</span> = dataset.with_options(option_no_order)
  <span class="hljs-attr">dataset</span> = dataset.interleave(tf.data.TFRecordDataset, <span class="hljs-attr">cycle_length=16,</span> <span class="hljs-attr">num_parallel_calls=AUTO)</span>
  <span class="hljs-attr">dataset</span> = dataset.<span class="hljs-built_in">map</span>(read_tfrecord, <span class="hljs-attr">num_parallel_calls=AUTO)</span>

  <span class="hljs-attr">dataset</span> = dataset.cache() <span class="hljs-comment"># This dataset fits in RAM</span>
  <span class="hljs-attr">dataset</span> = dataset.repeat()
  <span class="hljs-attr">dataset</span> = dataset.shuffle(<span class="hljs-number">2048</span>)
  <span class="hljs-attr">dataset</span> = dataset.batch(BATCH_SIZE, <span class="hljs-attr">drop_remainder=True)</span> <span class="hljs-comment"># drop_remainder will be needed on TPU</span>
  <span class="hljs-attr">dataset</span> = dataset.prefetch(AUTO) <span class="hljs-comment">#</span>

  return dataset
</code></pre>
<p>We call that function for both training and validation subsets, that we will define below</p>
<pre><code class="hljs"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_training_dataset</span><span class="hljs-params">()</span></span>:
  <span class="hljs-keyword">return</span> get_batched_dataset(training_filenames)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_validation_dataset</span><span class="hljs-params">()</span></span>:
  <span class="hljs-keyword">return</span> get_batched_dataset(validation_filenames)
</code></pre>
<p>The following function will read an individual example (random image, label pair) in a TFRecord. Extract the <code>tf.train.Example</code> protocol buffer messages from a TFRecord-format file. Each <code>tf.train.Example</code> record contains one or more “features”, and the input pipeline typically converts these features into tensors.</p>
<pre><code class="hljs">def read_tfrecord(<span class="hljs-built_in">example</span>):
    <span class="hljs-built_in">features</span> = {
        <span class="hljs-string">"image"</span>: tf.io.FixedLenFeature([], tf.<span class="hljs-built_in">string</span>),  # tf.<span class="hljs-built_in">string</span> = bytestring (<span class="hljs-keyword">not</span> text <span class="hljs-built_in">string</span>)
        <span class="hljs-string">"class"</span>: tf.io.FixedLenFeature([], tf.int64),   # shape [] means <span class="hljs-built_in">scalar</span>
    }
    # decode the TFRecord
    <span class="hljs-built_in">example</span> = tf.io.parse_single_example(<span class="hljs-built_in">example</span>, <span class="hljs-built_in">features</span>)

    <span class="hljs-built_in">image</span> = tf.<span class="hljs-built_in">image</span>.decode_jpeg(<span class="hljs-built_in">example</span>['<span class="hljs-built_in">image</span>'], channels=<span class="hljs-number">3</span>)
    <span class="hljs-built_in">image</span> = tf.cast(<span class="hljs-built_in">image</span>, tf.float32) / <span class="hljs-number">255.0</span>
    <span class="hljs-built_in">image</span> = tf.reshape(<span class="hljs-built_in">image</span>, [TARGET_SIZE,TARGET_SIZE, <span class="hljs-number">3</span>])

    class_label = tf.cast(<span class="hljs-built_in">example</span>['class'], tf.int32)

    <span class="hljs-built_in">return</span> <span class="hljs-built_in">image</span>, class_label

</code></pre>
<p>Here we set the batch size. This is a hyperparameter (i.e. set by you, not by model training) and its value is dictated (for the most part) by GPU memory considerations. We are using small imagery, so we can fit a relatively large batch into memory at once. We'll go for something relatively high (&gt; say, 20)</p>
<pre><code class="hljs"><span class="hljs-attr">BATCH_SIZE</span> = <span class="hljs-number">32</span>
</code></pre>
<p>This bit of code just makes sure that the dataset will be read correctly from the TFRecord files. We find them all using <code>glob</code> pattern matching (using 'cat*.tfrec') to form an input dataset, then use the <code>.take()</code> command to grab <code>1</code> batch. Print the labels out to screen - they should be integers. We also print the image dimensions out to ensure they are correct</p>
<pre><code class="hljs"><span class="hljs-attribute">training_filenames</span>=tf.io.gfile.glob('cat*.tfrec')

train_ds = get_training_dataset()
<span class="hljs-keyword">for</span> imgs,lbls <span class="hljs-keyword">in</span> train_ds.take(1):
  <span class="hljs-builtin-name">print</span>(lbls)
  <span class="hljs-builtin-name">print</span>(imgs.shape)
</code></pre>
<p>We wrote all of our images out to TFRecords - we didn't split them into test and validation sets first. That gives us more flexibility to assign train/validation subsets (called <code>splits</code>) here. Below we define <code>VALIDATION_SPLIT</code>, which is the proportion of the total data that will be used for validation. The rest will be used for training. We grab the filenames (again). These are already shuffled, but we can shuffle yet again to ensure the images really do get drawn as randomly as possible from the deck. Then we define train and validation file lists based on the split.</p>
<pre><code class="hljs">VALIDATION_SPLIT = <span class="hljs-number">0.19</span>

filenames=<span class="hljs-keyword">tf</span>.io.gfile.<span class="hljs-built_in">glob</span>(<span class="hljs-string">'cat*.tfrec'</span>)

random.shuffle(filenames)
<span class="hljs-keyword">split</span> = <span class="hljs-keyword">int</span>(<span class="hljs-built_in">len</span>(filenames) * VALIDATION_SPLIT)

training_filenames = filenames[spli<span class="hljs-variable">t:</span>]
validation_filenames = filenames[:<span class="hljs-keyword">split</span>]
</code></pre>
<p>During model training, one epoch provides the model an opportunity to 'see' the entire dataset. So the number of steps per epoch is essentially the number of unique samples of your dataset divided by the batch size.</p>
<pre><code class="hljs">validation_steps = int(<span class="hljs-name">nb_images</span> // len(<span class="hljs-name">filenames</span>) * len(validation_filenames)) // BATCH_SIZE
steps_per_epoch = int(nb_images // len(filenames) * len(<span class="hljs-name">training_filenames</span>)) // BATCH_SIZE  
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="model-training-using-tfrecords"></a><a href="#model-training-using-tfrecords" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training using TFRecords</h3>
<p>To demonstrate training using this workflow, we choose a simple (so-called <code>vanilla</code>) model that we construct using a few convolutional filter blocks of increasing size, interspersed with <code>MaxPooling</code> layers, and finally a global pooling and a classifier head layer. This is very similar in design to dozens of examples you can find online using toy datasets such as this. This isn't necessarily the most optimal or powerful model for this or any other dataset, but it'll do fine for demonstration (and will actually likely to be close to optimal considering the relative simplicity of the data/problem)</p>
<pre><code class="hljs">model = tf.keras.Sequential([

    tf.keras.layers.Conv2D(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">filters</span>=16, <span class="hljs-attribute">padding</span>=<span class="hljs-string">'same'</span>, <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>, input_shape=[TARGET_SIZE,TARGET_SIZE, 3]),
    tf.keras.layers.Conv2D(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">filters</span>=32, <span class="hljs-attribute">padding</span>=<span class="hljs-string">'same'</span>, <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.MaxPooling2D(<span class="hljs-attribute">pool_size</span>=2),

    tf.keras.layers.Conv2D(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">filters</span>=64, <span class="hljs-attribute">padding</span>=<span class="hljs-string">'same'</span>, <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.MaxPooling2D(<span class="hljs-attribute">pool_size</span>=2),

    tf.keras.layers.Conv2D(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">filters</span>=128, <span class="hljs-attribute">padding</span>=<span class="hljs-string">'same'</span>, <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>),
    tf.keras.layers.MaxPooling2D(<span class="hljs-attribute">pool_size</span>=2),

    tf.keras.layers.Conv2D(<span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">filters</span>=256, <span class="hljs-attribute">padding</span>=<span class="hljs-string">'same'</span>, <span class="hljs-attribute">activation</span>=<span class="hljs-string">'relu'</span>),

    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.Dense(1,<span class="hljs-string">'sigmoid'</span>)
])
</code></pre>
<p>You must <code>.compile</code> your model before you train, giving it an optimizer, loss function and a metric to keep track of. The options below are fairly standard - we use <code>binary_crossentropy</code> -  'binary' because we only have two classes (otherwise you would choose 'categorical') and 'crossentropy' because this is an image recognition problem</p>
<pre><code class="hljs">model.compile(<span class="hljs-attribute">optimizer</span>=<span class="hljs-string">'Adam'</span>,
              <span class="hljs-attribute">loss</span>=<span class="hljs-string">'binary_crossentropy'</span>,
              metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
<p>Call <code>.fit()</code> to train the model</p>
<pre><code class="hljs">model.fit(get_training_dataset(), <span class="hljs-attribute">steps_per_epoch</span>=steps_per_epoch, <span class="hljs-attribute">epochs</span>=10,
                      <span class="hljs-attribute">validation_data</span>=get_validation_dataset(), <span class="hljs-attribute">validation_steps</span>=validation_steps)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="model-validation"></a><a href="#model-validation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model validation</h3>
<p>This little function will convert the integer label into a string label</p>
<pre><code class="hljs">get_label = lambda <span class="hljs-symbol">x</span> : <span class="hljs-string">'cat'</span> <span class="hljs-keyword">if</span> (<span class="hljs-symbol">x</span>==<span class="hljs-number">0</span>) <span class="hljs-keyword">else</span> <span class="hljs-string">'dog'</span>
</code></pre>
<p>Below we call the validation dataset (<code>ds=get_validation_dataset()</code>) and plot one (<code>ds.take(1)</code>) batch. The figure shows 8 x 4 example images, their actual labels and their model-predicted values</p>
<pre><code class="hljs">fig = plt.figure(figsize=(12,28))

<span class="hljs-attribute">cnt</span>=1

<span class="hljs-attribute">ds</span>=get_validation_dataset()

<span class="hljs-keyword">for</span> imgs,lbls <span class="hljs-keyword">in</span> ds.take(1):
  <span class="hljs-attribute">predicted_classes</span>=model.predict_classes(imgs)
  <span class="hljs-keyword">for</span> img,lbl,cl <span class="hljs-keyword">in</span> zip(imgs,lbls,predicted_classes):

    fig.add_subplot(8,4, cnt)
    plt.title(<span class="hljs-string">'obs: {} / est: {}'</span>.format(get_label(lbl),get_label(cl[0])))
    plt.imshow(img)
    plt.axis(<span class="hljs-string">'off'</span>)
    <span class="hljs-attribute">cnt</span>=cnt+1

</code></pre>
<p>We need a new function for model evaluation. This version creates batches, but doesn't <em>repeat</em> them (no <code>dataset.repeat()</code> command)</p>
<pre><code class="hljs">def get_eval_dataset(filenames):
  <span class="hljs-attr">option_no_order</span> = tf.data.Options()
  option_no_order.<span class="hljs-attr">experimental_deterministic</span> = False

  <span class="hljs-attr">dataset</span> = tf.data.Dataset.list_files(filenames)
  <span class="hljs-attr">dataset</span> = dataset.with_options(option_no_order)
  <span class="hljs-attr">dataset</span> = dataset.interleave(tf.data.TFRecordDataset, <span class="hljs-attr">cycle_length=16,</span> <span class="hljs-attr">num_parallel_calls=AUTO)</span>
  <span class="hljs-attr">dataset</span> = dataset.<span class="hljs-built_in">map</span>(read_tfrecord, <span class="hljs-attr">num_parallel_calls=AUTO)</span>

  <span class="hljs-attr">dataset</span> = dataset.cache() <span class="hljs-comment"># This dataset fits in RAM</span>
  <span class="hljs-attr">dataset</span> = dataset.shuffle(<span class="hljs-number">2048</span>)
  <span class="hljs-attr">dataset</span> = dataset.batch(BATCH_SIZE, <span class="hljs-attr">drop_remainder=True)</span> <span class="hljs-comment"># drop_remainder will be needed on TPU</span>
  <span class="hljs-attr">dataset</span> = dataset.prefetch(AUTO) <span class="hljs-comment">#</span>

  return dataset

def get_validation_eval_dataset():
  return get_eval_dataset(validation_filenames)
</code></pre>
<p>To get a global sense of the skill of the model, call <code>.evaluate</code> on the entire validation set, which will use the model for prediction on all validation images, then compare the predicted versus observed labels for each, with what <code>metrics</code> you used when you compiled the model before training (we used <code>accuracy</code>). Print the mean accuracy in percent to screen.</p>
<pre><code class="hljs">loss, accuracy = model.evaluate(get_validation_eval_dataset())
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">'Test Mean Accuracy: '</span>, round((accuracy)<span class="hljs-number">*100</span>, 2))
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="model-deployment"></a><a href="#model-deployment" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model deployment</h3>
<p>Apply to test (unseen) sample imagery - here I have limited to <code>BATCH_SIZE</code> number of images just for illustration</p>
<pre><code class="hljs">test_filenames = glob.glob(<span class="hljs-string">'data/test1/*.jpg'</span>)[:BATCH_SIZE]
<span class="hljs-function"><span class="hljs-title">len</span><span class="hljs-params">(test_filenames)</span></span>
</code></pre>
<p>For prediction on raw imagery (rather than pre-processed tensors in the TFRecords file), we need a resizing and converting and normalizing function</p>
<pre><code class="hljs"><span class="hljs-function">def <span class="hljs-title">preprocess_image</span><span class="hljs-params">(<span class="hljs-built_in">image</span>)</span>:
  <span class="hljs-built_in">image</span> </span>= tf.<span class="hljs-built_in">image</span>.resize(<span class="hljs-built_in">image</span>, (TARGET_SIZE, TARGET_SIZE))
  <span class="hljs-built_in">image</span> = tf.<span class="hljs-built_in">image</span>.convert_image_dtype(<span class="hljs-built_in">image</span>, tf.float32)
  <span class="hljs-built_in">image</span> = <span class="hljs-built_in">image</span>/<span class="hljs-number">255.</span>
  <span class="hljs-keyword">return</span> <span class="hljs-built_in">image</span>
</code></pre>
<p>Test using one image</p>
<pre><code class="hljs">im = preprocess<span class="hljs-constructor">_image(<span class="hljs-params">imread</span>(<span class="hljs-params">test_filenames</span>[13])</span>)
plt.imshow(im)
</code></pre>
<pre><code class="hljs">predicted_classes=model.predict<span class="hljs-constructor">_classes(<span class="hljs-params">np</span>.<span class="hljs-params">expand_dims</span>(<span class="hljs-params">im</span>,<span class="hljs-params">axis</span>=0)</span>)
get<span class="hljs-constructor">_label(<span class="hljs-params">predicted_classes</span>.<span class="hljs-params">squeeze</span>()</span>)
</code></pre>
<p>Test on a whole batch</p>
<pre><code class="hljs">imgs = <span class="hljs-literal">[]</span>
predicted_classes = <span class="hljs-literal">[]</span>
for f <span class="hljs-keyword">in</span> test_filenames:
  im = preprocess<span class="hljs-constructor">_image(<span class="hljs-params">imread</span>(<span class="hljs-params">f</span>)</span>)
  imgs.append(im)
  predicted_classes.append(<span class="hljs-built_in">int</span>(model.predict<span class="hljs-constructor">_classes(<span class="hljs-params">np</span>.<span class="hljs-params">expand_dims</span>(<span class="hljs-params">im</span>,<span class="hljs-params">axis</span>=0)</span>).squeeze<span class="hljs-literal">()</span>.astype('<span class="hljs-built_in">int</span>')))
</code></pre>
<p>Make a similar plot as before, but this time we only have the model predicted class, not the ground truth class</p>
<pre><code class="hljs">fig = plt.figure(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">28</span>))

cnt=<span class="hljs-number">1</span>
<span class="hljs-keyword">for</span> img,cl <span class="hljs-keyword">in</span> zip(imgs,predicted_classes):
  fig.add_subplot(<span class="hljs-number">8</span>,<span class="hljs-number">4</span>, cnt)
  plt.title(<span class="hljs-string">'est: {}'</span>.format(get_label(cl)))
  plt.imshow(img)
  plt.axis(<span class="hljs-string">'off'</span>)
  cnt=cnt+<span class="hljs-number">1</span>
</code></pre>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/08/17/blog-post">Converting between YOLO and PASCAL-VOC object recognition formats, and creating a Tensorflow Dataset</a></h1><p class="post-meta">August 17, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><p>This blog post walks through the (somewhat cumbersome - I won't lie!) process of converting between YOLO and PASCAL-VOC 'bounding box' annotation data formats for image recognition problems.</p>
<p>The files we create using <code>makesense.ai</code> and downloaded in <code>YOLO</code> format with the <code>.txt</code> extension can be converted to the <code>PASCAL-VOC</code> format with the <code>.xml</code> extension. This blog post shows you how to do that with python. I also show you how to convert to a generic csv format that is also sometimes used. Finally, I show you how to convert your <code>PASCAL-VOC</code> format data into a Tensorflow <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord">TFRecord</a> that use <a href="https://developers.google.com/protocol-buffers/">Protocol buffers</a>, which are a cross-platform, cross-language library for efficient serialization of structured data.</p>
<h3><a class="anchor" aria-hidden="true" id="resources-i-used"></a><a href="#resources-i-used" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Resources I used</h3>
<p><a href="https://www.tensorflow.org/datasets/add_dataset">These</a> Tensorflow instructions for how to add a dataset, as well as some more specific Tensorflow object detection <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/preparing_inputs.md">workflows</a>, and finally the Tensorflow <a href="https://github.com/tensorflow/models">Model Garden</a>, which you'll use here. <a href="https://github.com/datitran/raccoon_dataset">This</a> and <a href="https://towardsdatascience.com/object-detection-tensorflow-854c7eb65fa">this</a> gave some outdated advice that was nevertheless useful, if not used here. <a href="https://www.tensorflow.org/tutorials/load_data/tfrecord">This</a> provides more details on TFRecords and their usages.</p>
<h3><a class="anchor" aria-hidden="true" id="first-dealing-with-empty-imageryannotations"></a><a href="#first-dealing-with-empty-imageryannotations" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>First, dealing with 'empty' imagery/annotations</h3>
<p>We first need to make sure there is a txt file for every image. Any <em>missing</em> .txt files are for images with no annotations (i.e. no people). So, we create an empty txt file with the right name if it is missing.</p>
<p>We only need two libraries for this:</p>
<pre><code class="hljs"><span class="hljs-keyword">import</span> os, glob
</code></pre>
<p>And one <code>for-loop</code> that iterates through each folder of images (<code>test</code>, <code>train</code>, and <code>validation</code> in the example below). If a certain .txt file is missing, it simply creates an empty one</p>
<pre><code class="hljs"><span class="hljs-keyword">for</span> cond in [<span class="hljs-string">'test'</span>, <span class="hljs-string">'train'</span>,<span class="hljs-string">'validation'</span>]:

    jpg = <span class="hljs-built_in">glob</span>.<span class="hljs-built_in">glob</span>(cond+<span class="hljs-string">'/*.jpg'</span>)

    <span class="hljs-keyword">for</span> <span class="hljs-keyword">f</span> in jp<span class="hljs-variable">g:</span>
       file_query = <span class="hljs-keyword">f</span>.replace(<span class="hljs-string">'jpg'</span>,<span class="hljs-string">'txt'</span>).replace(cond, cond+<span class="hljs-string">'_labels'</span>)
       <span class="hljs-keyword">if</span> os.path.isfile(file_query):
          pass
       <span class="hljs-keyword">else</span>:
          <span class="hljs-keyword">print</span>(<span class="hljs-string">"Creating %s"</span> % (file_query))
          with <span class="hljs-keyword">open</span>(file_query, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> fp:
             pass
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="second-yolo-to-pascal-voc-format-conversion"></a><a href="#second-yolo-to-pascal-voc-format-conversion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Second, YOLO to PASCAL-VOC format conversion</h3>
<p>PASCAL-VOC is a very common object recognition data format, probably more common than the YOLO format. Many example workflows will use either one of these two formats. Here we convert YOLO (<code>.txt</code>) format to PASCAL-VOC (<code>.xml</code>).</p>
<p>Let's set up the problem. Define an <code>IMG_PATH</code> containing jpg images (in the example below, called <code>test</code>), and a corresponding folder containing the associated .txt files (called <code>test_labels</code> below). This is what my file paths look like on my Linux box:</p>
<pre><code class="hljs"><span class="hljs-attr">IMG_PATH</span> = <span class="hljs-string">"/media/marda/TWOTB/USGS/SOFTWARE/MLMONDAYS/2_ObjRecog/test"</span>

<span class="hljs-comment"># txt_folder is txt file root that using makesense.ai rectbox</span>
<span class="hljs-attr">txt_folder</span> = <span class="hljs-string">"/media/marda/TWOTB/USGS/SOFTWARE/MLMONDAYS/2_ObjRecog/test_labels"</span>
</code></pre>
<p>We define a list of labels. We only have one label, <code>person</code></p>
<pre><code class="hljs">fw = os.listdir(IMG_PATH)
<span class="hljs-comment"># path of save xml file</span>
save_path = <span class="hljs-string">''</span> <span class="hljs-comment"># keep it blank</span>

labels = [<span class="hljs-string">'person'</span>]
global <span class="hljs-keyword">label</span><span class="bash">
label = <span class="hljs-string">''</span></span>
</code></pre>
<p>Some utilities:</p>
<pre><code class="hljs">def csvread(fn):
    <span class="hljs-keyword">with</span> <span class="hljs-keyword">open</span>(fn, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> csvfile:
        list_arr = []
        reader = csv.reader(csvfile, delimiter=<span class="hljs-string">' '</span>)
        <span class="hljs-keyword">for</span> <span class="hljs-keyword">row</span> <span class="hljs-keyword">in</span> reader:
            list_arr.append(<span class="hljs-keyword">row</span>)
    <span class="hljs-keyword">return</span> list_arr

<span class="hljs-keyword">def</span> convert_label(txt_file):
    <span class="hljs-keyword">global</span> label
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-keyword">range</span>(<span class="hljs-keyword">len</span>(labels)):
        <span class="hljs-keyword">if</span> txt_file[<span class="hljs-number">0</span>] == <span class="hljs-keyword">str</span>(i):
            label = labels[i]
            <span class="hljs-keyword">return</span> label
    <span class="hljs-keyword">return</span> label
</code></pre>
<p>This is the code that extract the info from a YOLO record:</p>
<pre><code class="hljs">def extract_coor(txt_file, img_width, img_height):
    x_rect_mid = <span class="hljs-built_in">float</span>(txt_file[<span class="hljs-number">1</span>])
    y_rect_mid = <span class="hljs-built_in">float</span>(txt_file[<span class="hljs-number">2</span>])
    width_rect = <span class="hljs-built_in">float</span>(txt_file[<span class="hljs-number">3</span>])
    height_rect = <span class="hljs-built_in">float</span>(txt_file[<span class="hljs-number">4</span>])

    x_min_rect = ((<span class="hljs-number">2</span> * x_rect_mid * img_width) - (width_rect * img_width)) / <span class="hljs-number">2</span>
    x_max_rect = ((<span class="hljs-number">2</span> * x_rect_mid * img_width) + (width_rect * img_width)) / <span class="hljs-number">2</span>
    y_min_rect = ((<span class="hljs-number">2</span> * y_rect_mid * img_height) -
                  (height_rect * img_height)) / <span class="hljs-number">2</span>
    y_max_rect = ((<span class="hljs-number">2</span> * y_rect_mid * img_height) +
                  (height_rect * img_height)) / <span class="hljs-number">2</span>

    <span class="hljs-keyword">return</span> x_min_rect, x_max_rect, y_min_rect, y_max_rect
</code></pre>
<p>Loop through each file (in <code>fw</code>) and carry out the conversion, writing one <code>xml</code> format file for each <code>txt</code> file you have</p>
<pre><code class="hljs">for line <span class="hljs-keyword">in</span> fw:
    <span class="hljs-attr">root</span> = etree.Element(<span class="hljs-string">"annotation"</span>)

    <span class="hljs-comment"># try debug to check your path</span>
    <span class="hljs-attr">img_style</span> = IMG_PATH.split('/')[-<span class="hljs-number">1</span>]
    <span class="hljs-attr">img_name</span> = line
    <span class="hljs-attr">image_info</span> = IMG_PATH + <span class="hljs-string">"/"</span> + line
    <span class="hljs-attr">img_txt_root</span> = txt_folder + <span class="hljs-string">"/"</span> + line[:-<span class="hljs-number">4</span>]
    <span class="hljs-comment"># print(img_txt_root)</span>
    <span class="hljs-attr">txt</span> = <span class="hljs-string">".txt"</span>

    <span class="hljs-attr">txt_path</span> = img_txt_root + txt
    <span class="hljs-comment"># print(txt_path)</span>
    <span class="hljs-attr">txt_file</span> = csvread(txt_path)

    <span class="hljs-comment"># read the image  information</span>
    <span class="hljs-attr">img_size</span> = Image.open(image_info).size

    <span class="hljs-attr">img_width</span> = img_size[<span class="hljs-number">0</span>]
    <span class="hljs-attr">img_height</span> = img_size[<span class="hljs-number">1</span>]
    <span class="hljs-attr">img_depth</span> = Image.open(image_info).layers

    <span class="hljs-attr">folder</span> = etree.Element(<span class="hljs-string">"folder"</span>)
    folder.<span class="hljs-attr">text</span> = <span class="hljs-string">"%s"</span> % (img_style)

    <span class="hljs-attr">filename</span> = etree.Element(<span class="hljs-string">"filename"</span>)
    filename.<span class="hljs-attr">text</span> = <span class="hljs-string">"%s"</span> % (img_name)

    <span class="hljs-attr">path</span> = etree.Element(<span class="hljs-string">"path"</span>)
    path.<span class="hljs-attr">text</span> = <span class="hljs-string">"%s"</span> % (IMG_PATH)

    <span class="hljs-attr">source</span> = etree.Element(<span class="hljs-string">"source"</span>)

    <span class="hljs-attr">source_database</span> = etree.SubElement(source, <span class="hljs-string">"database"</span>)
    source_database.<span class="hljs-attr">text</span> = <span class="hljs-string">"Unknown"</span>

    <span class="hljs-attr">size</span> = etree.Element(<span class="hljs-string">"size"</span>)
    <span class="hljs-attr">image_width</span> = etree.SubElement(size, <span class="hljs-string">"width"</span>)
    image_width.<span class="hljs-attr">text</span> = <span class="hljs-string">"%d"</span> % (img_width)

    <span class="hljs-attr">image_height</span> = etree.SubElement(size, <span class="hljs-string">"height"</span>)
    image_height.<span class="hljs-attr">text</span> = <span class="hljs-string">"%d"</span> % (img_height)

    <span class="hljs-attr">image_depth</span> = etree.SubElement(size, <span class="hljs-string">"depth"</span>)
    image_depth.<span class="hljs-attr">text</span> = <span class="hljs-string">"%d"</span> % (img_depth)

    <span class="hljs-attr">segmented</span> = etree.Element(<span class="hljs-string">"segmented"</span>)
    segmented.<span class="hljs-attr">text</span> = <span class="hljs-string">"0"</span>

    root.append(folder)
    root.append(filename)
    root.append(path)
    root.append(source)
    root.append(size)
    root.append(segmented)

    for ii <span class="hljs-keyword">in</span> range(len(txt_file)):
        <span class="hljs-attr">label</span> = convert_label(txt_file[ii][<span class="hljs-number">0</span>])
        x_min_rect, x_max_rect, y_min_rect, <span class="hljs-attr">y_max_rect</span> = extract_coor(
            txt_file[ii], img_width, img_height)

        <span class="hljs-attr">object</span> = etree.Element(<span class="hljs-string">"object"</span>)
        <span class="hljs-attr">name</span> = etree.SubElement(object, <span class="hljs-string">"name"</span>)
        name.<span class="hljs-attr">text</span> = <span class="hljs-string">"%s"</span> % (label)

        <span class="hljs-attr">pose</span> = etree.SubElement(object, <span class="hljs-string">"pose"</span>)
        pose.<span class="hljs-attr">text</span> = <span class="hljs-string">"Unspecified"</span>

        <span class="hljs-attr">truncated</span> = etree.SubElement(object, <span class="hljs-string">"truncated"</span>)
        truncated.<span class="hljs-attr">text</span> = <span class="hljs-string">"0"</span>

        <span class="hljs-attr">difficult</span> = etree.SubElement(object, <span class="hljs-string">"difficult"</span>)
        difficult.<span class="hljs-attr">text</span> = <span class="hljs-string">"0"</span>

        <span class="hljs-attr">bndbox</span> = etree.SubElement(object, <span class="hljs-string">"bndbox"</span>)
        <span class="hljs-attr">xmin</span> = etree.SubElement(bndbox, <span class="hljs-string">"xmin"</span>)
        xmin.<span class="hljs-attr">text</span> = <span class="hljs-string">"%d"</span> % (x_min_rect)
        <span class="hljs-attr">ymin</span> = etree.SubElement(bndbox, <span class="hljs-string">"ymin"</span>)
        ymin.<span class="hljs-attr">text</span> = <span class="hljs-string">"%d"</span> % (y_min_rect)
        <span class="hljs-attr">xmax</span> = etree.SubElement(bndbox, <span class="hljs-string">"xmax"</span>)
        xmax.<span class="hljs-attr">text</span> = <span class="hljs-string">"%d"</span> % (x_max_rect)
        <span class="hljs-attr">ymax</span> = etree.SubElement(bndbox, <span class="hljs-string">"ymax"</span>)
        ymax.<span class="hljs-attr">text</span> = <span class="hljs-string">"%d"</span> % (y_max_rect)

        root.append(object)

    <span class="hljs-attr">file_output</span> = etree.tostring(root, <span class="hljs-attr">pretty_print=True,</span> <span class="hljs-attr">encoding='UTF-8')</span>
    <span class="hljs-comment"># print(file_output.decode('utf-8'))</span>
    <span class="hljs-attr">ff</span> = open('%s.xml' % (img_name[:-<span class="hljs-number">4</span>]), 'w', <span class="hljs-attr">encoding="utf-8")</span>
    ff.write(file_output.decode('utf-<span class="hljs-number">8</span>'))
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="third-create-a-tf-record-from-the-pascal-voc-data"></a><a href="#third-create-a-tf-record-from-the-pascal-voc-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Third, create a TF-RECORD from the PASCAL-VOC data</h3>
<p>The preferred way to carry out this procedure seems to change regularly, so it can be tricky to find out this information. Let's start by creating a new <code>conda</code> environment for this task, callled <code>tf_test_py36</code>, containing a specific version of python (my current go-to at the time of writing is 3.6 rather than the stable 3.7, because of dependency issues that can sometimes arise on Windows OS)</p>
<pre><code class="hljs">conda <span class="hljs-built_in">create</span> <span class="hljs-comment">--name tf_test_py36 python=3.6 tensorflow lxml contextlib2</span>
</code></pre>
<p>and activate:</p>
<pre><code class="hljs">conda <span class="hljs-built_in">activate</span> tf_test_py36
</code></pre>
<h4><a class="anchor" aria-hidden="true" id="getting-tensorflow-garden-set-up"></a><a href="#getting-tensorflow-garden-set-up" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Getting Tensorflow Garden set up</h4>
<p>Clone the <a href="https://github.com/tensorflow/models/blob/master/official/README.md">Tensorflow Garden</a> GitHub repository:</p>
<pre><code class="hljs">git <span class="hljs-keyword">clone</span> <span class="hljs-title">https</span>://github.com/tensorflow/models.git
</code></pre>
<p>Add the top-level /models folder to your system Python path.</p>
<pre><code class="hljs">export PYTHONPATH=<span class="hljs-variable">$PYTHONPATH</span><span class="hljs-symbol">:/media/marda/TWOTB/USGS/SOFTWARE/models</span>
</code></pre>
<p>Install other dependencies:</p>
<pre><code class="hljs">pip install <span class="hljs-params">--user</span> -r official/requirements.txt
<span class="hljs-keyword">cd</span> research
protoc object_detection/protos/*<span class="hljs-string">.proto</span> <span class="hljs-params">--python_out=</span>.
</code></pre>
<h4><a class="anchor" aria-hidden="true" id="create-the-tf-record"></a><a href="#create-the-tf-record" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Create the tf-record</h4>
<p>(Your current directory should be <code>models/research</code>)</p>
<p>This workflow is specific to the <code>POB</code> (People on Beaches) dataset that only has one label, so first, create a file called <code>object_detection/data/pob_label_map.pbtxt</code> and copy the following into it:</p>
<pre><code class="hljs"><span class="hljs-selector-tag">item</span> {
  <span class="hljs-attribute">id</span>: <span class="hljs-number">1</span>
  name: <span class="hljs-string">'person'</span>
}
</code></pre>
<p>Second, create a new file called <code>POB_images</code>, and copy all your jpg files and corresponding xml files into it - all together</p>
<p>Third, create a new file <code>object_detection/dataset_tools/create_pob_tf_record.py</code> and copy the following code into it. The variable <code>num_shards</code> is the number of pieces you'd like to create. It matters not for this dataset; we use 10.</p>
<pre><code class="hljs">from glob <span class="hljs-keyword">import</span> glob
<span class="hljs-keyword">import</span> hashlib, io, os, logging, random, re, contextlib2
from lxml <span class="hljs-keyword">import</span> etree
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> PIL.Image
<span class="hljs-keyword">import</span> tensorflow.compat.v1 <span class="hljs-keyword">as</span> tf

from object_detection.dataset_tools <span class="hljs-keyword">import</span> tf_record_creation_util
from object_detection.utils <span class="hljs-keyword">import</span> dataset_util
from object_detection.utils <span class="hljs-keyword">import</span> label_map_util

flags = tf.app.flags
flags.DEFINE_string(<span class="hljs-string">'data_dir'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'Root directory to raw pet dataset.'</span>)
flags.DEFINE_string(<span class="hljs-string">'output_dir'</span>, <span class="hljs-string">''</span>, <span class="hljs-string">'Path to directory to output TFRecords.'</span>)
flags.DEFINE_string(<span class="hljs-string">'label_map_path'</span>, <span class="hljs-string">'data/pet_label_map.pbtxt'</span>,
                    <span class="hljs-string">'Path to label map proto'</span>)
flags.DEFINE_integer(<span class="hljs-string">'num_shards'</span>, <span class="hljs-number">10</span>, <span class="hljs-string">'Number of TFRecord shards'</span>)

FLAGS = flags.FLAGS
</code></pre>
<p>The following is the main function that gets called to carry out the conversion. It creates a single <code>tf.Example</code> message (or protobuf), which is a flexible message type that represents a <code>{&quot;string&quot;: value}</code> mapping</p>
<pre><code class="hljs">def dict_to_tf_example(data,
                       label_map_dict,
                       image_subdirectory,
                       ignore_difficult_instances=False):
  <span class="hljs-string">""</span><span class="hljs-comment">"Convert XML derived dict to tf.Example proto.</span>

  Notice that this <span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">normalizes</span> <span class="hljs-title">the</span> <span class="hljs-title">bounding</span> <span class="hljs-title">box</span> <span class="hljs-title">coordinates</span> <span class="hljs-title">provided</span></span>
  by the raw data.

  Arg<span class="hljs-variable">s:</span>
    dat<span class="hljs-variable">a:</span> dict holding PASCAL XML fields <span class="hljs-keyword">for</span> <span class="hljs-keyword">a</span> single image (obtained by
      running dataset_util.recursive_parse_xml_to_dict)
    label_map_dic<span class="hljs-variable">t:</span> A <span class="hljs-keyword">map</span> from <span class="hljs-built_in">string</span> label names <span class="hljs-keyword">to</span> integers ids.
    image_subdirectory: String specifying subdirectory within the
      Pascal dataset directory holding the actual image data.
    ignore_difficult_instance<span class="hljs-variable">s:</span> Whether <span class="hljs-keyword">to</span> skip difficult instances in the
      dataset  (defaul<span class="hljs-variable">t:</span> False).

  Return<span class="hljs-variable">s:</span>
    example: The converted <span class="hljs-keyword">tf</span>.Example.

  Raise<span class="hljs-variable">s:</span>
    ValueError: <span class="hljs-keyword">if</span> the image pointed <span class="hljs-keyword">to</span> by data[<span class="hljs-string">'filename'</span>] <span class="hljs-keyword">is</span> not <span class="hljs-keyword">a</span> valid JPEG
  <span class="hljs-string">""</span><span class="hljs-comment">"</span>
  img_path = os.path.<span class="hljs-keyword">join</span>(image_subdirectory, data[<span class="hljs-string">'filename'</span>])
  with <span class="hljs-keyword">tf</span>.gfile.GFile(img_path, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> fid:
    encoded_jpg = fid.<span class="hljs-keyword">read</span>()
  encoded_jpg_io = io.BytesIO(encoded_jpg)
  image = PIL.Image.<span class="hljs-keyword">open</span>(encoded_jpg_io)
  <span class="hljs-keyword">if</span> image.format != <span class="hljs-string">'JPEG'</span>:
    raise ValueError(<span class="hljs-string">'Image format not JPEG'</span>)
  key = hashlib.<span class="hljs-built_in">sha256</span>(encoded_jpg).hexdigest()

  width = <span class="hljs-keyword">int</span>(data[<span class="hljs-string">'size'</span>][<span class="hljs-string">'width'</span>])
  height = <span class="hljs-keyword">int</span>(data[<span class="hljs-string">'size'</span>][<span class="hljs-string">'height'</span>])

  xmins = []
  ymins = []
  xmaxs = []
  ymaxs = []
  classes = []
  classes_text = []
  truncated = []
  poses = []
  difficult_obj = []
  <span class="hljs-keyword">if</span> <span class="hljs-string">'object'</span> in dat<span class="hljs-variable">a:</span>
    <span class="hljs-keyword">for</span> obj in data[<span class="hljs-string">'object'</span>]:
      difficult = bool(<span class="hljs-keyword">int</span>(obj[<span class="hljs-string">'difficult'</span>]))
      <span class="hljs-keyword">if</span> ignore_difficult_instances <span class="hljs-built_in">and</span> difficul<span class="hljs-variable">t:</span>
        <span class="hljs-keyword">continue</span>
      difficult_obj.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">int</span>(difficult))

      xmin = float(obj[<span class="hljs-string">'bndbox'</span>][<span class="hljs-string">'xmin'</span>])
      xmax = float(obj[<span class="hljs-string">'bndbox'</span>][<span class="hljs-string">'xmax'</span>])
      ymin = float(obj[<span class="hljs-string">'bndbox'</span>][<span class="hljs-string">'ymin'</span>])
      ymax = float(obj[<span class="hljs-string">'bndbox'</span>][<span class="hljs-string">'ymax'</span>])

      xmins.<span class="hljs-keyword">append</span>(xmin / width)
      ymins.<span class="hljs-keyword">append</span>(ymin / height)
      xmaxs.<span class="hljs-keyword">append</span>(xmax / width)
      ymaxs.<span class="hljs-keyword">append</span>(ymax / height)
      class_name = <span class="hljs-string">'person'</span> #get_class_name_from_filename(data[<span class="hljs-string">'filename'</span>])
      classes_text.<span class="hljs-keyword">append</span>(class_name.encode(<span class="hljs-string">'utf8'</span>))
      classes.<span class="hljs-keyword">append</span>(label_map_dict[class_name])
      truncated.<span class="hljs-keyword">append</span>(<span class="hljs-keyword">int</span>(obj[<span class="hljs-string">'truncated'</span>]))
      poses.<span class="hljs-keyword">append</span>(obj[<span class="hljs-string">'pose'</span>].encode(<span class="hljs-string">'utf8'</span>))

  feature_dict = {
      <span class="hljs-string">'image/height'</span>: dataset_util.int64_feature(height),
      <span class="hljs-string">'image/width'</span>: dataset_util.int64_feature(width),
      <span class="hljs-string">'image/filename'</span>: dataset_util.bytes_feature(
          data[<span class="hljs-string">'filename'</span>].encode(<span class="hljs-string">'utf8'</span>)),
      <span class="hljs-string">'image/source_id'</span>: dataset_util.bytes_feature(
          data[<span class="hljs-string">'filename'</span>].encode(<span class="hljs-string">'utf8'</span>)),
      <span class="hljs-string">'image/key/sha256'</span>: dataset_util.bytes_feature(key.encode(<span class="hljs-string">'utf8'</span>)),
      <span class="hljs-string">'image/encoded'</span>: dataset_util.bytes_feature(encoded_jpg),
      <span class="hljs-string">'image/format'</span>: dataset_util.bytes_feature(<span class="hljs-string">'jpeg'</span>.encode(<span class="hljs-string">'utf8'</span>)),
      <span class="hljs-string">'image/object/bbox/xmin'</span>: dataset_util.float_list_feature(xmins),
      <span class="hljs-string">'image/object/bbox/xmax'</span>: dataset_util.float_list_feature(xmaxs),
      <span class="hljs-string">'image/object/bbox/ymin'</span>: dataset_util.float_list_feature(ymins),
      <span class="hljs-string">'image/object/bbox/ymax'</span>: dataset_util.float_list_feature(ymaxs),
      <span class="hljs-string">'image/object/class/text'</span>: dataset_util.bytes_list_feature(classes_text),
      <span class="hljs-string">'image/object/class/label'</span>: dataset_util.int64_list_feature(classes),
      <span class="hljs-string">'image/object/difficult'</span>: dataset_util.int64_list_feature(difficult_obj),
      <span class="hljs-string">'image/object/truncated'</span>: dataset_util.int64_list_feature(truncated),
      <span class="hljs-string">'image/object/view'</span>: dataset_util.bytes_list_feature(poses),
  }

  example = <span class="hljs-keyword">tf</span>.train.Example(features=<span class="hljs-keyword">tf</span>.train.Features(feature=feature_dict))
  <span class="hljs-keyword">return</span> example
</code></pre>
<p>This portion does the file writing (i.e. creates the <code>.tfrecord</code> files from the collection of <code>tf.Example</code> records):</p>
<pre><code class="hljs">def create_tf_record(output_filename,
                     num_shards,
                     label_map_dict,
                     annotations_dir,
                     image_dir,
                     examples):
  <span class="hljs-string">""</span><span class="hljs-comment">"Creates a TFRecord file from examples.</span>

  Arg<span class="hljs-variable">s:</span>
    output_filename: Path <span class="hljs-keyword">to</span> where output <span class="hljs-keyword">file</span> <span class="hljs-keyword">is</span> saved.
    num_shard<span class="hljs-variable">s:</span> Number of shards <span class="hljs-keyword">for</span> output <span class="hljs-keyword">file</span>.
    label_map_dic<span class="hljs-variable">t:</span> The label <span class="hljs-keyword">map</span> dictionary.
    annotations_dir: Directory where annotation <span class="hljs-keyword">files</span> are stored.
    image_dir: Directory where image <span class="hljs-keyword">files</span> are stored.
    example<span class="hljs-variable">s:</span> Examples <span class="hljs-keyword">to</span> parse <span class="hljs-built_in">and</span> save <span class="hljs-keyword">to</span> <span class="hljs-keyword">tf</span> record.
  <span class="hljs-string">""</span><span class="hljs-comment">"</span>
  with contextlib2.ExitStack() <span class="hljs-keyword">as</span> tf_record_close_stack:
    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
        tf_record_close_stack, output_filename, num_shards)
    <span class="hljs-keyword">for</span> idx, example in enumerate(examples):
      <span class="hljs-keyword">if</span> idx % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
        logging.info(<span class="hljs-string">'On image %d of %d'</span>, idx, <span class="hljs-built_in">len</span>(examples))
      xml_path = os.path.<span class="hljs-keyword">join</span>(annotations_dir, <span class="hljs-string">'xmls'</span>, example.<span class="hljs-keyword">split</span>(<span class="hljs-string">'.jpg'</span>)[<span class="hljs-number">0</span>] + <span class="hljs-string">'.xml'</span>)

      <span class="hljs-keyword">if</span> not os.path.<span class="hljs-built_in">exists</span>(xml_path):
        logging.warning(<span class="hljs-string">'Could not find %s, ignoring example.'</span>, xml_path)
        <span class="hljs-keyword">continue</span>
      with <span class="hljs-keyword">tf</span>.gfile.GFile(xml_path, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> fid:
        xml_str = fid.<span class="hljs-keyword">read</span>()
      xml = etree.fromstring(xml_str)
      data = dataset_util.recursive_parse_xml_to_dict(xml)[<span class="hljs-string">'annotation'</span>]

      <span class="hljs-keyword">try</span>:
        tf_example = dict_to_tf_example(
            data,
            label_map_dict,
            image_dir)
        <span class="hljs-keyword">if</span> tf_example:
          shard_idx = idx % num_shards
          output_tfrecords[shard_idx].<span class="hljs-keyword">write</span>(tf_example.SerializeToString())
      except ValueError:
        logging.warning(<span class="hljs-string">'Invalid example: %s, ignoring.'</span>, xml_path)
</code></pre>
<p>The <code>main</code> function reads all the jpg files in <code>POB_images</code>, as well as all the <code>xml</code> files in the same directory. Note that this could be set up differently, to read the <code>xml</code> files from a separate <code>annotations_dir</code>. The files are randomly shuffled. The number of training examples is 70% of the total, and the remaining 30% of files are used for validation. It then calls the <code>create_tf_record</code> function to create the <code>.tfrecord</code> set of 10 files.</p>
<pre><code class="hljs">def main(_):
  data_dir = FLAGS.data_dir
  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)

  logging.<span class="hljs-keyword">info</span>(<span class="hljs-string">'Reading from POB dataset.'</span>)
  image_dir = annotations_dir = os.path.<span class="hljs-keyword">join</span>(data_dir, <span class="hljs-string">'POB_images'</span>)

  examples_list = glob(image_dir+<span class="hljs-string">'/*.jpg'</span>)

  # Test images are <span class="hljs-keyword">not</span> included <span class="hljs-keyword">in</span> the downloaded data <span class="hljs-keyword">set</span>, so we shall <span class="hljs-keyword">perform</span>
  # our own split.
  random.seed(<span class="hljs-number">42</span>)
  random.shuffle(examples_list)
  num_examples = len(examples_list)
  num_train = <span class="hljs-type">int</span>(<span class="hljs-number">0.7</span> * num_examples)
  train_examples = examples_list[:num_train]
  val_examples = examples_list[num_train:]
  logging.<span class="hljs-keyword">info</span>(<span class="hljs-string">'%d training and %d validation examples.'</span>,
               len(train_examples), len(val_examples))

  train_output_path = os.path.<span class="hljs-keyword">join</span>(FLAGS.output_dir, <span class="hljs-string">'pob_train.record'</span>)
  val_output_path = os.path.<span class="hljs-keyword">join</span>(FLAGS.output_dir, <span class="hljs-string">'pob_val.record'</span>)

  # <span class="hljs-keyword">call</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">create</span> the training files
  create_tf_record(
      train_output_path,
      FLAGS.num_shards,
      label_map_dict,
      annotations_dir,
      image_dir,
      train_examples)

# <span class="hljs-keyword">call</span> again <span class="hljs-keyword">to</span> make the validation <span class="hljs-keyword">set</span>
  create_tf_record(
      val_output_path,
      FLAGS.num_shards,
      label_map_dict,
      annotations_dir,
      image_dir,
      val_examples)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
  tf.app.run()

</code></pre>
<p>And finally, run the script and make your tf-record format data ...</p>
<pre><code class="hljs">python object_detection/dataset_tools/create_pob_tf_record.py <span class="hljs-attribute">--label_map_path</span>=object_detection/data/POB_label_map.pbtxt  <span class="hljs-attribute">--data_dir</span>=`pwd` <span class="hljs-attribute">--output_dir</span>=`pwd`
</code></pre>
<p>This will create 10 files for the training data, and 10 for the validation set. You can now use these files to efficiently train a model using Tensoflow/Keras.</p>
<h3><a class="anchor" aria-hidden="true" id="optional-xml-to-csv"></a><a href="#optional-xml-to-csv" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[OPTIONAL] XML to CSV</h3>
<p>Sometimes you also see people use object annotations in csv format. Luckily we can use the <code>xml</code> library to help carry out the data parsing, and <code>pandas</code> to easily convert to a dataframe, and then to a formatted csv file</p>
<pre><code class="hljs"><span class="hljs-keyword">import</span> os, glob
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> <span class="hljs-type">xml</span>.etree.ElementTree <span class="hljs-keyword">as</span> ET

def xml_to_csv(<span class="hljs-type">path</span>):
    xml_list = []
    <span class="hljs-keyword">for</span> xml_file <span class="hljs-keyword">in</span> glob.glob(<span class="hljs-type">path</span> + <span class="hljs-string">'/*.xml'</span>):
        tree = ET.parse(xml_file)
        root = tree.getroot()
        <span class="hljs-keyword">for</span> member <span class="hljs-keyword">in</span> root.findall(<span class="hljs-string">'object'</span>):
            <span class="hljs-keyword">value</span> = (root.find(<span class="hljs-string">'filename'</span>).text,
                     <span class="hljs-type">int</span>(root.find(<span class="hljs-string">'size'</span>)[<span class="hljs-number">0</span>].text),
                     <span class="hljs-type">int</span>(root.find(<span class="hljs-string">'size'</span>)[<span class="hljs-number">1</span>].text),
                     member[<span class="hljs-number">0</span>].text,
                     <span class="hljs-type">int</span>(member[<span class="hljs-number">4</span>][<span class="hljs-number">0</span>].text),
                     <span class="hljs-type">int</span>(member[<span class="hljs-number">4</span>][<span class="hljs-number">1</span>].text),
                     <span class="hljs-type">int</span>(member[<span class="hljs-number">4</span>][<span class="hljs-number">2</span>].text),
                     <span class="hljs-type">int</span>(member[<span class="hljs-number">4</span>][<span class="hljs-number">3</span>].text)
                     )
            xml_list.append(<span class="hljs-keyword">value</span>)
    <span class="hljs-built_in">column_name</span> = [<span class="hljs-string">'filename'</span>, <span class="hljs-string">'width'</span>, <span class="hljs-string">'height'</span>, <span class="hljs-string">'class'</span>, <span class="hljs-string">'xmin'</span>, <span class="hljs-string">'ymin'</span>, <span class="hljs-string">'xmax'</span>, <span class="hljs-string">'ymax'</span>]
    xml_df = pd.DataFrame(xml_list, <span class="hljs-keyword">columns</span>=<span class="hljs-built_in">column_name</span>)
    <span class="hljs-keyword">return</span> xml_df

</code></pre>
<p>Then use like this:</p>
<pre><code class="hljs">image_path = os.path.join(os.getcwd<span class="hljs-literal">()</span>,'test_labels_xml')
xml_df = xml<span class="hljs-constructor">_to_csv(<span class="hljs-params">image_path</span>)</span>
xml_df.<span class="hljs-keyword">to</span><span class="hljs-constructor">_csv('<span class="hljs-params">test_labels</span>.<span class="hljs-params">csv</span>', <span class="hljs-params">index</span>=None)</span>
</code></pre>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/08/05/blog-post">Trimming and decompiling a video into png image files, for use in your deep learning project</a></h1><p class="post-meta">August 5, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><article class="post-content"><div><span><p><img src="/MLMONDAYS/blog/assets/secoora.png" alt=""></p>
<p>You can access from the NOAA NOS Web Camera Applications Testbed (WebCAT) <a href="https://secoora.org/webcat/">Live Cameras and Historic Feeds</a> site. The image above shows how to view the image I will use in the demonstration below.</p>
<p>To extract a subsection (or trim) of the file  <code>staugustinecam.2019-01-11_0900.mp4</code>, from 1 min 47 s to 6 min 22 s, and output it to the file <code>staugustinecam.2019-01-11_0900_trim.mp4</code>:</p>
<pre><code class="hljs">ffmpeg -i staugustinecam<span class="hljs-number">.2019</span><span class="hljs-number">-01</span><span class="hljs-number">-11</span>_0900.mp4 -ss <span class="hljs-number">00</span>:<span class="hljs-number">01</span>:<span class="hljs-number">47</span> -t <span class="hljs-number">00</span>:<span class="hljs-number">06</span>:<span class="hljs-number">22</span> -async <span class="hljs-number">1</span> staugustinecam<span class="hljs-number">.2019</span><span class="hljs-number">-01</span><span class="hljs-number">-11</span>_0900_trim.mp4
</code></pre>
<p>To extract <code>png</code> format files from that trimmed video, writing the frame number to the file name:</p>
<pre><code class="hljs">ffmpeg -i staugustinecam<span class="hljs-number">.2019</span><span class="hljs-number">-01</span><span class="hljs-number">-11</span>_0900_trim.mp4 staugustinecam<span class="hljs-number">.2019</span><span class="hljs-number">-01</span><span class="hljs-number">-11</span>_0900_%d.png
</code></pre>
<p>Make a directory:</p>
<pre><code class="hljs">mkdir staugustinecam_2019<span class="hljs-number">-01</span><span class="hljs-number">-11</span>_0900
</code></pre>
<p>Move all of the <code>png</code> files into that directory:</p>
<pre><code class="hljs">mv *.png staugustinecam_2019<span class="hljs-number">-01</span><span class="hljs-number">-11</span>_0900
</code></pre>
<p>Make a bash script so you can execute the decompiling (only) sequence of commands on an arbitrary mp4 file from the command line. First, open a new file called <code>nano decompile_mp4.sh</code>:</p>
<pre><code class="hljs"><span class="hljs-selector-tag">nano</span> <span class="hljs-selector-tag">decompile_mp4</span><span class="hljs-selector-class">.sh</span>
</code></pre>
<p>and write or copy the following into it:</p>
<pre><code class="hljs">echo <span class="hljs-string">"decompiling video file $1, and moving png frames into $2, in T minus 5 seconds ..."</span>
<span class="hljs-keyword">sleep</span> <span class="hljs-number">5</span><span class="hljs-keyword">s</span>
ffmpeg -i $1 $2<span class="hljs-number">_</span>%d.png
<span class="hljs-keyword">mkdir</span> $2
mv *.png $2
</code></pre>
<p>To exit <code>nano</code>:</p>
<p><code>Ctrl+X</code> (for exit)
<code>Y</code> (for yes)
<code>enter</code> (for save)</p>
<p>Use it like this:</p>
<pre><code class="hljs"><span class="hljs-selector-tag">bash</span> <span class="hljs-selector-tag">decompile_mp4</span><span class="hljs-selector-class">.sh</span> <span class="hljs-selector-tag">myvid</span><span class="hljs-selector-class">.mp4</span> <span class="hljs-selector-tag">myframes</span>
</code></pre>
<p>To extract 1 frame per minute, modify to:</p>
<pre><code class="hljs">echo <span class="hljs-string">"decompiling video file $1 - one frame every minute, and moving png frames into $2, in T minus 5 seconds ..."</span>
<span class="hljs-keyword">sleep</span> <span class="hljs-number">5</span><span class="hljs-keyword">s</span>
ffmpeg -i $1 -vf fps=<span class="hljs-number">1</span>/<span class="hljs-number">60</span> $2<span class="hljs-number">_</span>%d.png
<span class="hljs-keyword">mkdir</span> $2
mv *.png $2
</code></pre>
<p>Let's say you've carried out the above procedure on lots of videos and you have lots of folders containing frames. Now you want to randomly move some images from each directory into three separate folders that you'll use in your deep learning project, called <code>test</code>, <code>train</code>, and <code>validate</code> (it is also common to use <code>test</code> for the purposes of both testing and validating)</p>
<pre><code class="hljs"><span class="hljs-keyword">mkdir</span> train
<span class="hljs-keyword">mkdir</span> <span class="hljs-keyword">test</span>
<span class="hljs-keyword">mkdir</span> validate
</code></pre>
<p>To move 10 random files to each from a directory called <code>my_directory</code> containing png frames:</p>
<pre><code class="hljs">cd my_directory
shuf -n <span class="hljs-number">10</span> -e * | xargs -i <span class="hljs-class">mv </span>{} ..<span class="hljs-meta-keyword">/train/</span>
shuf -n <span class="hljs-number">10</span> -e * | xargs -i <span class="hljs-class">mv </span>{} ..<span class="hljs-meta-keyword">/test/</span>
shuf -n <span class="hljs-number">10</span> -e * | xargs -i <span class="hljs-class">mv </span>{} ..<span class="hljs-meta-keyword">/validate/</span>
</code></pre>
<p>or to generalize:</p>
<pre><code class="hljs"><span class="hljs-keyword">for</span> direc <span class="hljs-keyword">in</span> st*/
<span class="hljs-keyword">do</span>
<span class="hljs-built_in">cd</span> <span class="hljs-variable">$direc</span>
shuf -n 10 -e * | xargs -i mv {} ../train/
shuf -n 10 -e * | xargs -i mv {} ../<span class="hljs-built_in">test</span>/
shuf -n 10 -e * | xargs -i mv {} ../validate/  
<span class="hljs-built_in">cd</span> ..
<span class="hljs-keyword">done</span>
</code></pre>
<p>In the above, I say <code>st*/</code> to list only directories beginning with <code>st</code>, rather than listing all directories (<code>/*</code>), which would include <code>test</code>, <code>train</code>, and <code>validate</code> and would not work</p>
<p>Finally, to convert all png files to jpg files, use:</p>
<pre><code class="hljs"><span class="hljs-attribute">mogrify</span> -format jpg <span class="hljs-regexp">*.png</span>
</code></pre>
</span></div></article></div><div class="docs-prevnext"></div></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/MLMONDAYS/" class="nav-home"><img src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;" width="66" height="58"/></a><div><h5>Internal links</h5><a href="/MLMONDAYS/docs/en/doc1.html">Docs</a><a href="/MLMONDAYS/docs/en/doc2.html">Data</a><a href="/MLMONDAYS/docs/en/doc3.html">Help</a></div><div><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://www.usgs.gov/centers/cdi" target="_blank" rel="noreferrer noopener">USGS Community for Data Integration (CDI)</a><a href="https://www.usgs.gov/centers/pcmsc/science/remote-sensing-coastal-change?qt-science_center_objects=0#qt-science_center_objects" target="_blank" rel="noreferrer noopener">USGS Remote Sensing Coastal Change Project</a><a href="https://www.danielbuscombe.com" target="_blank" rel="noreferrer noopener">www.danielbuscombe.com</a></div><div><h5>More</h5><a href="/MLMONDAYS/blog">Blog</a><a href="https://github.com/dbuscombe-usgs/DL-CDI2020">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a><div class="social"><a href="https://twitter.com/magic_walnut" class="twitter-follow-button">Follow @magic_walnut</a></div></div></section><a href="https://mardascience.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/MLMONDAYS/img/dash-logo-new.png" alt="Marda Science" width="170" height="45"/></a><section class="copyright">Copyright © 2020 Marda Science, LLC</section></footer></div><script>window.twttr=(function(d,s, id){var js,fjs=d.getElementsByTagName(s)[0],t=window.twttr||{};if(d.getElementById(id))return t;js=d.createElement(s);js.id=id;js.src='https://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js, fjs);t._e = [];t.ready = function(f) {t._e.push(f);};return t;}(document, 'script', 'twitter-wjs'));</script></body></html>