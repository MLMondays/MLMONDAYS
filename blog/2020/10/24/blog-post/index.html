<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>From 2 geotiffs to a trained U-Net: 2D, 3D, and 4D imagery example. Part 2: Models · &quot;ML Mondays&quot;</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="# Segmentation Model"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="From 2 geotiffs to a trained U-Net: 2D, 3D, and 4D imagery example. Part 2: Models · &quot;ML Mondays&quot;"/><meta property="og:type" content="website"/><meta property="og:url" content="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/2020/10/24/blog-post"/><meta property="og:description" content="# Segmentation Model"/><meta property="og:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/MLMONDAYS/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/atom.xml" title="&quot;ML Mondays&quot; Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/feed.xml" title="&quot;ML Mondays&quot; Blog RSS Feed"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/MLMONDAYS/js/scrollSpy.js"></script><link rel="stylesheet" href="/MLMONDAYS/css/main.css"/><script src="/MLMONDAYS/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/MLMONDAYS/"><img class="logo" src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;"/><h2 class="headerTitleWithLogo">&quot;ML Mondays&quot;</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/MLMONDAYS/docs/doc1" target="_self">Docs</a></li><li class=""><a href="/MLMONDAYS/docs/doc2" target="_self">Data</a></li><li class=""><a href="/MLMONDAYS/docs/doc3" target="_self">Models</a></li><li class=""><a href="/MLMONDAYS/docs/doc4" target="_self">API</a></li><li class=""><a href="/MLMONDAYS/help" target="_self">Help</a></li><li class="siteNavGroupActive"><a href="/MLMONDAYS/blog/" target="_self">Blog</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Recent Posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Recent Posts</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/MLMONDAYS/blog/2020/10/24/blog-post">From 2 geotiffs to a trained U-Net: 2D, 3D, and 4D imagery example. Part 2: Models</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/23/blog-post">From 2 geotiffs to a trained U-Net: 2D, 3D, and 4D imagery example. Part 1: Data</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/14/blog-post">Converting makesense.ai JSON labels to label (mask) imagery for an image segmentation project</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/11/blog-post">Preparing a new dataset for an object recognition project</a></li><li class="navListItem"><a class="navItem" href="/MLMONDAYS/blog/2020/10/03/blog-post">ML terminology, demystified</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/MLMONDAYS/blog/2020/10/24/blog-post">From 2 geotiffs to a trained U-Net: 2D, 3D, and 4D imagery example. Part 2: Models</a></h1><p class="post-meta">October 24, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/magic_walnut" target="_blank" rel="noreferrer noopener">Dan Buscombe</a></p></div></header><div><span><h1><a class="anchor" aria-hidden="true" id="segmentation-model"></a><a href="#segmentation-model" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Segmentation Model</h1>
<p>In the <a href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/2020/10/23/blog-post">previous post</a> we prepared two analysis ready datasets, the first consisting of RGB (or 3D) images and associated greyscale labels, and the second consisting of 4D (RGB + DEM) imagery and the same labels. Here I show how to use that imagery in three different workflows, first using RGB imagery, then DEM, then a combination of the two. We'll evaluate using a validation dataset, and some unseen (unaugmented) sample imagery to test the ability of the model to generalize. We see that the 4D data is slightly better than the 3D data, which is a lot better than the 2D data for this task. However, in each case the model didn't do well on the sample imagery so some troubleshooting is required. Likely, a lot more data is required; this workflow uses only 16 original image tiles; many more would be required for accurate results. However, this blog post does demonstrate that the mlmondays workflows can be adapted to different data sets, and more complicated data and classes.</p>
<h2><a class="anchor" aria-hidden="true" id="rgb-imagery"></a><a href="#rgb-imagery" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RGB imagery</h2>
<h3><a class="anchor" aria-hidden="true" id="model-preparation"></a><a href="#model-preparation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model preparation</h3>
<pre><code class="hljs css language-python"><span class="hljs-keyword">from</span> imports <span class="hljs-keyword">import</span> *
</code></pre>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_batched_dataset</span><span class="hljs-params">(filenames)</span>:</span>

    option_no_order = tf.data.Options()
    option_no_order.experimental_deterministic = <span class="hljs-literal">True</span>

    dataset = tf.data.Dataset.list_files(filenames)
    dataset = dataset.with_options(option_no_order)
    dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=<span class="hljs-number">16</span>, num_parallel_calls=AUTO)
    dataset = dataset.map(read_seg_tfrecord_dunes, num_parallel_calls=AUTO)

    dataset = dataset.cache() <span class="hljs-comment"># This dataset fits in RAM</span>
    dataset = dataset.repeat()
    dataset = dataset.shuffle(<span class="hljs-number">2048</span>)
    dataset = dataset.batch(BATCH_SIZE, drop_remainder=<span class="hljs-literal">True</span>) <span class="hljs-comment"># drop_remainder will be needed on TPU</span>
    dataset = dataset.prefetch(AUTO) <span class="hljs-comment">#</span>

    <span class="hljs-keyword">return</span> dataset

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_training_dataset</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">return</span> get_batched_dataset(training_filenames)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_validation_dataset</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-keyword">return</span> get_batched_dataset(validation_filenames)
</code></pre>
<p>We need a function to seg each example record from the TFRecord shards. You'll see similar functions in mlmondays workflows for OBX and OysterNet dataset. We start by creating a dictionary to use to parse the two features (image and label pair) as binary strings. Then convert each to jpeg and scale to the range [0, 1]. If any number in the label is greater than 8, it is set to zero. Zero is being used as a NULL class for zero image pixels. Finally, the label image is converted into a one-hot stack, with 9 bands (one for each of the 8 classes and the null class).</p>
<pre><code class="hljs css language-python"><span class="hljs-meta">@tf.autograph.experimental.do_not_convert</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_seg_tfrecord_dunes</span><span class="hljs-params">(example)</span>:</span>
    features = {
        <span class="hljs-string">"image"</span>: tf.io.FixedLenFeature([], tf.string),  <span class="hljs-comment"># tf.string = bytestring (not text string)</span>
        <span class="hljs-string">"label"</span>: tf.io.FixedLenFeature([], tf.string),   <span class="hljs-comment"># shape [] means scalar</span>
    }
    <span class="hljs-comment"># decode the TFRecord</span>
    example = tf.io.parse_single_example(example, features)

    image = tf.image.decode_jpeg(example[<span class="hljs-string">'image'</span>], channels=<span class="hljs-number">3</span>)
    image = tf.cast(image, tf.float32)/ <span class="hljs-number">255.0</span>

    label = tf.image.decode_jpeg(example[<span class="hljs-string">'label'</span>], channels=<span class="hljs-number">1</span>)
    label = tf.cast(label, tf.uint8)

    cond = tf.greater(label, tf.ones(tf.shape(label),dtype=tf.uint8)*<span class="hljs-number">6</span>)<span class="hljs-comment">#8)</span>
    label = tf.where(cond, tf.ones(tf.shape(label),dtype=tf.uint8)*<span class="hljs-number">0</span>, label)

    label = tf.one_hot(tf.cast(label, tf.uint8), <span class="hljs-number">7</span>) <span class="hljs-comment">#9)</span>
    label = tf.squeeze(label)

    <span class="hljs-keyword">return</span> image, label
</code></pre>
<p>From now on the code look should look familiar, if you've run through the exercises as part of mlmondays week 3 image segmentation. We define a data path to the tfrecord files, a filepath for the model weights, a file path for the training history plot. Then specify a patience for the early stopping criterion, the number of images encoded in each shard, for specification of training and validation steps per model training epoch, the validation split, and batch size.</p>
<pre><code class="hljs css language-python">data_path= os.getcwd()+os.sep+<span class="hljs-string">"data/dunes"</span>

filepath = os.getcwd()+os.sep+<span class="hljs-string">'results/dunes_8class_best_weights_model.h5'</span>

hist_fig = os.getcwd()+os.sep+<span class="hljs-string">'results/dunes_8class_model.png'</span>

patience = <span class="hljs-number">20</span>

ims_per_shard = <span class="hljs-number">12</span>

VALIDATION_SPLIT = <span class="hljs-number">0.6</span>

BATCH_SIZE = <span class="hljs-number">4</span>

filenames = sorted(tf.io.gfile.glob(data_path+os.sep+<span class="hljs-string">'dunes3d*.tfrec'</span>))

nb_images = ims_per_shard * len(filenames)
print(nb_images)

split = int(len(filenames) * VALIDATION_SPLIT)

training_filenames = filenames[split:]
validation_filenames = filenames[:split]

validation_steps = int(nb_images // len(filenames) * len(validation_filenames)) // BATCH_SIZE
steps_per_epoch = int(nb_images // len(filenames) * len(training_filenames)) // BATCH_SIZE
</code></pre>
<pre><code class="hljs css language-python">train_ds = get_training_dataset()

L = []
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(<span class="hljs-number">12</span>):
  plt.figure(figsize=(<span class="hljs-number">16</span>,<span class="hljs-number">16</span>))
  <span class="hljs-keyword">for</span> imgs,lbls <span class="hljs-keyword">in</span> train_ds.take(<span class="hljs-number">1</span>):
    <span class="hljs-comment">#print(lbls)</span>
    <span class="hljs-keyword">for</span> count,(im,lab) <span class="hljs-keyword">in</span> enumerate(zip(imgs, lbls)):
       plt.subplot(int(BATCH_SIZE/<span class="hljs-number">2</span>),int(BATCH_SIZE/<span class="hljs-number">2</span>),count+<span class="hljs-number">1</span>)
       plt.imshow(im)
       plt.imshow(np.argmax(lab,<span class="hljs-number">-1</span>), cmap=plt.cm.bwr, alpha=<span class="hljs-number">0.5</span>)<span class="hljs-comment">#, vmin=0, vmax=7)</span>
       <span class="hljs-comment">#plt.imshow(lab, cmap=plt.cm.bwr, alpha=0.5, vmin=0, vmax=9)</span>
       plt.axis(<span class="hljs-string">'off'</span>)
       L.append(np.unique(np.argmax(lab,<span class="hljs-number">-1</span>)))

  plt.show()
</code></pre>
<p>What unique values do we have in our augmented imagery?</p>
<pre><code class="hljs css language-python">print(np.round(np.unique(np.hstack(L))))

[<span class="hljs-number">0</span> <span class="hljs-number">1</span> <span class="hljs-number">2</span> <span class="hljs-number">3</span> <span class="hljs-number">4</span> <span class="hljs-number">5</span> <span class="hljs-number">6</span>]
</code></pre>
<pre><code class="hljs css language-python">val_ds = get_validation_dataset()
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="model-training"></a><a href="#model-training" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training</h3>
<p>Define the number of classes (9, including the null class) and target size (the encoded image's size), then create a model. Compile it, define callbacks.</p>
<pre><code class="hljs css language-python">nclasses=<span class="hljs-number">7</span> <span class="hljs-comment">#9</span>
TARGET_SIZE = <span class="hljs-number">608</span>
model = res_unet((TARGET_SIZE, TARGET_SIZE, <span class="hljs-number">3</span>), BATCH_SIZE, <span class="hljs-string">'multiclass'</span>, nclasses)
<span class="hljs-comment"># model.compile(optimizer = 'adam', loss = tf.keras.losses.CategoricalHinge(), metrics = [mean_iou])</span>
model.compile(optimizer = <span class="hljs-string">'adam'</span>, loss = <span class="hljs-string">'categorical_crossentropy'</span>, metrics = [mean_iou])

earlystop = EarlyStopping(monitor=<span class="hljs-string">"val_loss"</span>,
                              mode=<span class="hljs-string">"min"</span>, patience=patience)

model_checkpoint = ModelCheckpoint(filepath, monitor=<span class="hljs-string">'val_loss'</span>,
                                verbose=<span class="hljs-number">0</span>, save_best_only=<span class="hljs-literal">True</span>, mode=<span class="hljs-string">'min'</span>,
                                save_weights_only = <span class="hljs-literal">True</span>)

lr_callback = tf.keras.callbacks.LearningRateScheduler(<span class="hljs-keyword">lambda</span> epoch: lrfn(epoch), verbose=<span class="hljs-literal">True</span>)

callbacks = [model_checkpoint, earlystop, lr_callback]
</code></pre>
<p>Fit the model, and make a plot of the model training history (loss and mean IOU).</p>
<pre><code class="hljs css language-python">
<span class="hljs-comment">#warmup</span>
model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=MAX_EPOCHS,
                      validation_data=val_ds, validation_steps=validation_steps,
                      callbacks=callbacks)

history = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=MAX_EPOCHS,
                      validation_data=val_ds, validation_steps=validation_steps,
                      callbacks=callbacks)

plot_seg_history_iou(history, hist_fig)

plt.close(<span class="hljs-string">'all'</span>)
K.clear_session()
</code></pre>
<p><img src="/MLMONDAYS/blog/assets/dunes_8class_model.png" alt=""></p>
<h3><a class="anchor" aria-hidden="true" id="model-evaluation"></a><a href="#model-evaluation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model evaluation</h3>
<p>Evaluate the model using the validation set. Print the average loss and IoU score.</p>
<pre><code class="hljs css language-python">scores = model.evaluate(val_ds, steps=validation_steps)

print(<span class="hljs-string">'loss={loss:0.4f}, Mean IOU={iou:0.4f}'</span>.format(loss=scores[<span class="hljs-number">0</span>], iou=scores[<span class="hljs-number">1</span>]))

loss=<span class="hljs-number">0.2917</span>, Mean IOU=<span class="hljs-number">0.9542</span>

</code></pre>
<pre><code class="hljs css language-python">sample_data_path = os.getcwd()+os.sep+<span class="hljs-string">'data/dunes/images/files'</span>
test_samples_fig = os.getcwd()+os.sep+<span class="hljs-string">'dunes_sample_16class_est16samples.png'</span>
sample_label_data_path = os.getcwd()+os.sep+<span class="hljs-string">'data/dunes/labels/files'</span>

sample_filenames = sorted(tf.io.gfile.glob(sample_data_path+os.sep+<span class="hljs-string">'*.jpg'</span>))
sample_label_filenames = sorted(tf.io.gfile.glob(sample_label_data_path+os.sep+<span class="hljs-string">'*.jpg'</span>))
</code></pre>
<p>These are the same hex coolor codes as the plotly <code>G10</code> colormap used to make the color label imagery, made into a custom matplotlib discrete colormap</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">from</span> matplotlib.colors <span class="hljs-keyword">import</span> ListedColormap
cmap = ListedColormap([<span class="hljs-string">"#000000"</span>,
      <span class="hljs-string">"#3366CC"</span>, <span class="hljs-string">"#DC3912"</span>,
      <span class="hljs-string">"#FF9900"</span>, <span class="hljs-string">"#109618"</span>,
      <span class="hljs-string">"#990099"</span>, <span class="hljs-string">"#0099C6"</span>])<span class="hljs-comment">#,</span>
      <span class="hljs-comment"># "#DD4477", "#66AA00"])</span>
</code></pre>
<p>We're going to adopt a spatial filter again to remove high-frequency noise associated with jpeg compression and unpacking</p>
<pre><code class="hljs css language-python"><span class="hljs-keyword">from</span> skimage.filters.rank <span class="hljs-keyword">import</span> median
<span class="hljs-keyword">from</span> skimage.morphology <span class="hljs-keyword">import</span> disk
</code></pre>
<p>This is the same function as in the mlmondays repository, with the additional <code>TARGET_SIZE</code> argument</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">seg_file2tensor</span><span class="hljs-params">(f, TARGET_SIZE)</span>:</span>

    bits = tf.io.read_file(f)
    image = tf.image.decode_jpeg(bits)

    w = tf.shape(image)[<span class="hljs-number">0</span>]
    h = tf.shape(image)[<span class="hljs-number">1</span>]
    tw = TARGET_SIZE
    th = TARGET_SIZE
    resize_crit = (w * th) / (h * tw)
    image = tf.cond(resize_crit &lt; <span class="hljs-number">1</span>,
                  <span class="hljs-keyword">lambda</span>: tf.image.resize(image, [w*tw/w, h*tw/w]), <span class="hljs-comment"># if true</span>
                  <span class="hljs-keyword">lambda</span>: tf.image.resize(image, [w*th/h, h*th/h])  <span class="hljs-comment"># if false</span>
                 )

    nw = tf.shape(image)[<span class="hljs-number">0</span>]
    nh = tf.shape(image)[<span class="hljs-number">1</span>]
    image = tf.image.crop_to_bounding_box(image, (nw - tw) // <span class="hljs-number">2</span>, (nh - th) // <span class="hljs-number">2</span>, tw, th)
    <span class="hljs-comment"># image = tf.cast(image, tf.uint8) #/ 255.0</span>

    <span class="hljs-keyword">return</span> image
</code></pre>
<p>Cycle through each ground truth sample label image and create a list of those, <code>L</code>. Make a 4 x 4 subplot plot of the ground truth label images</p>
<pre><code class="hljs css language-python">L = []
plt.figure(figsize=(<span class="hljs-number">24</span>,<span class="hljs-number">24</span>))

<span class="hljs-keyword">for</span> counter,(f,l) <span class="hljs-keyword">in</span> enumerate(zip(sample_filenames, sample_label_filenames)):
    image = seg_file2tensor(f, TARGET_SIZE)
    label = seg_file2tensor(l, TARGET_SIZE)
    label = label.numpy().squeeze()

    label = median(label/<span class="hljs-number">255.</span>, disk(<span class="hljs-number">5</span>)).astype(np.uint8)

    label[image[:,:,<span class="hljs-number">0</span>]==<span class="hljs-number">0</span>] = <span class="hljs-number">0</span> <span class="hljs-comment">#(0,0,0)</span>

    plt.subplot(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,counter+<span class="hljs-number">1</span>)
    name = sample_filenames[counter].split(os.sep)[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'.jpg'</span>)[<span class="hljs-number">0</span>]
    plt.title(name, fontsize=<span class="hljs-number">10</span>)
    plt.imshow(image)

    plt.imshow(label, cmap=cmap, vmin=<span class="hljs-number">0</span>, vmax=<span class="hljs-number">6</span>) <span class="hljs-comment">#8)</span>

    plt.axis(<span class="hljs-string">'off'</span>)
    L.append(label)

plt.savefig(test_samples_fig.replace(<span class="hljs-string">'.png'</span>,<span class="hljs-string">'_gt.png'</span>),
            dpi=<span class="hljs-number">200</span>, bbox_inches=<span class="hljs-string">'tight'</span>)
plt.close(<span class="hljs-string">'all'</span>)
</code></pre>
<p>These are the ground truth labels</p>
<p><img src="/MLMONDAYS/blog/assets/dunes_sample_16class_est16samples_gt.png" alt=""></p>
<p>Cycle through each sample image and use the model to estimate the label image. Compare the one-hot encoded versions of the ground truth and prediction by computing a per-sample IoU score.</p>
<pre><code class="hljs css language-python">IOU = []
plt.figure(figsize=(<span class="hljs-number">24</span>,<span class="hljs-number">24</span>))

<span class="hljs-keyword">for</span> counter,f <span class="hljs-keyword">in</span> enumerate(sample_filenames):
    image = seg_file2tensor(f, TARGET_SIZE)/<span class="hljs-number">255</span>
    est_label = model.predict(tf.expand_dims(image, <span class="hljs-number">0</span>) , batch_size=<span class="hljs-number">1</span>).squeeze()

    est_labelp = tf.argmax(est_label, axis=<span class="hljs-number">-1</span>)
    l = tf.one_hot(tf.cast(L[counter], tf.uint8), <span class="hljs-number">7</span>) <span class="hljs-comment">#9)</span>
    iou = mean_iou_np(np.expand_dims(l.numpy(),<span class="hljs-number">0</span>), np.expand_dims(est_label,<span class="hljs-number">0</span>))

    plt.subplot(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,counter+<span class="hljs-number">1</span>)
    name = sample_filenames[counter].split(os.sep)[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'.jpg'</span>)[<span class="hljs-number">0</span>]
    plt.title(name+<span class="hljs-string">' '</span>+str(iou)[:<span class="hljs-number">5</span>], fontsize=<span class="hljs-number">12</span>)
    plt.imshow(image)

    plt.imshow(est_labelp, alpha=<span class="hljs-number">0.5</span>, cmap=cmap, vmin=<span class="hljs-number">0</span>, vmax=<span class="hljs-number">6</span>) <span class="hljs-comment">#8)</span>

    plt.axis(<span class="hljs-string">'off'</span>)
    <span class="hljs-keyword">del</span> est_labelp

    IOU.append(iou)

plt.savefig(test_samples_fig,
            dpi=<span class="hljs-number">200</span>, bbox_inches=<span class="hljs-string">'tight'</span>)
plt.close(<span class="hljs-string">'all'</span>)
</code></pre>
<p>These are the predictions:</p>
<p><img src="/MLMONDAYS/blog/assets/dunes_sample_16class_est16samples.png" alt=""></p>
<p>As you can see, the model does well at delineating the landscape but doesn't always get the class prediction correct. IoU scores are fairly low, 0.2, 0.4. The mean is only 0.3. This, I would confidently predict, is due to a lack of data. Big neural networks like this are designed for more independent examples.</p>
<p>Next we'll show the workflow and performance of a model trained to predict based on DEM alone</p>
<h2><a class="anchor" aria-hidden="true" id="dem-imagery"></a><a href="#dem-imagery" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>DEM imagery</h2>
<h3><a class="anchor" aria-hidden="true" id="model-preparation-1"></a><a href="#model-preparation-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model preparation</h3>
<p>For 2D imagery, we'll choose the DEM (last) channel in the 4D TFRecord stacks. SO we'll use the same function as before except returning <code>image[:,:,-1]</code> as the dem</p>
<pre><code class="hljs css language-python"><span class="hljs-meta">@tf.autograph.experimental.do_not_convert</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_seg_tfrecord_dunes</span><span class="hljs-params">(example)</span>:</span>
    features = {
        <span class="hljs-string">"image"</span>: tf.io.FixedLenFeature([], tf.string),  <span class="hljs-comment"># tf.string = bytestring (not text string)</span>
        <span class="hljs-string">"label"</span>: tf.io.FixedLenFeature([], tf.string),   <span class="hljs-comment"># shape [] means scalar</span>
    }
    <span class="hljs-comment"># decode the TFRecord</span>
    example = tf.io.parse_single_example(example, features)

    image = tf.image.decode_png(example[<span class="hljs-string">'image'</span>], channels=<span class="hljs-number">1</span>)
    image = tf.cast(image, tf.float32)/ <span class="hljs-number">255.0</span>

    label = tf.image.decode_jpeg(example[<span class="hljs-string">'label'</span>], channels=<span class="hljs-number">1</span>)
    label = tf.cast(label, tf.uint8)

    cond = tf.greater(label, tf.ones(tf.shape(label),dtype=tf.uint8)*<span class="hljs-number">6</span>) <span class="hljs-comment">#8)</span>
    label = tf.where(cond, tf.ones(tf.shape(label),dtype=tf.uint8)*<span class="hljs-number">0</span>, label)

    label = tf.one_hot(tf.cast(label, tf.uint8), <span class="hljs-number">7</span>) <span class="hljs-comment">#9)</span>
    label = tf.squeeze(label)

    <span class="hljs-keyword">return</span> image[:,:,<span class="hljs-number">-1</span>], label
</code></pre>
<p>Redefine the variables for new files to contain the 2D results</p>
<pre><code class="hljs css language-python">data_path= os.getcwd()+os.sep+<span class="hljs-string">"data/dunes"</span>

filepath = os.getcwd()+os.sep+<span class="hljs-string">'results/dunes2d_8class_best_weights_model.h5'</span>

hist_fig = os.getcwd()+os.sep+<span class="hljs-string">'results/dunes2d_8class_model.png'</span>

filenames = sorted(tf.io.gfile.glob(data_path+os.sep+<span class="hljs-string">'dunes4d*.tfrec'</span>))

nb_images = ims_per_shard * len(filenames)
print(nb_images)

split = int(len(filenames) * VALIDATION_SPLIT)

training_filenames = filenames[split:]
validation_filenames = filenames[:split]

validation_steps = int(nb_images // len(filenames) * len(validation_filenames)) // BATCH_SIZE
steps_per_epoch = int(nb_images // len(filenames) * len(training_filenames)) // BATCH_SIZE
</code></pre>
<pre><code class="hljs css language-python">train_ds = get_training_dataset()
val_ds = get_validation_dataset()
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="model-training-1"></a><a href="#model-training-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training</h3>
<p>Notice that the model is compiled with the input size <code>(TARGET_SIZE, TARGET_SIZE, 1)</code> rather than <code>(TARGET_SIZE, TARGET_SIZE, 3)</code> as before. Everything else is the same</p>
<pre><code class="hljs css language-python">model = res_unet((TARGET_SIZE, TARGET_SIZE, <span class="hljs-number">1</span>), BATCH_SIZE, <span class="hljs-string">'multiclass'</span>, nclasses)
<span class="hljs-comment">#model.compile(optimizer = 'adam', loss = tf.keras.losses.CategoricalHinge(), metrics = [mean_iou])</span>
model.compile(optimizer = <span class="hljs-string">'adam'</span>, loss = <span class="hljs-string">'categorical_crossentropy'</span>, metrics = [mean_iou])

earlystop = EarlyStopping(monitor=<span class="hljs-string">"val_loss"</span>,
                              mode=<span class="hljs-string">"min"</span>, patience=patience)

model_checkpoint = ModelCheckpoint(filepath, monitor=<span class="hljs-string">'val_loss'</span>,
                                verbose=<span class="hljs-number">0</span>, save_best_only=<span class="hljs-literal">True</span>, mode=<span class="hljs-string">'min'</span>,
                                save_weights_only = <span class="hljs-literal">True</span>)
</code></pre>
<p>We've decreased the data size, so I'm inclined to increase the learning rate a little.  We'll set the parameters and redefine <code>lrfn</code></p>
<pre><code class="hljs css language-python">start_lr = <span class="hljs-number">1e-4</span>
min_lr = start_lr
max_lr = <span class="hljs-number">1e-3</span>
rampup_epochs = <span class="hljs-number">5</span>
sustain_epochs = <span class="hljs-number">0</span>
exp_decay = <span class="hljs-number">.9</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lrfn</span><span class="hljs-params">(epoch)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lr</span><span class="hljs-params">(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)</span>:</span>
        <span class="hljs-keyword">if</span> epoch &lt; rampup_epochs:
            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr
        <span class="hljs-keyword">elif</span> epoch &lt; rampup_epochs + sustain_epochs:
            lr = max_lr
        <span class="hljs-keyword">else</span>:
            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr
        <span class="hljs-keyword">return</span> lr
    <span class="hljs-keyword">return</span> lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)


lr_callback = tf.keras.callbacks.LearningRateScheduler(<span class="hljs-keyword">lambda</span> epoch: lrfn(epoch), verbose=<span class="hljs-literal">True</span>)
</code></pre>
<p>Fir the model and plot the training history as before</p>
<pre><code class="hljs css language-python">callbacks = [model_checkpoint, earlystop, lr_callback]

model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=MAX_EPOCHS,
                      validation_data=val_ds, validation_steps=validation_steps,
                      callbacks=callbacks)


history = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=MAX_EPOCHS,
                      validation_data=val_ds, validation_steps=validation_steps,
                      callbacks=callbacks)

plot_seg_history_iou(history, hist_fig)

plt.close(<span class="hljs-string">'all'</span>)
K.clear_session()
</code></pre>
<p><img src="/MLMONDAYS/blog/assets/dunes2d_8class_model.png" alt=""></p>
<h3><a class="anchor" aria-hidden="true" id="model-evaluation-1"></a><a href="#model-evaluation-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model evaluation</h3>
<p>Evaluate the model in the same way as for the 3D imagery case</p>
<pre><code class="hljs css language-python">scores = model.evaluate(val_ds, steps=validation_steps)

print(<span class="hljs-string">'loss={loss:0.4f}, Mean IOU={iou:0.4f}'</span>.format(loss=scores[<span class="hljs-number">0</span>], iou=scores[<span class="hljs-number">1</span>]))

loss=<span class="hljs-number">0.5537</span>, Mean IOU=<span class="hljs-number">0.9835</span>
</code></pre>
<pre><code class="hljs css language-python">sample_data_path = os.getcwd()+os.sep+<span class="hljs-string">'data/dunes/dems/files'</span>

test_samples_fig = os.getcwd()+os.sep+<span class="hljs-string">'dunes2d_sample_16class_est16samples.png'</span>

sample_filenames = sorted(tf.io.gfile.glob(sample_data_path+os.sep+<span class="hljs-string">'*.jpg'</span>))

IOU = []
plt.figure(figsize=(<span class="hljs-number">24</span>,<span class="hljs-number">24</span>))

<span class="hljs-keyword">for</span> counter,f <span class="hljs-keyword">in</span> enumerate(sample_filenames):
    image = seg_file2tensor(f, TARGET_SIZE)/<span class="hljs-number">255</span>
    est_label = model.predict(tf.expand_dims(image, <span class="hljs-number">0</span>) , batch_size=<span class="hljs-number">1</span>).squeeze()

    est_labelp = tf.argmax(est_label, axis=<span class="hljs-number">-1</span>)
    l = tf.one_hot(tf.cast(L[counter], tf.uint8), <span class="hljs-number">7</span>) <span class="hljs-comment">#9)</span>
    iou = mean_iou_np(np.expand_dims(l.numpy(),<span class="hljs-number">0</span>), np.expand_dims(est_label,<span class="hljs-number">0</span>))

    plt.subplot(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,counter+<span class="hljs-number">1</span>)
    name = sample_filenames[counter].split(os.sep)[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'.jpg'</span>)[<span class="hljs-number">0</span>]
    plt.title(name+<span class="hljs-string">' '</span>+str(iou)[:<span class="hljs-number">5</span>], fontsize=<span class="hljs-number">8</span>)
    plt.imshow(image, cmap=plt.cm.gray)

    plt.imshow(est_labelp, alpha=<span class="hljs-number">0.5</span>, cmap=cmap, vmin=<span class="hljs-number">0</span>, vmax=<span class="hljs-number">6</span>) <span class="hljs-comment">#8)</span>

    plt.axis(<span class="hljs-string">'off'</span>)
    <span class="hljs-keyword">del</span> est_labelp

    IOU.append(iou)

plt.savefig(test_samples_fig,
            dpi=<span class="hljs-number">200</span>, bbox_inches=<span class="hljs-string">'tight'</span>)
plt.close(<span class="hljs-string">'all'</span>)
</code></pre>
<p>Model not performing well at all on DEM data alone. A mean IoU score of around 0.1. This isn't particularly surprising; elevation is a poor descriptor of these classes alone, since marsh and beach are the same elevation, and bare and vegetated established dunes are also similar elevations.</p>
<p><img src="/MLMONDAYS/blog/assets/dunes2d_sample_16class_est16samples.png" alt=""></p>
<h2><a class="anchor" aria-hidden="true" id="rgb--dem-imagery"></a><a href="#rgb--dem-imagery" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RGB + DEM imagery</h2>
<p>By combining RGB and DEM information together, the hope is that the model can exploit classes such as incipient foredune and iceplant that have distinct elevation zones, and make better distinctions between the other classes that differ in elevation characteristics.</p>
<h3><a class="anchor" aria-hidden="true" id="model-preparation-2"></a><a href="#model-preparation-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model preparation</h3>
<pre><code class="hljs css language-python"><span class="hljs-meta">@tf.autograph.experimental.do_not_convert</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">read_seg_tfrecord_dunes</span><span class="hljs-params">(example)</span>:</span>
    features = {
        <span class="hljs-string">"image"</span>: tf.io.FixedLenFeature([], tf.string),  <span class="hljs-comment"># tf.string = bytestring (not text string)</span>
        <span class="hljs-string">"label"</span>: tf.io.FixedLenFeature([], tf.string),   <span class="hljs-comment"># shape [] means scalar</span>
    }
    <span class="hljs-comment"># decode the TFRecord</span>
    example = tf.io.parse_single_example(example, features)

    image = tf.image.decode_png(example[<span class="hljs-string">'image'</span>], channels=<span class="hljs-number">4</span>)
    image = tf.cast(image, tf.float32)/ <span class="hljs-number">255.0</span>

    label = tf.image.decode_jpeg(example[<span class="hljs-string">'label'</span>], channels=<span class="hljs-number">1</span>)
    label = tf.cast(label, tf.uint8)

    cond = tf.greater(label, tf.ones(tf.shape(label),dtype=tf.uint8)*<span class="hljs-number">6</span>) <span class="hljs-comment">#8)</span>
    label = tf.where(cond, tf.ones(tf.shape(label),dtype=tf.uint8)*<span class="hljs-number">0</span>, label)

    label = tf.one_hot(tf.cast(label, tf.uint8), <span class="hljs-number">7</span>) <span class="hljs-comment">#9)</span>
    label = tf.squeeze(label)

    <span class="hljs-keyword">return</span> image, label
</code></pre>
<pre><code class="hljs css language-python">data_path= os.getcwd()+os.sep+<span class="hljs-string">"data/dunes"</span>

filepath = os.getcwd()+os.sep+<span class="hljs-string">'results/dunes4d_8class_best_weights_model.h5'</span>

hist_fig = os.getcwd()+os.sep+<span class="hljs-string">'results/dunes4d_8class_model.png'</span>

filenames = sorted(tf.io.gfile.glob(data_path+os.sep+<span class="hljs-string">'dunes4d*.tfrec'</span>))

nb_images = ims_per_shard * len(filenames)
print(nb_images)

split = int(len(filenames) * VALIDATION_SPLIT)

training_filenames = filenames[split:]
validation_filenames = filenames[:split]

validation_steps = int(nb_images // len(filenames) * len(validation_filenames)) // BATCH_SIZE
steps_per_epoch = int(nb_images // len(filenames) * len(training_filenames)) // BATCH_SIZE

train_ds = get_training_dataset()

val_ds = get_validation_dataset()
</code></pre>
<p>Another thing you could play with is the kernel size used in the convolutional layers of the UNet. Previously that was set to 3 by default. Below I increase that to 5, in the hope a larger receptive field will mean greater elevation-image covariation scales to be captured.</p>
<pre><code class="hljs css language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">res_unet</span><span class="hljs-params">(sz, f, flag, nclasses=<span class="hljs-number">1</span>)</span>:</span>
    inputs = tf.keras.layers.Input(sz)

    <span class="hljs-comment">## downsample</span>
    e1 = bottleneck_block(inputs, f, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)); f = int(f*<span class="hljs-number">2</span>)
    e2 = res_block(e1, f, strides=<span class="hljs-number">2</span>, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)); f = int(f*<span class="hljs-number">2</span>)
    e3 = res_block(e2, f, strides=<span class="hljs-number">2</span>, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)); f = int(f*<span class="hljs-number">2</span>)
    e4 = res_block(e3, f, strides=<span class="hljs-number">2</span>, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)); f = int(f*<span class="hljs-number">2</span>)
    _ = res_block(e4, f, strides=<span class="hljs-number">2</span>, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))

    <span class="hljs-comment">## bottleneck</span>
    b0 = conv_block(_, f, strides=<span class="hljs-number">1</span>)
    _ = conv_block(b0, f, strides=<span class="hljs-number">1</span>)

    <span class="hljs-comment">## upsample</span>
    _ = upsamp_concat_block(_, e4)
    _ = res_block(_, f, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)); f = int(f/<span class="hljs-number">2</span>)

    _ = upsamp_concat_block(_, e3)
    _ = res_block(_, f, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)); f = int(f/<span class="hljs-number">2</span>)

    _ = upsamp_concat_block(_, e2)
    _ = res_block(_, f, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>)); f = int(f/<span class="hljs-number">2</span>)

    _ = upsamp_concat_block(_, e1)
    _ = res_block(_, f, kernel_size=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))

    <span class="hljs-comment">## classify</span>
    <span class="hljs-keyword">if</span> flag <span class="hljs-keyword">is</span> <span class="hljs-string">'binary'</span>:
        outputs = tf.keras.layers.Conv2D(nclasses, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"sigmoid"</span>)(_)
    <span class="hljs-keyword">else</span>:
        outputs = tf.keras.layers.Conv2D(nclasses, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"softmax"</span>)(_)

    <span class="hljs-comment">#model creation</span>
    model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])
    <span class="hljs-keyword">return</span> model
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="model-training-2"></a><a href="#model-training-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training</h3>
<p>Everything the same as before except the <code>(TARGET_SIZE, TARGET_SIZE, 4)</code> indicating a 4th input dimension</p>
<pre><code class="hljs css language-python">model = res_unet((TARGET_SIZE, TARGET_SIZE, <span class="hljs-number">4</span>), BATCH_SIZE, <span class="hljs-string">'multiclass'</span>, nclasses)
<span class="hljs-comment"># model.compile(optimizer = 'adam', loss = tf.keras.losses.CategoricalHinge(), metrics = [mean_iou])</span>
model.compile(optimizer = <span class="hljs-string">'adam'</span>, loss = <span class="hljs-string">'categorical_crossentropy'</span>, metrics = [mean_iou])

earlystop = EarlyStopping(monitor=<span class="hljs-string">"val_loss"</span>,
                              mode=<span class="hljs-string">"min"</span>, patience=patience)

model_checkpoint = ModelCheckpoint(filepath, monitor=<span class="hljs-string">'val_loss'</span>,
                                verbose=<span class="hljs-number">0</span>, save_best_only=<span class="hljs-literal">True</span>, mode=<span class="hljs-string">'min'</span>,
                                save_weights_only = <span class="hljs-literal">True</span>)
</code></pre>
<p>Increase learning rate (again, we're just simulating things you could change rather than necessarily be the optimal hyperparameters)</p>
<pre><code class="hljs css language-python">start_lr = <span class="hljs-number">1e-6</span> <span class="hljs-comment">#0.00001</span>
min_lr = start_lr
max_lr = <span class="hljs-number">1e-3</span>
rampup_epochs = <span class="hljs-number">5</span>
sustain_epochs = <span class="hljs-number">0</span>
exp_decay = <span class="hljs-number">.9</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lrfn</span><span class="hljs-params">(epoch)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lr</span><span class="hljs-params">(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)</span>:</span>
        <span class="hljs-keyword">if</span> epoch &lt; rampup_epochs:
            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr
        <span class="hljs-keyword">elif</span> epoch &lt; rampup_epochs + sustain_epochs:
            lr = max_lr
        <span class="hljs-keyword">else</span>:
            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr
        <span class="hljs-keyword">return</span> lr
    <span class="hljs-keyword">return</span> lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)


lr_callback = tf.keras.callbacks.LearningRateScheduler(<span class="hljs-keyword">lambda</span> epoch: lrfn(epoch), verbose=<span class="hljs-literal">True</span>)

callbacks = [model_checkpoint, earlystop, lr_callback]
</code></pre>
<p>Fit the model</p>
<pre><code class="hljs css language-python"><span class="hljs-comment">#warm start</span>
model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=MAX_EPOCHS,
                      validation_data=val_ds, validation_steps=validation_steps,
                      callbacks=callbacks)

history = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=MAX_EPOCHS,
                      validation_data=val_ds, validation_steps=validation_steps,
                      callbacks=callbacks)

plot_seg_history_iou(history, hist_fig)

plt.close(<span class="hljs-string">'all'</span>)
K.clear_session()
</code></pre>
<p><img src="/MLMONDAYS/blog/assets/dunes4d_8class_model.png" alt=""></p>
<h3><a class="anchor" aria-hidden="true" id="model-evaluation-2"></a><a href="#model-evaluation-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model evaluation</h3>
<p>Evaluate the same way as previously</p>
<pre><code class="hljs css language-python">scores = model.evaluate(val_ds, steps=validation_steps)

print(<span class="hljs-string">'loss={loss:0.4f}, Mean IOU={iou:0.4f}'</span>.format(loss=scores[<span class="hljs-number">0</span>], iou=scores[<span class="hljs-number">1</span>]))

loss=<span class="hljs-number">0.2936</span>, Mean IOU=<span class="hljs-number">0.9748</span>
</code></pre>
<p>Almost identical to the 3D example</p>
<pre><code class="hljs css language-python">sample_data_path = os.getcwd()+os.sep+<span class="hljs-string">'data/dunes/images/files'</span>

test_samples_fig = os.getcwd()+os.sep+<span class="hljs-string">'dunes4d_sample_16class_est16samples.png'</span>

sample_filenames = sorted(tf.io.gfile.glob(sample_data_path+os.sep+<span class="hljs-string">'*.jpg'</span>))
</code></pre>
<pre><code class="hljs css language-python">IOU = []
plt.figure(figsize=(<span class="hljs-number">24</span>,<span class="hljs-number">24</span>))

<span class="hljs-keyword">for</span> counter,f <span class="hljs-keyword">in</span> enumerate(sample_filenames):
    image = seg_file2tensor(f, TARGET_SIZE)/<span class="hljs-number">255</span>

    dem = seg_file2tensor(f.replace(<span class="hljs-string">'images'</span>,<span class="hljs-string">'dems'</span>).replace(<span class="hljs-string">'ortho'</span>,<span class="hljs-string">'dem'</span>), TARGET_SIZE)/<span class="hljs-number">255</span>

    merged = np.dstack((image.numpy(), dem.numpy()[:,:,<span class="hljs-number">0</span>]))

    est_label = model.predict(tf.expand_dims(merged, <span class="hljs-number">0</span>) , batch_size=<span class="hljs-number">1</span>).squeeze()

    l = tf.one_hot(tf.cast(L[counter], tf.uint8), <span class="hljs-number">7</span>) <span class="hljs-comment">#9)</span>
    iou = mean_iou_np(np.expand_dims(l.numpy(),<span class="hljs-number">0</span>), np.expand_dims(est_label,<span class="hljs-number">0</span>))

    est_label = tf.argmax(est_label, axis=<span class="hljs-number">-1</span>)

    plt.subplot(<span class="hljs-number">4</span>,<span class="hljs-number">4</span>,counter+<span class="hljs-number">1</span>)
    name = sample_filenames[counter].split(os.sep)[<span class="hljs-number">-1</span>].split(<span class="hljs-string">'.jpg'</span>)[<span class="hljs-number">0</span>]
    plt.title(name, fontsize=<span class="hljs-number">10</span>)
    plt.imshow(dem, cmap=plt.cm.gray)

    plt.imshow(est_label, alpha=<span class="hljs-number">0.5</span>, cmap=cmap, vmin=<span class="hljs-number">0</span>, vmax=<span class="hljs-number">6</span>) <span class="hljs-comment">#8)</span>

    plt.axis(<span class="hljs-string">'off'</span>)

    IOU.append(iou)

<span class="hljs-comment"># plt.show()</span>
plt.savefig(test_samples_fig,
            dpi=<span class="hljs-number">200</span>, bbox_inches=<span class="hljs-string">'tight'</span>)
plt.close(<span class="hljs-string">'all'</span>)
</code></pre>
<p>Again, only an IOU of 0.32 -  a marginal improvement over the 3D data. But overall I conclude that 1) you can use 2D, 3D, or 4D imagery with a U-Net and get a reasonable segmentation, however 2) I hypothesize that this workflow requires much more data. I achieved similar results with 8 classes.</p>
<p><img src="/MLMONDAYS/blog/assets/dunes4d_sample_16class_est16samples.png" alt=""></p>
</span></div></div><div class="blogSocialSection"></div></div><div class="blog-recent"><a class="button" href="/MLMONDAYS/blog/">Recent Posts</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#rgb-imagery">RGB imagery</a><ul class="toc-headings"><li><a href="#model-preparation">Model preparation</a></li><li><a href="#model-training">Model training</a></li><li><a href="#model-evaluation">Model evaluation</a></li></ul></li><li><a href="#dem-imagery">DEM imagery</a><ul class="toc-headings"><li><a href="#model-preparation-1">Model preparation</a></li><li><a href="#model-training-1">Model training</a></li><li><a href="#model-evaluation-1">Model evaluation</a></li></ul></li><li><a href="#rgb--dem-imagery">RGB + DEM imagery</a><ul class="toc-headings"><li><a href="#model-preparation-2">Model preparation</a></li><li><a href="#model-training-2">Model training</a></li><li><a href="#model-evaluation-2">Model evaluation</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/MLMONDAYS/" class="nav-home"><img src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;" width="66" height="58"/></a><div><h5>Internal links</h5><a href="/MLMONDAYS/docs/en/doc1.html">Docs</a><a href="/MLMONDAYS/docs/en/doc2.html">Data</a><a href="/MLMONDAYS/docs/en/doc3.html">Help</a></div><div><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://www.usgs.gov/centers/cdi" target="_blank" rel="noreferrer noopener">USGS Community for Data Integration (CDI)</a><a href="https://www.usgs.gov/centers/pcmsc/science/remote-sensing-coastal-change?qt-science_center_objects=0#qt-science_center_objects" target="_blank" rel="noreferrer noopener">USGS Remote Sensing Coastal Change Project</a><a href="https://www.danielbuscombe.com" target="_blank" rel="noreferrer noopener">www.danielbuscombe.com</a></div><div><h5>More</h5><a href="/MLMONDAYS/blog">Blog</a><a href="https://github.com/dbuscombe-usgs/DL-CDI2020">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a><div class="social"><a href="https://twitter.com/magic_walnut" class="twitter-follow-button">Follow @magic_walnut</a></div></div></section><a href="https://mardascience.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/MLMONDAYS/img/dash-logo-new.png" alt="Marda Science" width="170" height="45"/></a><section class="copyright">Copyright © 2020 Marda Science, LLC</section></footer></div><script>window.twttr=(function(d,s, id){var js,fjs=d.getElementsByTagName(s)[0],t=window.twttr||{};if(d.getElementById(id))return t;js=d.createElement(s);js.id=id;js.src='https://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js, fjs);t._e = [];t.ready = function(f) {t._e.push(f);};return t;}(document, 'script', 'twitter-wjs'));</script></body></html>