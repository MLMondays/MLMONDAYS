---
title: ML terminology, demystified
author: Dan Buscombe
authorURL: http://twitter.com/magic_walnut
---

https://developers.google.com/machine-learning/glossary

### Machine learning (ML), deep learning (DL), and artificial intelligence (AI)

### model

The representation of what a machine learning system has learned from the training data. Within TensorFlow, model is an overloaded term, which can have either of the following two related meanings:

   The TensorFlow graph that expresses the structure of how a prediction will be computed.
   The particular weights and biases of that TensorFlow graph, which are determined by training.



### Data augmentation

### Regularization

The penalty on a model's complexity. Regularization helps prevent overfitting. Different kinds of regularization include:

    L1 regularization
    L2 regularization
    dropout regularization
    early stopping (this is not a formal regularization method, but can effectively limit overfitting)



#### batch normalization

Normalizing the input or output of the activation functions in a hidden layer. Batch normalization can provide the following benefits:

   Make neural networks more stable by protecting against outlier weights.
   Enable higher learning rates.
   Reduce overfitting.

   dropout regularization

  A form of regularization useful in training neural networks. Dropout regularization works by removing a random selection of a fixed number of the units in a network layer for a single gradient step. The more units dropped out, the stronger the regularization. This is analogous to training the network to emulate an exponentially large ensemble of smaller networks. For full details, see Dropout: A Simple Way to Prevent Neural Networks from Overfitting.

  early stopping

 A method for regularization that involves ending model training before training loss finishes decreasing. In early stopping, you end model training when the loss on a validation dataset starts to increase, that is, when generalization performance worsens.


### epoch

A full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch size training iterations, where N is the total number of examples.



### convergence

 Informally, often refers to a state reached during training in which training loss and validation loss change very little or not at all with each iteration after a certain number of iterations. In other words, a model reaches convergence when additional training on the current data will not improve the model. In deep learning, loss values sometimes stay constant or nearly so for many iterations before finally descending, temporarily producing a false sense of convergence.

 See also early stopping.  

 convolutional neural network
 #image

 A neural network in which at least one layer is a convolutional layer. A typical convolutional neural network consists of some combination of the following layers:

     convolutional layers
     pooling layers
     dense layers

 Convolutional neural networks have had great success in certain kinds of problems, such as image recognition.


 cross-entropy

A generalization of Log Loss to multi-class classification problems. Cross-entropy quantifies the difference between two probability distributions. See also perplexity.


discriminative model

A model that predicts labels from a set of one or more features. More formally, discriminative models define the conditional probability of an output given the features and weights; that is:

p(output | features, weights)

For example, a model that predicts whether an email is spam from features and weights is a discriminative model.

The vast majority of supervised learning models, including classification and regression models, are discriminative models.

Contrast with generative model.



hyperparameter

The "knobs" that you tweak during successive runs of training a model. For example, learning rate is a hyperparameter.

Contrast with parameter.


loss

A measure of how far a model's predictions are from its label. Or, to phrase it more pessimistically, a measure of how bad the model is. To determine this value, a model must define a loss function. For example, linear regression models typically use mean squared error for a loss function, while logistic regression models use Log Loss.


### one-vs.-all

Given a classification problem with N possible solutions, a one-vs.-all solution consists of N separate binary classifiers—one binary classifier for each possible outcome. For example, given a model that classifies examples as animal, vegetable, or mineral, a one-vs.-all solution would provide the following three separate binary classifiers:

    animal vs. not animal
    vegetable vs. not vegetable
    mineral vs. not mineral

this is the approach we take in segmentation



### one-shot learning

A machine learning approach, often used for object classification, designed to learn effective classifiers from a single training example.

this is the approach we take in object detection




### optimizer

A specific implementation of the gradient descent algorithm. TensorFlow's base class for optimizers is tf.train.Optimizer. Popular optimizers include:

    AdaGrad, which stands for ADAptive GRADient descent.
    Adam, which stands for ADAptive with Momentum.

Different optimizers may leverage one or more of the following concepts to enhance the effectiveness of gradient descent on a given training set:

    momentum (Momentum)
    update frequency
    sparsity/regularization (Ftrl)
    more complex math (Proximal, and others)

You might even imagine an NN-driven optimizer.


### semi-supervised learning

Training a model on data where some of the training examples have labels but others don’t. One technique for semi-supervised learning is to infer labels for the unlabeled examples, and then to train on the inferred labels to create a new model. Semi-supervised learning can be useful if labels are expensive to obtain but unlabeled examples are plentiful.


### supervised machine learning

Training a model from input data and its corresponding labels. Supervised machine learning is analogous to a student learning a subject by studying a set of questions and their corresponding answers. After mastering the mapping between questions and answers, the student can then provide answers to new (never-before-seen) questions on the same topic. Compare with unsupervised machine learning.


## transfer learning

Transferring information from one machine learning task to another. For example, in multi-task learning, a single model solves multiple tasks, such as a deep model that has different output nodes for different tasks. Transfer learning might involve transferring knowledge from the solution of a simpler task to a more complex one, or involve transferring knowledge from a task where there is more data to one where there is less data.

Most machine learning systems solve a single task. Transfer learning is a baby step towards artificial intelligence in which a single program can solve multiple tasks.
