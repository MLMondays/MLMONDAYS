<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Documentation · &quot;ML Mondays&quot;</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="ML-Mondays consists of 4 in-person classes, on Oct 5, Oct 13 (a day delayed, due to the Federal Holiday Columbus Day), Oct 19, and Oct 26. Each class follows on from the last. Classes 1 and 4 are pairs, as are classes 2 and 3. Participants are therefore expected to last the course. Optional homework assignments will be set for participants to carry out in their own time."/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Documentation · &quot;ML Mondays&quot;"/><meta property="og:type" content="website"/><meta property="og:url" content="https://dbuscombe-usgs.github.io/MLMONDAYS/"/><meta property="og:description" content="ML-Mondays consists of 4 in-person classes, on Oct 5, Oct 13 (a day delayed, due to the Federal Holiday Columbus Day), Oct 19, and Oct 26. Each class follows on from the last. Classes 1 and 4 are pairs, as are classes 2 and 3. Participants are therefore expected to last the course. Optional homework assignments will be set for participants to carry out in their own time."/><meta property="og:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/MLMONDAYS/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/atom.xml" title="&quot;ML Mondays&quot; Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/feed.xml" title="&quot;ML Mondays&quot; Blog RSS Feed"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/MLMONDAYS/js/scrollSpy.js"></script><link rel="stylesheet" href="/MLMONDAYS/css/main.css"/><script src="/MLMONDAYS/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/MLMONDAYS/"><img class="logo" src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;"/><h2 class="headerTitleWithLogo">&quot;ML Mondays&quot;</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive siteNavItemActive"><a href="/MLMONDAYS/docs/doc1" target="_self">Docs</a></li><li class="siteNavGroupActive"><a href="/MLMONDAYS/docs/doc2" target="_self">Data</a></li><li class="siteNavGroupActive"><a href="/MLMONDAYS/docs/doc3" target="_self">Models</a></li><li class="siteNavGroupActive"><a href="/MLMONDAYS/docs/doc4" target="_self">API</a></li><li class=""><a href="/MLMONDAYS/help" target="_self">Help</a></li><li class=""><a href="/MLMONDAYS/blog/" target="_self">Blog</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>ML Mondays</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">ML Mondays</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/MLMONDAYS/docs/doc1">Overview</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Data</h3><ul class=""><li class="navListItem"><a class="navItem" href="/MLMONDAYS/docs/doc2">Data</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Models and Workflows</h3><ul class=""><li class="navListItem"><a class="navItem" href="/MLMONDAYS/docs/doc3">Models</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">API</h3><ul class=""><li class="navListItem"><a class="navItem" href="/MLMONDAYS/docs/doc4">ML Mondays API</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Documentation</h1></header><article><div><span><p>ML-Mondays consists of 4 in-person classes, on Oct 5, Oct 13 (a day delayed, due to the Federal Holiday Columbus Day), Oct 19, and Oct 26. Each class follows on from the last. Classes 1 and 4 are pairs, as are classes 2 and 3. Participants are therefore expected to last the course. Optional homework assignments will be set for participants to carry out in their own time.</p>
<hr>
<h2><a class="anchor" aria-hidden="true" id="useful-links"></a><a href="#useful-links" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Useful Links</h2>
<h3><a class="anchor" aria-hidden="true" id="reference"></a><a href="#reference" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reference</h3>
<ul>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS">Github repository</a> (where all the code and website lives)</p></li>
<li><p><a href="https://dbuscombe-usgs.github.io/MLMONDAYS/docs/doc1">Documentation</a></p></li>
<li><p><a href="https://dbuscombe-usgs.github.io/MLMONDAYS/docs/doc4">API</a> (list of all code functions, their inputs and outputs and what they do)</p></li>
<li><p><a href="https://dbuscombe-usgs.github.io/MLMONDAYS/docs/doc1">Summary of models</a></p></li>
<li><p><a href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/">Blog</a></p></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="notebooks"></a><a href="#notebooks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Notebooks</h3>
<ul>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/1_ImageRecog/notebooks/MLMondays_week1_live_partA.ipynb">Part 1a jupyter notebook</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/1_ImageRecog/notebooks/MLMondays_week1_live_partA_colab.ipynb">Part 1a jupyter notebook for Colab</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/1_ImageRecog/notebooks/MLMondays_week1_live_partB.ipynb">Part 1b jupyter notebook</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/2_ObjRecog/notebooks/MLMondays_week2_live.ipynb">Part 2 jupyter notebook</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/2_ObjRecog/notebooks/MLMondays_week2_live_colab.ipynb">Part 2 jupyter notebook for Colab</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/1_ImageSeg/notebooks/MLMondays_week3_live.ipynb">Part 3 jupyter notebook for Colab</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/1_ImageSeg/notebooks/MLMondays_week3_live_colab.ipynb">Part 3 jupyter notebook</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/1_ImageUnsupRecog/notebooks/MLMondays_week4_live.ipynb">Part 4 jupyter notebook</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/MLMONDAYS/blob/master/1_ImageUnsupRecog/notebooks/MLMondays_week4_live_colab.ipynb">Part 4 jupyter notebook for Colab</a></p></li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="data"></a><a href="#data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data</h3>
<ul>
<li><p><a href="https://github.com/dbuscombe-usgs/mlmondays_data_imrecog">Part 1 datasets repository</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/mlmondays_data_objrecog">Part 2 datasets repository</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/mlmondays_data_imseg">Part 3 datasets repository</a></p></li>
<li><p><a href="https://github.com/dbuscombe-usgs/mlmondays_data_ssimrecog">Part 4 datasets repository</a></p></li>
</ul>
<hr>
<h2><a class="anchor" aria-hidden="true" id="required-pre-course-reading"></a><a href="#required-pre-course-reading" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Required pre-course reading</h2>
<p><img src="/MLMONDAYS/docs/assets/phd.png" alt=""></p>
<p>Martin Gorner's 123 min (approx.) course called <a href="https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist/#0">TensorFlow, Keras and deep learning, without a PhD</a> is a clear, approachable, fun introduction to neural networks. It is required pre-course reading for participants.</p>
<h2><a class="anchor" aria-hidden="true" id="suggested-pre-requisites"></a><a href="#suggested-pre-requisites" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Suggested pre-requisites</h2>
<p>To gain more familiarity with machine learning and deep learning concepts and terminology, I recommend the following resources:</p>
<ul>
<li>a <a href="https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/">great blog</a> on the RetinaNet model for object detection</li>
<li>a <a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/">visual introduction</a> to machine learning</li>
<li>a recent <a href="https://arxiv.org/abs/2006.13311">review</a> of machine learning in geoscience</li>
<li>this recent deep learning <a href="https://dennybritz.com/blog/deep-learning-most-important-ideas/">review</a> and this reading <a href="https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap">roadmap</a></li>
<li>Daniel Worrall's excellent introductory lecture to machine learning <a href="https://www.youtube.com/watch?v=FrbWQDdGpHQ&amp;feature=youtu.be&amp;t=40">video</a> and <a href="https://deworrall92.github.io/docs/MLSSIndo1_lo_res.pdf">slides</a></li>
<li>Daniel Worrall's introductory lecture to machine learning <a href="https://www.youtube.com/watch?v=K59cmobQKew&amp;feature=youtu.be&amp;t=270">video</a> and <a href="https://deworrall92.github.io/docs/MLSSIndo2_lo_res.pdf">slides</a></li>
<li>a <a href="https://www.notion.so/fd42b6a13305452ba17a5e2fa71467a2?v=7d56617d132e4ec3b98121ae1070f024">list of resources</a> for machine learning application in remote sensing</li>
<li>a <a href="https://sayak.dev/tf.keras/data_augmentation/image/2020/05/10/augmemtation-recipes.html">blog</a> on different tensorflow/keras data data augmentation recipes</li>
<li>an overview of <a href="https://ruder.io/optimizing-gradient-descent/">gradient descent</a></li>
</ul>
<p>(note: in the following <code>recognition</code> and <code>segmentation</code> are terms that imply specific forms of the more general term, <code>classification</code>)</p>
<hr>
<h2><a class="anchor" aria-hidden="true" id="week-1-supervised-image-recognition"></a><a href="#week-1-supervised-image-recognition" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Week 1: Supervised Image Recognition</h2>
<p><em>A) Live session</em>: we'll work through jupyter notebooks containing workflows for image recognition (whole image classification). We'll be trying to answer the question, <code>How much of the Texas coastline is developed?</code>. To answer this question, we will train a deep learning model to classify aerial (oblique) images of the Texas coast, categorized into several natural and non-natural landuse/cover classes. See the <a href="doc2#how-much-of-the-texas-coastline-is-developed">data page</a> for more information on the dataset.</p>
<h4><a class="anchor" aria-hidden="true" id="data-visualization"></a><a href="#data-visualization" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Visualization</h4>
<p>We're going to spend some time visualizing data, to introduce some techniques to do so that might be helpful in designing your own class labels for your own datasets. This is a practical class!</p>
<p>The visualizations will include mean images per class, per-channel, per-class histograms of image values, and dimensionality reduction techniques that might help both visualize and inform categorization of imagery. We'll see that the class boundaries are not at all distinct for the TAMUCC dataset - class boundaries are not distinct in terms of easily extractable image features. This exercise should convince you of the need for a more powerful supervised model</p>
<p>Finally, we'll see how the class boundaries in the NWPU dataset are better visualized using an unsupervised dimensionality reduction approach</p>
<h4><a class="anchor" aria-hidden="true" id="image-recognition-model-training-workflow"></a><a href="#image-recognition-model-training-workflow" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Image recognition model training workflow</h4>
<p>In this lesson, we will train a neural network 'end to end' in an extremely discriminative approach that explicitly maps the classes to the image features, and optimized to extract the features that explicitly predict the class. The network works by linking an image feature extractor to a classifying head, such that feature extraction is limited to only those that help predict the class. The feature extraction therefore results in classification directly.</p>
<p>For datasets where classes are obviously distinct, this is an extremely successful approach. We will see this with the NWPU dataset. However, for the TAMUCC dataset, where there is a lot more variability within classes and a lot less variability within classes, we will see how successful this approach is.</p>
<ol>
<li>Set up a data workflow to feed the model as it trains</li>
</ol>
<ul>
<li>use batches fed optimally to the GPU from TFRecord files</li>
<li>use data augmentation as a regularization strategy</li>
<li>split into train (40% of the data) and validation portions (60%)</li>
</ul>
<ol start="2">
<li>Train it</li>
</ol>
<ul>
<li>use transfer learning to train a classifier</li>
<li>use a learning rate scheduler to pass variable learning rates to the model as it trains</li>
<li>use 'early stopping' to base cessation of training on observed plateauing of validation loss</li>
<li>use a checkpoint to monitor validation loss and save the best model weights when the validation loss improves</li>
<li>use class weightings to lessen effects of class imbalance</li>
</ul>
<ol start="3">
<li>Evaluate it</li>
</ol>
<ul>
<li>study the model training history - the loss and accuracy curves of train and validation sets</li>
<li>evaluate the performance of the trained model on the validation set</li>
<li>plot a 'confusion matrix' of correspondences between actual and estimate class labels</li>
<li>read some sample images from file, and use the model for prediction</li>
</ul>
<ol start="4">
<li>Look at results from a  similar workflow on different class subsets</li>
</ol>
<p><em>B) Optional class assignment</em>: an additional dataset will be provided that you can work on using the same models introduced in the class. The <a href="http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html">NWPU-RESISC45</a> is a publicly available benchmark for REmote Sensing Image Scene Classification (RESISC), created by Northwestern Polytechnical University (NWPU). This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. Participants will also be encouraged to adapt what they learned in the class to their own image recognition problems using their own data.</p>
<hr>
<h2><a class="anchor" aria-hidden="true" id="week-2-supervised-image-object-detection"></a><a href="#week-2-supervised-image-object-detection" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Week 2: Supervised Image Object Detection</h2>
<p><em>A) Live session</em>: We'll work through jupyter notebooks containing workflows for image object detection (pixelwise classification). We'll be trying to answer the question, <code>How do people use beaches?</code>. To answer this question, we will train a deep learning model to detect and locate people in webcam (static, oblique) images of a beach in Florida. See the <a href="doc2#how-do-people-use-beaches">data page</a> for more information.</p>
<h4><a class="anchor" aria-hidden="true" id="retinanet"></a><a href="#retinanet" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RetinaNet</h4>
<p>We are going to build a model called RetinaNet, to detect people in images of often-crowded beaches. RetinaNet is a popular single-stage object detector, which is accurate and runs fast. It uses a feature pyramid network to efficiently detect objects at multiple scales and uses a new Focal loss function, to alleviate the problem of the extreme foreground-background class imbalance.</p>
<ul>
<li><a href="https://arxiv.org/abs/1708.02002">Retina paper</a> Lin, T.Y., Goyal, P., Girshick, R., He, K. and Dollár, P., 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988).</li>
<li><a href="https://arxiv.org/abs/1612.03144">Feature pyramid paper</a></li>
</ul>
<p>RetinaNet adopts the Feature Pyramid Network (FPN) proposed by Lin et al. (2017) as its backbone, which is in turn built on top of ResNet-50 in a fully convolutional fashion. The fully convolutional nature enables the network to take an image of an arbitrary size and outputs proportionally sized feature maps at multiple levels in the feature pyramid.</p>
<p>See this excellent <a href="https://blog.zenggyu.com/en/post/2018-12-05/retinanet-explained-and-demystified/">blog post</a> for more information</p>
<h4><a class="anchor" aria-hidden="true" id="fine-tune-with-coco2017-subset"></a><a href="#fine-tune-with-coco2017-subset" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fine-tune with COCO2017 subset</h4>
<p>First, we'll initialize the model with COCO2017 weights for a model trained on the COCO2017 <a href="https://cocodataset.org/#overview">data set</a> consisting of 164,000 images and  897,000  annotated  objects  from 80 categories. The 80 classes include 'person', which is also what we have labeled the SECOORA imagery. The other 79 classes include various types of vehicles, animals, sports, kitchen items, food items, personal accessories, furniture, etc. In other words, objects generally at close scale. In the imagery of beaches, or in imagery of natural environments, the objects of interest may not be so close-up. So, one of the attractive features of RetinaNet is the image pyramiding that detects object at 5 different scales. This implementation is based heavily on code <a href="https://keras.io/examples/vision/retinanet/">here</a>. When we ran <code>download_data.py</code> earlier on, we downloaded the weights and a subset of the COCO imagery <a href="https://github.com/srihari-humbarwadi/datasets/releases">here</a></p>
<p>Next, we'll fine-tune it on the SECOORA imagery, to illustrate the process of fine-tuning a model on similar dataset. We'll adopt a similar workflow to that in Part 1, where we used a learning rate scheduler, early stopping, and model checkpoints</p>
<h4><a class="anchor" aria-hidden="true" id="used-the-model-to-predict-unseen-imagery"></a><a href="#used-the-model-to-predict-unseen-imagery" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Used the model to predict unseen imagery</h4>
<p>Finally, I'll demonstrate the process of how you would use the trained/fined tuned model on sample imagery (jpeg images in a local folder)</p>
<h4><a class="anchor" aria-hidden="true" id="train-a-model-from-scratch"></a><a href="#train-a-model-from-scratch" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Train a model from scratch</h4>
<p>Next we'll show how to train a model from scratch - this takes too much time, so we'll download the weights from this exercise, load them into the model, and finally use for prediction</p>
<p>In the end, we'll see our model trained from scratch is an excellent way to count people on beaches</p>
<p><em>B) Optional class assignment</em>: participants will be encouraged to adapt what they learned in the class to their own object recognition problems using their own data.</p>
<hr>
<h2><a class="anchor" aria-hidden="true" id="week-3-supervised-image-segmentation"></a><a href="#week-3-supervised-image-segmentation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Week 3: Supervised Image Segmentation</h2>
<p><em>A) Live session</em>: We'll work through jupyter notebooks containing workflows for image segmentation (pixelwise classification). We'll be trying to answer the question, <code>How much sand is there in the Outer Banks?</code>. To answer this question, we will train a deep learning model to segment dry sand pixels in aerial (nadir) imagery of the Outer Banks in North Carolina. See the <a href="doc2#how-much-sand-is-there-in-the-outer-banks">data page</a> for more information.</p>
<p>In this exercise, we will build and evaluate models to segment images of beaches. We will do so using two different class subsets on the same set of images. First, a binary segmentation of deep water versus everything else (shallow water and all subaerial environments). Second, four classes; deep water, shallow water, broken water (surf, swash), and dry land.</p>
<p>To this this, we construct a model that is a U-Net with residual (or lateral/skip connections). For binary segmentation, we use binary image masks, which in turn dictates use of binary categorical crossentropy as a loss function, and a sigmoid activation function for classification.</p>
<p>For multiclass segmentation, we take our label images, which are 2D images consisting of integers that encode classes, into a stack of binary masks of each class. Each binary mask uses 1 to represent the class, and 0 for everything else. This makes it amenable to simultaneous inference of all classes for all pixels using categorical crossentropy, and the softmax activation function on the classifying head.</p>
<p>We will set up and train 3 different models:</p>
<ol>
<li>for binary (i.e. 2-class) segmentation</li>
<li>for 4-class segmentation</li>
<li>for 4-class segmentation, using a conditional random field (CRF) for post-processing the predicted label images</li>
</ol>
<p>We will use the CRF in the same sense as used by <a href="https://www.mdpi.com/2076-3263/8/7/244">Buscombe and Ritchie, 2018</a>, in that the deep learning model predicts the pixel class, but the CRF evaluates the likelihood of that class based on the image itself, and will return a modified version that is usually more accurate.</p>
<p>Finally, we make another model trained in a different way (with a different loss function), so we have two models for the same task. And we create an ensemble model based on the outputs of the CRF model for each label estimate</p>
<p>The general workflow:</p>
<ol>
<li>Set up a data workflow to feed the model as it trains</li>
</ol>
<ul>
<li>use batches fed optimally to the GPU from TFRecord files</li>
<li>split into train (50% of the data) and validation portions (50%)</li>
</ul>
<ol start="2">
<li><p>For the binary model, compile with dice loss, to better deal with class imbalance, and with an IoU metric for accuracy assessment. For multiclass models, we use categorical crossentropy</p></li>
<li><p>Train it</p></li>
</ol>
<ul>
<li>we are training the model from scratch because we don't have a similar dataset to train two models and transfer the weights, although that would be ideal possibly</li>
<li>use a learning rate scheduler to pass variable learning rates to the model as it trains</li>
<li>use 'early stopping' to base cessation of training on observed plateauing of validation loss</li>
<li>use a checkpoint to monitor validation loss and save the best model weights when the validation loss improves</li>
</ul>
<ol start="4">
<li>Evaluate it</li>
</ol>
<ul>
<li>study the model training history - the loss and accuracy curves of train and validation sets</li>
<li>evaluate the performance of the trained model on the validation set</li>
<li>read some sample images from file, and use the model for prediction</li>
</ul>
<p><em>B) Optional class assignment</em>: an additional dataset will be provided that you can work on using the same models introduced in the class on your own. This <a href="https://scholars.duke.edu/display/pub1419444">dataset</a> consists of aerial UAV colour imagery and labels of oyster reefs in shallow water, made publicly available by Duke University researcher <a href="https://github.com/patrickcgray/oyster_net">Patrick Gray</a>. There are two labels: <code>reef</code> and <code>no reef</code>. Participants will be encouraged to adapt what they learned in the class to their own image segmentation problems using their own data.</p>
<h2><a class="anchor" aria-hidden="true" id="week-4-semi-supervised-image-recognition"></a><a href="#week-4-semi-supervised-image-recognition" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Week 4: Semi-supervised Image Recognition</h2>
<p><em>A) Live session</em>: We'll work through jupyter notebooks containing workflows for more advanced and cutting edge <em>semi-supervised</em> methods for image recognition (whole image classification). We'll revisit the question posed in week 1, <code>How much of the Texas coastline is developed?</code>. See the <a href="doc2#how-much-of-the-texas-coastline-is-developed">data page</a> for more information on the dataset. To answer this question, we will train a deep learning model to classify aerial (oblique) images of the Texas coast, categorized into several natural and non-natural landuse/cover classes. This time, however, we will use a different form of model that quantifies not only what class an image is in, but also a metric reporting close that is to the other classes. Training will utilize <code>soft</code> rather than <code>hard</code> labeling - a concept explained in the class - which is a potential strategy for dealing with small training datasets.</p>
<ol>
<li>Set up a data workflow to feed the model as it trains</li>
</ol>
<ul>
<li>use batches fed optimally to the GPU from TFRecord files</li>
<li>this time we have to read imagery into memory, because the model needs to be fed small batches of anchor and positive examples, but access to all negative examples too</li>
<li>split into train (40% of the data) and validation portions (60%)</li>
</ul>
<ol start="2">
<li>Train a &quot;weakly supervised&quot; image embedding model</li>
</ol>
<ul>
<li>the model is an autoencoder that creates an embedding feature for each image, such that that feature is maximally distant from all other features extracted from other classes</li>
<li>use a constant learning rate (a scheduler doesn't result in better results; this model is more stable with a constant learning rate, which becomes an important tunable hyperparameter)</li>
<li>use 'early stopping' to base cessation of training on observed plateauing of validation loss</li>
<li>use a checkpoint to monitor validation loss and save the best model weights when the validation loss improves</li>
</ul>
<ol start="3">
<li>Evaluate the feature extractor</li>
</ol>
<ul>
<li>study the model training history - the loss and accuracy curves of train and validation sets</li>
</ul>
<ol start="4">
<li>Construct an &quot;unsupervised&quot; classification model</li>
</ol>
<ul>
<li>build a k-nearest-neighbour (kNN) classifier that classifies unseen imagery based on the k nearest neighbours to the current image. Or, more correctly, the nearest neighbour's of the image's embedding vector in the training set of embedding vectors</li>
</ul>
<ol start="5">
<li>Evaluate the classifier</li>
</ol>
<ul>
<li>evaluate the performance of the trained model on the validation set</li>
<li>plot a 'confusion matrix' of correspondences between actual and estimate class labels</li>
<li>read some sample images from file, and use the model for prediction</li>
</ul>
<ol start="4">
<li><p>Fine-tune the model and evaluate it</p></li>
<li><p>Take a look at the same workflow for the NWPU data</p></li>
</ol>
<p><em>B) Optional class assignment</em>: participants will be encouraged to adapt what they learned in the class to their own semi-supervised image recognition problems using their own data.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-next button" href="/MLMONDAYS/docs/doc2"><span>Data</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#useful-links">Useful Links</a><ul class="toc-headings"><li><a href="#reference">Reference</a></li><li><a href="#notebooks">Notebooks</a></li><li><a href="#data">Data</a></li></ul></li><li><a href="#required-pre-course-reading">Required pre-course reading</a></li><li><a href="#suggested-pre-requisites">Suggested pre-requisites</a></li><li><a href="#week-1-supervised-image-recognition">Week 1: Supervised Image Recognition</a></li><li><a href="#week-2-supervised-image-object-detection">Week 2: Supervised Image Object Detection</a></li><li><a href="#week-3-supervised-image-segmentation">Week 3: Supervised Image Segmentation</a></li><li><a href="#week-4-semi-supervised-image-recognition">Week 4: Semi-supervised Image Recognition</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/MLMONDAYS/" class="nav-home"><img src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;" width="66" height="58"/></a><div><h5>Internal links</h5><a href="/MLMONDAYS/docs/en/doc1.html">Docs</a><a href="/MLMONDAYS/docs/en/doc2.html">Data</a><a href="/MLMONDAYS/docs/en/doc3.html">Help</a></div><div><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://www.usgs.gov/centers/cdi" target="_blank" rel="noreferrer noopener">USGS Community for Data Integration (CDI)</a><a href="https://www.usgs.gov/centers/pcmsc/science/remote-sensing-coastal-change?qt-science_center_objects=0#qt-science_center_objects" target="_blank" rel="noreferrer noopener">USGS Remote Sensing Coastal Change Project</a><a href="https://www.danielbuscombe.com" target="_blank" rel="noreferrer noopener">www.danielbuscombe.com</a></div><div><h5>More</h5><a href="/MLMONDAYS/blog">Blog</a><a href="https://github.com/dbuscombe-usgs/DL-CDI2020">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a><div class="social"><a href="https://twitter.com/magic_walnut" class="twitter-follow-button">Follow @magic_walnut</a></div></div></section><a href="https://mardascience.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/MLMONDAYS/img/dash-logo-new.png" alt="Marda Science" width="170" height="45"/></a><section class="copyright">Copyright © 2020 Marda Science, LLC</section></footer></div><script>window.twttr=(function(d,s, id){var js,fjs=d.getElementsByTagName(s)[0],t=window.twttr||{};if(d.getElementById(id))return t;js=d.createElement(s);js.id=id;js.src='https://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js, fjs);t._e = [];t.ready = function(f) {t._e.push(f);};return t;}(document, 'script', 'twitter-wjs'));</script></body></html>