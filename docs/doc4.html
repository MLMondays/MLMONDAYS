<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>ML Mondays API · &quot;ML Mondays&quot;</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Page under construction -- please check back later"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="ML Mondays API · &quot;ML Mondays&quot;"/><meta property="og:type" content="website"/><meta property="og:url" content="https://dbuscombe-usgs.github.io/MLMONDAYS/"/><meta property="og:description" content="Page under construction -- please check back later"/><meta property="og:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_online.svg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://dbuscombe-usgs.github.io/MLMONDAYS/img/undraw_tweetstorm.svg"/><link rel="shortcut icon" href="/MLMONDAYS/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/atom.xml" title="&quot;ML Mondays&quot; Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://dbuscombe-usgs.github.io/MLMONDAYS/blog/feed.xml" title="&quot;ML Mondays&quot; Blog RSS Feed"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script src="/MLMONDAYS/js/scrollSpy.js"></script><link rel="stylesheet" href="/MLMONDAYS/css/main.css"/><script src="/MLMONDAYS/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/MLMONDAYS/"><img class="logo" src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;"/><h2 class="headerTitleWithLogo">&quot;ML Mondays&quot;</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/MLMONDAYS/docs/doc1" target="_self">Docs</a></li><li class="siteNavGroupActive"><a href="/MLMONDAYS/docs/doc2" target="_self">Data</a></li><li class="siteNavGroupActive"><a href="/MLMONDAYS/docs/doc3" target="_self">Models</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/MLMONDAYS/docs/doc4" target="_self">API</a></li><li class=""><a href="/MLMONDAYS/help" target="_self">Help</a></li><li class=""><a href="/MLMONDAYS/blog/" target="_self">Blog</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>API</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">ML Mondays</h3><ul class=""><li class="navListItem"><a class="navItem" href="/MLMONDAYS/docs/doc1">Overview</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Data</h3><ul class=""><li class="navListItem"><a class="navItem" href="/MLMONDAYS/docs/doc2">Data</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">Models and Workflows</h3><ul class=""><li class="navListItem"><a class="navItem" href="/MLMONDAYS/docs/doc3">Models</a></li></ul></div><div class="navGroup"><h3 class="navGroupCategoryTitle">API</h3><ul class=""><li class="navListItem navListItemActive"><a class="navItem" href="/MLMONDAYS/docs/doc4">ML Mondays API</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">ML Mondays API</h1></header><article><div><span><p>Page under construction -- please check back later</p>
<p>This is the class and function reference of the ML Mondays course code</p>
<h2><a class="anchor" aria-hidden="true" id="1_imagerecog"></a><a href="#1_imagerecog" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>1_ImageRecog</h2>
<h3><a class="anchor" aria-hidden="true" id="general-workflow-using-your-own-data"></a><a href="#general-workflow-using-your-own-data" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>General workflow using your own data</h3>
<ol>
<li>Create a TFREcord dataset from your data, organised as follows:</li>
</ol>
<ul>
<li>copy training images into a folder called <code>train</code></li>
<li>copy validation images into a folder called <code>validation</code></li>
<li>ensure the class name is written to each file name. Ideally this is a prefix such that it is trivial to extract the class name from the file name</li>
<li>modify one of the provided workflows (such as <code>tamucc_make_tfrecords.py</code>) for your dataset, to create your train and validation tfrecord shards</li>
</ul>
<ol start="2">
<li>Set up your model</li>
</ol>
<ul>
<li>Decide on whether you want to train a small custom model from scratch, a large model from scratch, or a large model trained using weights transfered from another task</li>
<li>If a small custom model, use <code>make_cat_model</code> with <code>shallow=True</code> for a relatively small model, and <code>shallow=False</code> for a relatively large model</li>
<li>If a large model with transfer learning, decide on which one to utilize (<code>transfer_learning_mobilenet_model</code>, <code>transfer_learning_xception_model</code>, or <code>transfer_learning_model_vgg</code>)</li>
<li>If you wish to train a large model from scratch, decide on which one to utilize (<code>mobilenet_model</code>, or <code>xception_model</code>)</li>
</ul>
<ol start="3">
<li>Set up a data pipeline</li>
</ol>
<ul>
<li>Modify and follow the provided examples to create a <code>get_training_dataset()</code> and <code>get_validation_dataset()</code>. This will likely require you copy and modify <code>get_batched_dataset</code> to your own needs, depending on the format of your labels in filenames, by writing your own <code>read_tfrecord</code> function for your dataset (depending on the model selected)</li>
</ul>
<ol start="4">
<li>Set up a model training pipeline</li>
</ol>
<ul>
<li><code>.compile()</code> your model with an appropriate loss function and metrics</li>
<li>define a <code>LearningRateScheduler</code> function to vary learning rates over training as a function of training epoch</li>
<li>define an <code>EarlyStopping</code> criteria and create a <code>ModelCheckpoint</code> to save trained model weights</li>
<li>if transfer learning using weights not from imagenet, load your initial weights from somewhere else</li>
</ul>
<ol start="5">
<li>Train the model</li>
</ol>
<ul>
<li>Use <code>history = model.fit()</code> to create a record of the training history. Pass the training and validation datasets, and a list of callbacks containing your model checkpoint, learning rate scheduler, and early stopping monitor)</li>
</ul>
<ol start="6">
<li>Evaluate your model</li>
</ol>
<ul>
<li>Plot and study the <code>history</code> time-series of losses and metrics. If unsatisfactory, begin the iterative process of model optimization</li>
<li>Use the <code>loss, accuracy = model.evaluate(get_validation_dataset(), batch_size=BATCH_SIZE, steps=validation_steps)</code> function using the validation dataset and specifying the number of validation steps</li>
<li>Make plots of model outputs, organized in such a way that you can at-a-glance see where the model is failing. Make use of <code>make_sample_plot</code> and <code>p_confmat</code>, as a starting point, to visualize sample imagery with their model predictions, and a confusion matrix of predicted/true class-correspondences</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="model_funcspy"></a><a href="#model_funcspy" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>model_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="model-creation"></a><a href="#model-creation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model creation</h4>
<hr>
<pre><code class="hljs css language-python">transfer_learning_model_vgg(num_classes, input_shape, dropout_rate=<span class="hljs-number">0.5</span>)
</code></pre>
<p>This function creates an implementation of a convolutional deep learning model for estimating
a discrete category based on vgg, trained using transfer learning
(initialized using pretrained imagenet weights)</p>
<ul>
<li>INPUTS:
<ul>
<li>num_classes = number of classes (output nodes on classification layer)</li>
<li>input_shape = size of input layer (i.e. image tensor)</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>dropout_rate = proportion of neurons to randomly set to zero, after the pooling layer</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: keras model instance</li>
</ul>
<hr>
<pre><code class="hljs css language-python">mobilenet_model(num_classes, input_shape, dropout_rate=<span class="hljs-number">0.5</span>)
</code></pre>
<p>This function creates an implementation of a convolutional deep learning model for estimating
a discrete category based on mobilenet, trained from scratch</p>
<ul>
<li>INPUTS:
<ul>
<li>num_classes = number of classes (output nodes on classification layer)</li>
<li>input_shape = size of input layer (i.e. image tensor)</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>dropout_rate = proportion of neurons to randomly set to zero, after the pooling layer</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: keras model instance</li>
</ul>
<hr>
<pre><code class="hljs css language-python">transfer_learning_mobilenet_model(num_classes, input_shape, dropout_rate=<span class="hljs-number">0.5</span>)
</code></pre>
<p>This function creates an implementation of a convolutional deep learning model for estimating
a discrete category based on mobilenet v2, trained using transfer learning
(initialized using pretrained imagenet weights)</p>
<ul>
<li>INPUTS:
<ul>
<li>num_classes = number of classes (output nodes on classification layer)</li>
<li>input_shape = size of input layer (i.e. image tensor)</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>dropout_rate = proportion of neurons to randomly set to zero, after the pooling layer</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: keras model instance</li>
</ul>
<hr>
<pre><code class="hljs css language-python">transfer_learning_xception_model(num_classes, input_shape, dropout_rate=<span class="hljs-number">0.25</span>)
</code></pre>
<p>This function creates an implementation of a convolutional deep learning model for estimating
a discrete category based on xception, trained using transfer learning
(initialized using pretrained imagenet weights)</p>
<ul>
<li>INPUTS:
<ul>
<li>num_classes = number of classes (output nodes on classification layer)</li>
<li>input_shape = size of input layer (i.e. image tensor)</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>dropout_rate = proportion of neurons to randomly set to zero, after the pooling layer</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: keras model instance</li>
</ul>
<hr>
<pre><code class="hljs css language-python">xception_model(num_classes, input_shape, dropout_rate=<span class="hljs-number">0.25</span>)
</code></pre>
<p>This function creates an implementation of a convolutional deep learning model for estimating
a discrete category based on xception, trained from scratch</p>
<ul>
<li>INPUTS:
<ul>
<li>num_classes = number of classes (output nodes on classification layer)</li>
<li>input_shape = size of input layer (i.e. image tensor)</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>dropout_rate = proportion of neurons to randomly set to zero, after the pooling layer</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: keras model instance</li>
</ul>
<hr>
<pre><code class="hljs css language-python">conv_block(inp, filters=<span class="hljs-number">32</span>, bn=<span class="hljs-literal">True</span>, pool=<span class="hljs-literal">True</span>)
</code></pre>
<p>This function generates a convolutional block</p>
<ul>
<li>INPUTS:
<ul>
<li>inp = input layer</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>filters = number of convolutional filters to use</li>
<li>bn=False, use batch normalization in each convolutional layer</li>
<li>pool=True, use pooling in each convolutional layer</li>
<li>shallow=True, if False, a larger model with more convolution layers is used</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: keras model layer object</li>
</ul>
<hr>
<pre><code class="hljs css language-python">make_cat_model(num_classes, dropout, denseunits, base_filters, bn=<span class="hljs-literal">False</span>, pool=<span class="hljs-literal">True</span>, shallow=<span class="hljs-literal">True</span>)
</code></pre>
<p>This function creates an implementation of a convolutional deep learning model for estimating
a discrete category</p>
<ul>
<li>INPUTS:
<ul>
<li>num_classes = number of classes (output nodes on classification layer)</li>
<li>dropout = proportion of neurons to randomly set to zero, after the pooling layer</li>
<li>denseunits = number of neurons in the classifying layer</li>
<li>base_filters = number of convolutional filters to use in the first layer</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>bn=False, use batch normalization in each convolutional layer</li>
<li>pool=True, use pooling in each convolutional layer</li>
<li>shallow=True, if False, a larger model with more convolution layers is used</li>
</ul></li>
<li>GLOBAL INPUTS: TARGET_SIZE</li>
<li>OUTPUTS: keras model instance</li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="model-training"></a><a href="#model-training" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training</h4>
<hr>
<pre><code class="hljs css language-python">lrfn(epoch)
</code></pre>
<p>This function creates a custom piecewise linear-exponential learning rate function for a custom learning rate scheduler. It is linear to a max, then exponentially decays</p>
<ul>
<li>INPUTS: current epoch number</li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay</li>
<li>OUTPUTS:  the function lr with all arguments passed</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="tfrecords_funcspy"></a><a href="#tfrecords_funcspy" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>tfrecords_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="tf-dataset-creation"></a><a href="#tf-dataset-creation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TF-dataset creation</h4>
<hr>
<pre><code class="hljs css language-python">get_batched_dataset(filenames)
</code></pre>
<p>This function defines a workflow for the model to read data from
tfrecord files by defining the degree of parallelism, batch size, pre-fetching, etc
and also formats the imagery properly for model training
(assumes mobilenet by using read_tfrecord_mv2)</p>
<ul>
<li>INPUTS:
<ul>
<li>filenames [list]</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: BATCH_SIZE, AUTO</li>
<li>OUTPUTS: tf.data.Dataset object</li>
</ul>
<hr>
<pre><code class="hljs css language-python">get_eval_dataset(filenames)
</code></pre>
<p>This function defines a workflow for the model to read data from
tfrecord files by defining the degree of parallelism, batch size, pre-fetching, etc
and also formats the imagery properly for model training
(assumes mobilenet by using read_tfrecord_mv2). This evaluation version does not .repeat() because it is not being called repeatedly by a model</p>
<ul>
<li>INPUTS:
<ul>
<li>filenames [list]</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: BATCH_SIZE, AUTO</li>
<li>OUTPUTS: tf.data.Dataset object</li>
</ul>
<hr>
<pre><code class="hljs css language-python">resize_and_crop_image(image, label)
</code></pre>
<p>This function crops to square and resizes an image. The label passes through unmodified</p>
<ul>
<li>INPUTS:
<ul>
<li>image [tensor array]</li>
<li>label [int]</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: TARGET_SIZE</li>
<li>OUTPUTS:
<ul>
<li>image [tensor array]</li>
<li>label [int]</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">recompress_image(image, label)
</code></pre>
<p>This function takes an image encoded as a byte string and recodes as an 8-bit jpeg. Label passes through unmodified</p>
<ul>
<li>INPUTS:
<ul>
<li>image [tensor array]</li>
<li>label [int]</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS:
<ul>
<li>image [tensor array]</li>
<li>label [int]</li>
</ul></li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="tfrecord-reading"></a><a href="#tfrecord-reading" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TFRecord reading</h4>
<hr>
<pre><code class="hljs css language-python">file2tensor(f, model=<span class="hljs-string">'mobilenet'</span>)
</code></pre>
<p>This function reads a jpeg image from file into a cropped and resized tensor,
for use in prediction with a trained mobilenet or vgg model
(the imagery is standardized depending on target model framework)</p>
<ul>
<li>INPUTS:
<ul>
<li>f [string] file name of jpeg</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>model = {'mobilenet' | 'vgg'}</li>
</ul></li>
<li>OUTPUTS:
<ul>
<li>image [tensor array]: unstandardized image</li>
<li>im [tensor array]: standardized image</li>
</ul></li>
<li>GLOBAL INPUTS: TARGET_SIZE</li>
</ul>
<hr>
<pre><code class="hljs css language-python">read_classes_from_json(json_file)
</code></pre>
<p>This function reads the contents of a json file enumerating classes</p>
<ul>
<li>INPUTS:
<ul>
<li>json_file [string]: full path to the json file</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS:
<ul>
<li>CLASSES [list]: list of classesd as byte strings</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">read_tfrecord_vgg(example)
</code></pre>
<p>This function reads an example record from a tfrecord file
and parses into label and image ready for vgg model training</p>
<ul>
<li>INPUTS:
<ul>
<li>example: an tfrecord 'example' object, containing an image and label</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: TARGET_SIZE</li>
<li>OUTPUTS:
<ul>
<li>image [tensor]: resized and pre-processed for vgg</li>
<li>class_label [tensor] 32-bit integer</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">read_tfrecord_mv2(example)
</code></pre>
<p>This function reads an example record from a tfrecord file
and parses into label and image ready for mobilenet model training</p>
<ul>
<li>INPUTS:
<ul>
<li>example: an tfrecord 'example' object, containing an image and label</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: TARGET_SIZE</li>
<li>OUTPUTS:
<ul>
<li>image [tensor]: resized and pre-processed for mobilenetv2</li>
<li>class_label [tensor] 32-bit integer</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">read_tfrecord(example)
</code></pre>
<p>This function reads an example from a TFrecord file into a single image and label</p>
<ul>
<li>INPUTS:
<ul>
<li>TFRecord example object</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: TARGET_SIZE</li>
<li>OUTPUTS:
<ul>
<li>image [tensor array]</li>
<li>class_label [tensor int]</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">read_image_and_label(img_path)
</code></pre>
<p>This function reads a jpeg image from a provided filepath and extracts the label from the filename (assuming the class name is before &quot;IMG&quot; in the filename)</p>
<ul>
<li>INPUTS:
<ul>
<li>img_path [string]: filepath to a jpeg image</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS:
<ul>
<li>image [tensor array]</li>
<li>class_label [tensor int]</li>
</ul></li>
</ul>
<h4><a class="anchor" aria-hidden="true" id="tfrecord-creation"></a><a href="#tfrecord-creation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TFRecord creation</h4>
<hr>
<pre><code class="hljs css language-python">get_dataset_for_tfrecords(recoded_dir, shared_size)
</code></pre>
<p>This function reads a list of TFREcord shard files, decode the images and label resize and crop the image to TARGET_SIZE, and create batches</p>
<ul>
<li>INPUTS:
<ul>
<li>recoded_dir</li>
<li>shared_size</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: TARGET_SIZE</li>
<li>OUTPUTS:
<ul>
<li>tf.data.Dataset object</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">write_records(tamucc_dataset, tfrecord_dir, CLASSES)
</code></pre>
<p>This function writes a tf.data.Dataset object to TFRecord shards</p>
<ul>
<li>INPUTS:
<ul>
<li>tamucc_dataset [tf.data.Dataset]</li>
<li>tfrecord_dir [string] : path to directory where files will be written</li>
<li>CLASSES [list] of class string names</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: None (files written to disk)</li>
</ul>
<hr>
<pre><code class="hljs css language-python">to_tfrecord(img_bytes, label, CLASSES)
</code></pre>
<p>This function creates a TFRecord example from an image byte string and a label feature</p>
<ul>
<li>INPUTS:
<ul>
<li>img_bytes: an image bytestring</li>
<li>label: label string of image</li>
<li>CLASSES: list of string classes in the entire dataset</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: tf.train.Feature example</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="plot_funcspy"></a><a href="#plot_funcspy" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>plot_funcs.py</h3>
<hr>
<pre><code class="hljs css language-python">plot_history(history, train_hist_fig)
</code></pre>
<p>This function plots the training history of a model</p>
<ul>
<li>INPUTS:
<ul>
<li>history [dict]: the output dictionary of the model.fit() process, i.e. history = model.fit(...)</li>
<li>train_hist_fig [string]: the filename where the plot will be printed</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: None (figure printed to file)</li>
</ul>
<hr>
<pre><code class="hljs css language-python">get_label_pairs(val_ds, model)
</code></pre>
<p>This function gets label observations and model estimates</p>
<ul>
<li>INPUTS:
<ul>
<li>val_ds: a batched data set object</li>
<li>model: trained and compiled keras model instance</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS:
<ul>
<li>labs [ndarray]: 1d vector of numeric labels</li>
<li>preds [ndarray]: 1d vector of correspodning model predicted numeric labels</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">p_confmat(labs, preds, cm_filename, CLASSES, thres = <span class="hljs-number">0.1</span>)
</code></pre>
<p>This function computes a confusion matrix (matrix of correspondences between true and estimated classes)
using the sklearn function of the same name. Then normalizes by column totals, and makes a heatmap plot of the matrix
saving out to the provided filename, cm_filename</p>
<ul>
<li>INPUTS:
<ul>
<li>labs [ndarray]: 1d vector of labels</li>
<li>preds [ndarray]: 1d vector of model predicted labels</li>
<li>cm_filename [string]: filename to write the figure to</li>
<li>CLASSES [list] of strings: class names</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>thres [float]: threshold controlling what values are displayed</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: None (figure printed to file)</li>
</ul>
<hr>
<pre><code class="hljs css language-python">make_sample_plot(model, sample_filenames, test_samples_fig, CLASSES))
</code></pre>
<p>This function computes a confusion matrix (matrix of correspondences between true and estimated classes)
using the sklearn function of the same name. Then normalizes by column totals, and makes a heatmap plot of the matrix
saving out to the provided filename, cm_filename</p>
<ul>
<li>INPUTS:
<ul>
<li>model: trained and compiled keras model</li>
<li>sample_filenames: [list] of strings</li>
<li>test_samples_fig [string]: filename to print figure to</li>
<li>CLASSES [list] os trings: class names</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: None (matplotlib figure, printed to file)</li>
</ul>
<hr>
<pre><code class="hljs css language-python">compute_hist(images)
</code></pre>
<p>Compute the per channel histogram for a batch
of images</p>
<ul>
<li>INPUTS:
<ul>
<li>images [ndarray]: batch of shape (N x W x H x 3)</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS:
<ul>
<li>hist_r [dict]: histogram frequencies {'hist'} and bins {'bins'} for red channel</li>
<li>hist_g [dict]: histogram frequencies {'hist'} and bins {'bins'} for green channel</li>
<li>hist_b [dict]: histogram frequencies {'hist'} and bins {'bins'} for blue channel</li>
</ul></li>
</ul>
<hr>
<pre><code class="hljs css language-python">plot_distribution(images, labels, class_id, CLASSES)
</code></pre>
<p>Compute the per channel histogram for a batch of images</p>
<ul>
<li>INPUTS:
<ul>
<li>images [ndarray]: batch of shape (N x W x H x 3)</li>
<li>labels [ndarray]: batch of shape (N x 1)</li>
<li>class_id [int]: class integer to plot</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: matplotlib figure</li>
</ul>
<hr>
<pre><code class="hljs css language-python">plot_one_class(inp_batch, sample_idx, label, batch_size, CLASSES, rows=<span class="hljs-number">8</span>, cols=<span class="hljs-number">8</span>, size=(<span class="hljs-number">20</span>,<span class="hljs-number">15</span>))
</code></pre>
<p>Plot &quot;batch_size&quot; images that belong to the class &quot;label&quot;</p>
<ul>
<li>INPUTS:
<ul>
<li>inp_batch [ndarray]: batch of N images</li>
<li>sample_idx [list]: indices of the N images</li>
<li>label [string]: class string</li>
<li>batch_size [int]: number of images to plot</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>rows=8 [int]: number of rows</li>
<li>cols=8 [int]: number of columns</li>
<li>size=(20,15) [tuple]: size of matplotlib figure</li>
</ul></li>
<li>GLOBAL INPUTS: None (matplotlib figure, printed to file)</li>
</ul>
<hr>
<pre><code class="hljs css language-python">compute_mean_image(images, opt=<span class="hljs-string">"mean"</span>)
</code></pre>
<p>Compute and return mean image given a batch of images</p>
<ul>
<li>INPUTS:
<ul>
<li>images [ndarray]: batch of shape (N x W x H x 3)</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>opt=&quot;mean&quot; or &quot;median&quot;</li>
</ul></li>
<li>GLOBAL INPUTS:</li>
<li>OUTPUTS: 2d mean image [ndarray]</li>
</ul>
<hr>
<pre><code class="hljs css language-python">plot_mean_images(images, labels, CLASSES, rows=<span class="hljs-number">3</span>, cols = <span class="hljs-number">2</span>)
</code></pre>
<p>Plot the mean image of a set of images</p>
<ul>
<li>INPUTS:
<ul>
<li>images [ndarray]: batch of shape (N x W x H x 3)</li>
<li>labels [ndarray]: batch of shape (N x 1)</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>rows [int]: number of rows</li>
<li>cols [int]: number of columns</li>
</ul></li>
<li>GLOBAL INPUTS: CLASSES</li>
<li>OUTPUTS: matplotlib figure</li>
</ul>
<hr>
<pre><code class="hljs css language-python">plot_tsne(tsne_result, label_ids, CLASSES)
</code></pre>
<p>Plot TSNE loadings and colour code by class. <a href="https://www.kaggle.com/gaborvecsei/plants-t-sne">Source</a></p>
<ul>
<li>INPUTS:
<ul>
<li>tsne_result [ndarray]: N x 2 data of loadings on two axes</li>
<li>label_ids [int]: N class labels</li>
</ul></li>
<li>OPTIONAL INPUTS: None</li>
<li>GLOBAL INPUTS: CLASSES</li>
<li>OUTPUTS: matplotlib figure, matplotlib figure axes object</li>
</ul>
<hr>
<pre><code class="hljs css language-python">visualize_scatter_with_images(X_2d_data, labels, images, figsize=(<span class="hljs-number">15</span>,<span class="hljs-number">15</span>), image_zoom=<span class="hljs-number">1</span>,xlim = (<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>), ylim=(<span class="hljs-number">-3</span>,<span class="hljs-number">3</span>))
</code></pre>
<p>Plot TSNE loadings and colour code by class. <a href="https://www.kaggle.com/gaborvecsei/plants-t-sne">Source</a></p>
<ul>
<li>INPUTS:
<ul>
<li>X_2d_data [ndarray]: N x 2 data of loadings on two axes</li>
<li>images [ndarray] : N batch of images to plot</li>
</ul></li>
<li>OPTIONAL INPUTS:
<ul>
<li>figsize=(15,15)</li>
<li>image_zoom=1 [float]: control the scaling of the imagery (make smaller for smaller thumbnails)</li>
<li>xlim = (-3,3) [tuple]: set x axes limits</li>
<li>ylim = (-3,3) [tuple]: set y axes limits]</li>
</ul></li>
<li>GLOBAL INPUTS: None</li>
<li>OUTPUTS: matplotlib figure</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="2_objrecog"></a><a href="#2_objrecog" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>2_ObjRecog</h2>
<h3><a class="anchor" aria-hidden="true" id="general-workflow-using-your-own-data-1"></a><a href="#general-workflow-using-your-own-data-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>General workflow using your own data</h3>
<h3><a class="anchor" aria-hidden="true" id="model_funcspy-1"></a><a href="#model_funcspy-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>model_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="model-creation-1"></a><a href="#model-creation-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model creation</h4>
<h4><a class="anchor" aria-hidden="true" id="model-training-1"></a><a href="#model-training-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training</h4>
<h3><a class="anchor" aria-hidden="true" id="data_funcspy"></a><a href="#data_funcspy" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>data_funcs.py</h3>
<h3><a class="anchor" aria-hidden="true" id="tfrecords_funcspy-1"></a><a href="#tfrecords_funcspy-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>tfrecords_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="tf-dataset-creation-1"></a><a href="#tf-dataset-creation-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TF-dataset creation</h4>
<h4><a class="anchor" aria-hidden="true" id="tfrecord-reading-1"></a><a href="#tfrecord-reading-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TFRecord reading</h4>
<h3><a class="anchor" aria-hidden="true" id="plot_funcspy-1"></a><a href="#plot_funcspy-1" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>plot_funcs.py</h3>
<h2><a class="anchor" aria-hidden="true" id="3_imageseg"></a><a href="#3_imageseg" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>3_ImageSeg</h2>
<h3><a class="anchor" aria-hidden="true" id="general-workflow-using-your-own-data-2"></a><a href="#general-workflow-using-your-own-data-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>General workflow using your own data</h3>
<h3><a class="anchor" aria-hidden="true" id="model_funcspy-2"></a><a href="#model_funcspy-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>model_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="model-creation-2"></a><a href="#model-creation-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model creation</h4>
<h4><a class="anchor" aria-hidden="true" id="model-training-2"></a><a href="#model-training-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training</h4>
<h3><a class="anchor" aria-hidden="true" id="tfrecords_funcspy-2"></a><a href="#tfrecords_funcspy-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>tfrecords_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="tf-dataset-creation-2"></a><a href="#tf-dataset-creation-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TF-dataset creation</h4>
<h4><a class="anchor" aria-hidden="true" id="tfrecord-reading-2"></a><a href="#tfrecord-reading-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TFRecord reading</h4>
<h3><a class="anchor" aria-hidden="true" id="plot_funcspy-2"></a><a href="#plot_funcspy-2" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>plot_funcs.py</h3>
<h2><a class="anchor" aria-hidden="true" id="4_unsupimagerecog"></a><a href="#4_unsupimagerecog" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>4_UnsupImageRecog</h2>
<h3><a class="anchor" aria-hidden="true" id="general-workflow-using-your-own-data-3"></a><a href="#general-workflow-using-your-own-data-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>General workflow using your own data</h3>
<h3><a class="anchor" aria-hidden="true" id="model_funcspy-3"></a><a href="#model_funcspy-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>model_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="model-creation-3"></a><a href="#model-creation-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model creation</h4>
<h4><a class="anchor" aria-hidden="true" id="model-training-3"></a><a href="#model-training-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model training</h4>
<h3><a class="anchor" aria-hidden="true" id="tfrecords_funcspy-3"></a><a href="#tfrecords_funcspy-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>tfrecords_funcs.py</h3>
<h4><a class="anchor" aria-hidden="true" id="tf-dataset-creation-3"></a><a href="#tf-dataset-creation-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TF-dataset creation</h4>
<h4><a class="anchor" aria-hidden="true" id="tfrecord-reading-3"></a><a href="#tfrecord-reading-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>TFRecord reading</h4>
<h3><a class="anchor" aria-hidden="true" id="plot_funcspy-3"></a><a href="#plot_funcspy-3" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>plot_funcs.py</h3>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/MLMONDAYS/docs/doc3"><span class="arrow-prev">← </span><span>Models</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#1_imagerecog">1_ImageRecog</a><ul class="toc-headings"><li><a href="#general-workflow-using-your-own-data">General workflow using your own data</a></li><li><a href="#model_funcspy">model_funcs.py</a></li><li><a href="#tfrecords_funcspy">tfrecords_funcs.py</a></li><li><a href="#plot_funcspy">plot_funcs.py</a></li></ul></li><li><a href="#2_objrecog">2_ObjRecog</a><ul class="toc-headings"><li><a href="#general-workflow-using-your-own-data-1">General workflow using your own data</a></li><li><a href="#model_funcspy-1">model_funcs.py</a></li><li><a href="#data_funcspy">data_funcs.py</a></li><li><a href="#tfrecords_funcspy-1">tfrecords_funcs.py</a></li><li><a href="#plot_funcspy-1">plot_funcs.py</a></li></ul></li><li><a href="#3_imageseg">3_ImageSeg</a><ul class="toc-headings"><li><a href="#general-workflow-using-your-own-data-2">General workflow using your own data</a></li><li><a href="#model_funcspy-2">model_funcs.py</a></li><li><a href="#tfrecords_funcspy-2">tfrecords_funcs.py</a></li><li><a href="#plot_funcspy-2">plot_funcs.py</a></li></ul></li><li><a href="#4_unsupimagerecog">4_UnsupImageRecog</a><ul class="toc-headings"><li><a href="#general-workflow-using-your-own-data-3">General workflow using your own data</a></li><li><a href="#model_funcspy-3">model_funcs.py</a></li><li><a href="#tfrecords_funcspy-3">tfrecords_funcs.py</a></li><li><a href="#plot_funcspy-3">plot_funcs.py</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/MLMONDAYS/" class="nav-home"><img src="/MLMONDAYS/img/favicon.ico" alt="&quot;ML Mondays&quot;" width="66" height="58"/></a><div><h5>Internal links</h5><a href="/MLMONDAYS/docs/en/doc1.html">Docs</a><a href="/MLMONDAYS/docs/en/doc2.html">Data</a><a href="/MLMONDAYS/docs/en/doc3.html">Help</a></div><div><h5>Community</h5><a href="https://stackoverflow.com/questions/tagged/" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://www.usgs.gov/centers/cdi" target="_blank" rel="noreferrer noopener">USGS Community for Data Integration (CDI)</a><a href="https://www.usgs.gov/centers/pcmsc/science/remote-sensing-coastal-change?qt-science_center_objects=0#qt-science_center_objects" target="_blank" rel="noreferrer noopener">USGS Remote Sensing Coastal Change Project</a><a href="https://www.danielbuscombe.com" target="_blank" rel="noreferrer noopener">www.danielbuscombe.com</a></div><div><h5>More</h5><a href="/MLMONDAYS/blog">Blog</a><a href="https://github.com/dbuscombe-usgs/DL-CDI2020">GitHub</a><a class="github-button" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a><div class="social"><a href="https://twitter.com/magic_walnut" class="twitter-follow-button">Follow @magic_walnut</a></div></div></section><a href="https://mardascience.com/" target="_blank" rel="noreferrer noopener" class="fbOpenSource"><img src="/MLMONDAYS/img/dash-logo-new.png" alt="Marda Science" width="170" height="45"/></a><section class="copyright">Copyright © 2020 Marda Science, LLC</section></footer></div><script>window.twttr=(function(d,s, id){var js,fjs=d.getElementsByTagName(s)[0],t=window.twttr||{};if(d.getElementById(id))return t;js=d.createElement(s);js.id=id;js.src='https://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js, fjs);t._e = [];t.ready = function(f) {t._e.push(f);};return t;}(document, 'script', 'twitter-wjs'));</script></body></html>