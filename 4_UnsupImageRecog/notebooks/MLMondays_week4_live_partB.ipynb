{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getcwd() is not '/home/jovyan/MLMONDAYS/4_UnsupImageRecog':\n",
    "    os.chdir('/home/jovyan/MLMONDAYS/4_UnsupImageRecog')\n",
    "    print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.getcwd()+'/data/tamucc/subset_12class/tamucc_subset_12classes.json'):\n",
    "    !python download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with a high validation split. If model poor on train data, can decrease\n",
    "VALIDATION_SPLIT = 0.6\n",
    "\n",
    "#start small - can increase later with larger hardware\n",
    "TARGET_SIZE= 400\n",
    "\n",
    "if TARGET_SIZE==400:\n",
    "   BATCH_SIZE = 6\n",
    "elif TARGET_SIZE==224:\n",
    "   BATCH_SIZE = 16\n",
    "\n",
    "num_classes = 12 \n",
    "\n",
    "ims_per_shard = 200\n",
    "\n",
    "patience = 10\n",
    "\n",
    "# the number of embedding dims\n",
    "num_embed_dim = 16 \n",
    "\n",
    "max_epochs = 100 #400 - this is more like the number you'll actually need (or more), but time in class is limited\n",
    "lr = 1e-4\n",
    "\n",
    "n_neighbors = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "### DATA FUNCTIONS\n",
    "###############################################################\n",
    "#-----------------------------------\n",
    "def get_training_dataset():\n",
    "    \"\"\"\n",
    "    This function will return a batched dataset for model training\n",
    "    INPUTS: None\n",
    "    OPTIONAL INPUTS: None\n",
    "    GLOBAL INPUTS: training_filenames\n",
    "    OUTPUTS: batched data set object\n",
    "    \"\"\"\n",
    "    return get_batched_dataset(training_filenames)\n",
    "\n",
    "def get_validation_dataset():\n",
    "    \"\"\"\n",
    "    This function will return a batched dataset for model training\n",
    "    INPUTS: None\n",
    "    OPTIONAL INPUTS: None\n",
    "    GLOBAL INPUTS: validation_filenames\n",
    "    OUTPUTS: batched data set object\n",
    "    \"\"\"\n",
    "    return get_batched_dataset(validation_filenames)\n",
    "\n",
    "def get_validation_eval_dataset():\n",
    "    \"\"\"\n",
    "    This function will return a batched dataset for model training\n",
    "    INPUTS: None\n",
    "    OPTIONAL INPUTS: None\n",
    "    GLOBAL INPUTS: validation_filenames\n",
    "    OUTPUTS: batched data set object\n",
    "    \"\"\"\n",
    "    return get_eval_dataset(validation_filenames)\n",
    "\n",
    "#-----------------------------------\n",
    "def get_batched_dataset(filenames):\n",
    "    \"\"\"\n",
    "    \"get_batched_dataset\"\n",
    "    This function defines a workflow for the model to read data from\n",
    "    tfrecord files by defining the degree of parallelism, batch size, pre-fetching, etc\n",
    "    and also formats the imagery properly for model training\n",
    "    (assumes mobilenet by using read_tfrecord_mv2)\n",
    "    INPUTS:\n",
    "        * filenames [list]\n",
    "    OPTIONAL INPUTS: None\n",
    "    GLOBAL INPUTS: BATCH_SIZE, AUTO\n",
    "    OUTPUTS: tf.data.Dataset object\n",
    "    \"\"\"\n",
    "    option_no_order = tf.data.Options()\n",
    "    option_no_order.experimental_deterministic = True\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(filenames)\n",
    "    dataset = dataset.with_options(option_no_order)\n",
    "    dataset = dataset.interleave(tf.data.TFRecordDataset, cycle_length=16, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
    "\n",
    "    dataset = dataset.cache() # This dataset fits in RAM\n",
    "    #dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # drop_remainder will be needed on TPU\n",
    "    dataset = dataset.prefetch(AUTO) #\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "def get_train_stuff(num_batches):\n",
    "    \"\"\"\n",
    "    \"get_train_stuff\"\n",
    "    This function returns all the images and labels from a tf.data.Dataset\n",
    "    INPUTS:\n",
    "        * num_batches [int]\n",
    "    OPTIONAL INPUTS: None\n",
    "    GLOBAL INPUTS: None\n",
    "    OUTPUTS:\n",
    "        * X_train [list] of ndarray images\n",
    "        * y_train [list] of integer labels\n",
    "        * class_idx_to_train_idxs [dict] of indices into each class\n",
    "    \"\"\"\n",
    "    X_train = []\n",
    "    ytrain = []\n",
    "    train_ds = get_training_dataset()\n",
    "\n",
    "    counter = 0\n",
    "    for imgs,lbls in train_ds.take(num_batches):\n",
    "      ytrain.append(lbls.numpy())\n",
    "      for im in imgs:\n",
    "        X_train.append(im.numpy().astype(\"float32\"))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    ytrain = np.hstack(ytrain)\n",
    "\n",
    "    # get X_train, y_train arrays\n",
    "    X_train = X_train.astype(\"float32\")\n",
    "    ytrain = np.squeeze(ytrain)\n",
    "\n",
    "    # code repurposed from https://keras.io/examples/vision/metric_learning/\n",
    "    class_idx_to_train_idxs = defaultdict(list)\n",
    "    for y_train_idx, y in enumerate(ytrain):\n",
    "        class_idx_to_train_idxs[y].append(y_train_idx)\n",
    "\n",
    "    return X_train, ytrain, class_idx_to_train_idxs\n",
    "\n",
    "#-----------------------------------\n",
    "def get_test_stuff(num_batches):\n",
    "    \"\"\"\n",
    "    \"get_test_stuff\"\n",
    "    This function returns all the images and labels from a tf.data.Dataset\n",
    "    INPUTS:\n",
    "        * num_batches [int]\n",
    "    OPTIONAL INPUTS: None\n",
    "    GLOBAL INPUTS: None\n",
    "    OUTPUTS:\n",
    "        * X_test [list] of ndarray images\n",
    "        * y_test [list] of integer labels\n",
    "        * class_idx_to_test_idxs [dict] of indices into each class\n",
    "    \"\"\"\n",
    "    X_test = []\n",
    "    ytest = []\n",
    "    test_ds = get_validation_dataset()\n",
    "\n",
    "    counter = 0\n",
    "    for imgs,lbls in test_ds.take(num_batches):\n",
    "      ytest.append(lbls.numpy())\n",
    "      for im in imgs:\n",
    "        X_test.append(im.numpy())\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    ytest = np.hstack(ytest)\n",
    "\n",
    "    # get X_test, y_test arrays\n",
    "    X_test = X_test.astype(\"float32\")\n",
    "    ytest = np.squeeze(ytest)\n",
    "\n",
    "    # code repurposed from https://keras.io/examples/vision/metric_learning/\n",
    "    class_idx_to_test_idxs = defaultdict(list)\n",
    "    for y_test_idx, y in enumerate(ytest):\n",
    "        class_idx_to_test_idxs[y].append(y_test_idx)\n",
    "\n",
    "    return X_test, ytest, class_idx_to_test_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorPositivePairs(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    # code modified from https://keras.io/examples/vision/metric_learning/\n",
    "    \"AnchorPositivePairs\"\n",
    "    This Class selects an anchor and positive example images from each label class\n",
    "    INPUTS: None\n",
    "    OPTIONAL INPUTS: None\n",
    "    GLOBAL INPUTS: None\n",
    "    OUTPUTS:\n",
    "        * x [ndarray]: a pair of example images of each class, (2, num_classes, TARGET_SIZE, TARGET_SIZE, 3)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_batchs):\n",
    "        self.num_batchs = num_batchs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batchs\n",
    "\n",
    "    def __getitem__(self, _idx):\n",
    "        x = np.empty((2, num_classes, TARGET_SIZE, TARGET_SIZE, 3), dtype=np.float32)\n",
    "        for class_idx in range(num_classes):\n",
    "            examples_for_class = class_idx_to_train_idxs[class_idx]\n",
    "            anchor_idx = np.random.choice(examples_for_class)\n",
    "            positive_idx = np.random.choice(examples_for_class)\n",
    "            while positive_idx == anchor_idx:\n",
    "                positive_idx = np.random.choice(examples_for_class)\n",
    "            x[0, class_idx] = X_train[anchor_idx]\n",
    "            x[1, class_idx] = X_train[positive_idx]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model inputs\n",
    "json_file = os.getcwd()+os.sep+'data/tamucc/subset_12class/tamucc_subset_12classes.json'\n",
    "\n",
    "data_path= os.getcwd()+os.sep+\"data/tamucc/subset_12class/400\"\n",
    "test_samples_fig = os.getcwd()+os.sep+'results/tamucc_sample_12class_model1_est36samples.png'\n",
    "\n",
    "cm_filename = os.getcwd()+os.sep+'results/tamucc_sample_12class_model1_cm_val.png'\n",
    "\n",
    "sample_data_path= os.getcwd()+os.sep+\"data/tamucc/subset_12class/sample\"\n",
    "\n",
    "filepath = os.getcwd()+os.sep+'results/tamucc_subset_12class_best_weights_model1.h5'\n",
    "\n",
    "hist_fig = os.getcwd()+os.sep+'results/tamucc_subset_12class_custom_model1.png'\n",
    "\n",
    "cm_fig = os.getcwd()+os.sep+'results/tamucc_subset_12class_cm_test.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(tf.io.gfile.glob(data_path+os.sep+'*.tfrec'))\n",
    "\n",
    "nb_images = ims_per_shard * len(filenames)\n",
    "print(nb_images)\n",
    "\n",
    "split = int(len(filenames) * VALIDATION_SPLIT)\n",
    "\n",
    "training_filenames = filenames[split:]\n",
    "validation_filenames = filenames[:split]\n",
    "\n",
    "validation_steps = int(nb_images // len(filenames) * len(validation_filenames)) // BATCH_SIZE\n",
    "steps_per_epoch = int(nb_images // len(filenames) * len(training_filenames)) // BATCH_SIZE\n",
    "\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)\n",
    "\n",
    "CLASSES = read_classes_from_json(json_file)\n",
    "\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_training_dataset()\n",
    "\n",
    "val_ds = get_validation_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better? At this point, it would be common to try to ***fine-tune*** the model. This usually involves training for longer at a lower learning rate, in the hope that it will find further optimal solutions in the loss landscape\n",
    "\n",
    "We could also freeze model layers at this point, so lower layers can no longer learn but higher layers are still free to. However, in the spirit of experimentation (i.e. only varying one variable at a time), we'll leave the model as is, increase the `patience` to 20, lower the learning rate to `5e-5` (i.e. half a magnitude step down in learning rate), and train again with a starting point of the best weights from the previous model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ytest, class_idx_to_test_idxs\n",
    "\n",
    "X_train, ytrain, class_idx_to_train_idxs = get_train_stuff(num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = os.getcwd()+os.sep+'data/tamucc/subset_12class/tamucc_subset_12classes.json'\n",
    "\n",
    "data_path= os.getcwd()+os.sep+\"data/tamucc/subset_12class/400\"\n",
    "test_samples_fig = os.getcwd()+os.sep+'results/tamucc_sample_12class_model2_est36samples.png'\n",
    "\n",
    "cm_filename = os.getcwd()+os.sep+'results/tamucc_sample_12class_model2_cm_val.png'\n",
    "\n",
    "sample_data_path= os.getcwd()+os.sep+\"data/tamucc/subset_12class/sample\"\n",
    "\n",
    "filepath = os.getcwd()+os.sep+'results/tamucc_subset_12class_best_weights_model2.h5'\n",
    "\n",
    "hist_fig = os.getcwd()+os.sep+'results/tamucc_subset_12class_custom_model2.png'\n",
    "\n",
    "cm_fig = os.getcwd()+os.sep+'results/tamucc_subset_12class_cm_test_model2.png'\n",
    "\n",
    "initial_filepath = os.getcwd()+os.sep+'results/weights_copy/tamucc_subset_12class_best_weights_model1.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = get_large_embedding_model(TARGET_SIZE, num_classes, num_embed_dim)\n",
    "\n",
    "# use a smaller learning rate, because we are fine-tuning\n",
    "lr = 5e-5\n",
    "\n",
    "patience = 20\n",
    "\n",
    "model2.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "     metrics=['accuracy'],\n",
    ")\n",
    "earlystop = EarlyStopping(monitor=\"loss\",\n",
    "                              mode=\"min\", patience=patience)\n",
    "\n",
    "# set checkpoint file\n",
    "model_checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "                                verbose=0, save_best_only=True, mode='min',\n",
    "                                save_weights_only = True)\n",
    "\n",
    "callbacks = [model_checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load with previous weights\n",
    "model2.load_weights(initial_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_train = False #True\n",
    "\n",
    "if do_train:\n",
    "    history1 = model2.fit(AnchorPositivePairs(num_batchs=num_batches), epochs=max_epochs,\n",
    "                          callbacks=callbacks)\n",
    "\n",
    "    plt.figure(figsize = (10,10))\n",
    "    plt.subplot(221)\n",
    "    plt.plot(history1.history[\"loss\"])\n",
    "    plt.xlabel('Model training epoch number')\n",
    "    plt.ylabel('Loss (soft cosine distance)')\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plt.plot(history1.history[\"accuracy\"])\n",
    "    plt.xlabel('Model training epoch number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    # plt.show()\n",
    "    # plt.savefig(hist_fig, dpi=200, bbox_inches='tight')\n",
    "    # plt.close('all')\n",
    "\n",
    "else:\n",
    "    model2.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim_use = num_embed_dim #2\n",
    "\n",
    "knn3 = fit_knn_to_embeddings(model2, X_train, ytrain, n_neighbors)\n",
    "\n",
    "knn5 = fit_knn_to_embeddings(model2, X_train, ytrain, 5)\n",
    "\n",
    "knn7 = fit_knn_to_embeddings(model2, X_train, ytrain, 7)\n",
    "\n",
    "del X_train, ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, ytest, class_idx_to_test_idxs = get_test_stuff(num_batches)\n",
    "\n",
    "touse = len(X_test) \n",
    "\n",
    "# touse = 300\n",
    "\n",
    "embeddings_test = model2.predict(X_test[:touse])\n",
    "embeddings_test = tf.nn.l2_normalize(embeddings_test, axis=-1)\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = knn3.predict(embeddings_test[:,:num_dim_use])\n",
    "y_pred2 = knn5.predict(embeddings_test[:,:num_dim_use])\n",
    "y_pred3 = knn7.predict(embeddings_test[:,:num_dim_use])\n",
    "\n",
    "y_prob1 = knn3.predict_proba(embeddings_test[:,:num_dim_use])\n",
    "y_prob2 = knn5.predict_proba(embeddings_test[:,:num_dim_use])\n",
    "y_prob3 = knn7.predict_proba(embeddings_test[:,:num_dim_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = knn3.score(embeddings_test[:,:num_dim_use], ytest[:touse])\n",
    "score2 = knn5.score(embeddings_test[:,:num_dim_use], ytest[:touse])\n",
    "score3 = knn7.score(embeddings_test[:,:num_dim_use], ytest[:touse])\n",
    "\n",
    "print('3-NN score: %f' % score1)\n",
    "print('5-NN score: %f' % score2)\n",
    "print('7-NN score: %f' % score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.c_[y_pred1, y_pred2, y_pred3]\n",
    "\n",
    "use = np.any(mask>.9, axis=1) #only predictions where all probabilities are > 0.9\n",
    "mask = mask[use,:]\n",
    "\n",
    "# weighted average - you might decide to use this based on each model's average scores\n",
    "# y_en = np.round(np.average(mask, axis=1, weights=[.1, .1, .5]))\n",
    "\n",
    "y_en = np.median(mask, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = ytest[:touse][use]\n",
    "preds = y_en\n",
    "\n",
    "cm = confusion_matrix(labs, preds)\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "thres=0.1\n",
    "cm[cm<thres] = 0\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(cm,\n",
    "  annot=True,\n",
    "  cmap = sns.cubehelix_palette(dark=0, light=1, as_cmap=True))\n",
    "\n",
    "tick_marks = np.arange(len(CLASSES))+.5\n",
    "plt.xticks(tick_marks, [c.decode() for c in CLASSES], rotation=90,fontsize=12)\n",
    "plt.yticks(tick_marks, [c.decode() for c in CLASSES],rotation=0, fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo] *",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
